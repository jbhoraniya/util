{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Description Useful (I hope) supplementary to util project. Information about various linux tools to aid day-to-day work. Cheatsheet http://brendangregg.com/linuxperf.html Writing zsh completions references https://askql.wordpress.com/2011/01/11/zsh-writing-own-completion/ https://github.com/zsh-users/zsh-completions/blob/master/zsh-completions-howto.org https://stackoverflow.com/questions/10194558/auto-completion-in-zsh-on-3rd-parameter http://zsh.sourceforge.net/Doc/Release/index.html http://mads-hartmann.com/2017/08/06/writing-zsh-completion-scripts.html https://wikimatze.de/writing-zsh-completion-for-padrino/","title":"Home"},{"location":"#description","text":"Useful (I hope) supplementary to util project. Information about various linux tools to aid day-to-day work.","title":"Description"},{"location":"#cheatsheet","text":"http://brendangregg.com/linuxperf.html","title":"Cheatsheet"},{"location":"#writing-zsh-completions-references","text":"https://askql.wordpress.com/2011/01/11/zsh-writing-own-completion/ https://github.com/zsh-users/zsh-completions/blob/master/zsh-completions-howto.org https://stackoverflow.com/questions/10194558/auto-completion-in-zsh-on-3rd-parameter http://zsh.sourceforge.net/Doc/Release/index.html http://mads-hartmann.com/2017/08/06/writing-zsh-completion-scripts.html https://wikimatze.de/writing-zsh-completion-for-padrino/","title":"Writing zsh completions references"},{"location":"basics/basics/","text":"Terminal/Console Execution environment for applications. Provided by the kernel. Usually provides the hardware (keyboard). Exposes text buffer. Only one application runs in foreground (this application \"owns\" keyboard/mouse). Shell Application. The command interpreter. Shortcuts ctrl + k cut/delete text from cursor position till the end of the line ctrl + u cut line ctrl + y paste line ctrl + a move cursor to the begining of the line ctrl + e move cursor to the end of the line alt + f move cursor one word right alt + b move cursor one word left ctrl + l clear screen alt + . last command's argument Uses terminal, exposes OS functionality for user (via commands). Allows configuration of execution environment (via variables) Redirections In fact: duplicating When anything like 2>&1 is encountered, it means that anything that descriptor 2 contains is copied to descriptor 1, thus the order of \"redirects\" matter: ls / not_existing_file 2>&1 1>/dev/null - the stderr is copied to whatever the descriptor 1 points to ( stdout by default), the stdout is copied to /dev/null , the stderr is not updated ls / not_existing_file 1>/dev/null 2>&1 - the stdout is copied to /dev/null the stderr is copied to whatever the descriptor 1 points to ( dev/null in this case) Closing file descriptors To close descriptor, duplicate - , e.g.: Close stdin: 0<&- Close stdout: 1<&- Environment variables Shell utilizes two types of variables: - local : accessible only via current shell. Not passed to child processes. Created: a=some_value . Displayed with set command - exported : passed to every child process. Created: export a=some_value . Displayed with env command Dotfiles This term refers to all (some will argue that not only) user local hidden files (like your .vimrc ). At some point management of these file becomes cumbersome. There are a lot of techniques to aid this problem, personally I found one to be particularly good . References https://unix.stackexchange.com/questions/4126/what-is-the-exact-difference-between-a-terminal-a-shell-a-tty-and-a-con http://mywiki.wooledge.org/SignalTrap https://wiki.bash-hackers.org/howto/redirection_tutorial https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/","title":"Basics"},{"location":"basics/basics/#terminalconsole","text":"Execution environment for applications. Provided by the kernel. Usually provides the hardware (keyboard). Exposes text buffer. Only one application runs in foreground (this application \"owns\" keyboard/mouse).","title":"Terminal/Console"},{"location":"basics/basics/#shell","text":"Application. The command interpreter.","title":"Shell"},{"location":"basics/basics/#shortcuts","text":"ctrl + k cut/delete text from cursor position till the end of the line ctrl + u cut line ctrl + y paste line ctrl + a move cursor to the begining of the line ctrl + e move cursor to the end of the line alt + f move cursor one word right alt + b move cursor one word left ctrl + l clear screen alt + . last command's argument Uses terminal, exposes OS functionality for user (via commands). Allows configuration of execution environment (via variables)","title":"Shortcuts"},{"location":"basics/basics/#redirections","text":"In fact: duplicating When anything like 2>&1 is encountered, it means that anything that descriptor 2 contains is copied to descriptor 1, thus the order of \"redirects\" matter: ls / not_existing_file 2>&1 1>/dev/null - the stderr is copied to whatever the descriptor 1 points to ( stdout by default), the stdout is copied to /dev/null , the stderr is not updated ls / not_existing_file 1>/dev/null 2>&1 - the stdout is copied to /dev/null the stderr is copied to whatever the descriptor 1 points to ( dev/null in this case)","title":"Redirections"},{"location":"basics/basics/#closing-file-descriptors","text":"To close descriptor, duplicate - , e.g.: Close stdin: 0<&- Close stdout: 1<&-","title":"Closing file descriptors"},{"location":"basics/basics/#environment-variables","text":"Shell utilizes two types of variables: - local : accessible only via current shell. Not passed to child processes. Created: a=some_value . Displayed with set command - exported : passed to every child process. Created: export a=some_value . Displayed with env command","title":"Environment variables"},{"location":"basics/basics/#dotfiles","text":"This term refers to all (some will argue that not only) user local hidden files (like your .vimrc ). At some point management of these file becomes cumbersome. There are a lot of techniques to aid this problem, personally I found one to be particularly good .","title":"Dotfiles"},{"location":"basics/basics/#references","text":"https://unix.stackexchange.com/questions/4126/what-is-the-exact-difference-between-a-terminal-a-shell-a-tty-and-a-con http://mywiki.wooledge.org/SignalTrap https://wiki.bash-hackers.org/howto/redirection_tutorial https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/","title":"References"},{"location":"basics/editing/","text":"To set per-user default editor use EDITOR environmental variable. Legend symbol meaning C Control key M Alt key Vim In order to set options permanently, append them to ~/.vimrc Edition operation shortcut cut/copy/paste select with v (whole line with V , rectangular blocks with ctrl + v ), use cursors, copy with y , cut with d , paste before cursor with P , after with p undo u reformat code = , e.g. gg=G Syntax options option shortcut highlight :syntax on/off line numbers :set number / set nonumber Vi General options option shortcut compatibility mode (compatibility with very old plain vi ), following command disables compatibility :set nocompatible Tmux Shortcuts Assuming C-b is the prefix option shortcut bring tmux command line C-b : synchronize panes C-b :setw synchronize-panes align panes vertical: C-b M-2 , horizontal: C-b M-1 References http://vim.wikia.com/wiki/Vim_Tips_Wiki https://gist.github.com/MohamedAlaa/2961058","title":"Editors"},{"location":"basics/editing/#vim","text":"In order to set options permanently, append them to ~/.vimrc","title":"Vim"},{"location":"basics/editing/#edition","text":"operation shortcut cut/copy/paste select with v (whole line with V , rectangular blocks with ctrl + v ), use cursors, copy with y , cut with d , paste before cursor with P , after with p undo u reformat code = , e.g. gg=G","title":"Edition"},{"location":"basics/editing/#syntax-options","text":"option shortcut highlight :syntax on/off line numbers :set number / set nonumber","title":"Syntax options"},{"location":"basics/editing/#vi","text":"","title":"Vi"},{"location":"basics/editing/#general-options","text":"option shortcut compatibility mode (compatibility with very old plain vi ), following command disables compatibility :set nocompatible","title":"General options"},{"location":"basics/editing/#tmux","text":"","title":"Tmux"},{"location":"basics/editing/#shortcuts","text":"Assuming C-b is the prefix option shortcut bring tmux command line C-b : synchronize panes C-b :setw synchronize-panes align panes vertical: C-b M-2 , horizontal: C-b M-1","title":"Shortcuts"},{"location":"basics/editing/#references","text":"http://vim.wikia.com/wiki/Vim_Tips_Wiki https://gist.github.com/MohamedAlaa/2961058","title":"References"},{"location":"basics/git/","text":"Hands-on commands Checkout remote branch: git checkout -b <name> origin/<name> Checkout directory to given changeset: git checkout 3257289075289037592abcde253 -- path/to/the/folder/ Multiple working trees sharing one repository: git worktree add ../some/path/ branch Checkouts branch under ../some/path/ Compare file with branch: git diff mybranch otherbranch -- somefile Transfer single commit between branches: git checkout desired_branch git cherry-pick commit_to_be_transferred_here_by_hash git checkout orig_branch #rollback commit git reset --hard HEAD^ Branches Checkout Remote to local: git checkout -b <local_branch_name> <remote_name>/<remote_branch_name> Delete Remote: git push <remote_name> --delete <branch_name> Locally: git branch -d <local_branch_name> Rename First rename locally, then delete old branch on remote, finally push \"new\" git branch -m old_branch new_branch git push origin :old_branch git push --set-upstream origin new_branch Submodules Project can depend on other projects. Root project is called superproject . Submodule is always a commit in other repostiory. In other words: submodule points to particular commit. Adding git submodule add https://repo.url... [path] add submodule to project at path . If no path is given then repo name is used. Deleting git submodule deinit <submodule> git rm <submodule> rm -rf .git/modules/asubmodule Cloning By default submodules are not cloned. In order to clone repository that contains submodules: git clone <repo url> git submodule init git submodule update Upgrading In order to \"upgrade\" the submodule (actually submodule's commit): cd submodule_dir git checkout desired_branch git pull cd .. git add submodule_dir && git commit -m \"...\" Other In order to change submodule's URL, edit .gitmodules files and run git submodule sync Useful infographic References http://www.saintsjd.com/2011/01/what-is-a-bare-git-repository/ http://gitolite.com/gcs.html#%281%29","title":"GIT"},{"location":"basics/git/#hands-on-commands","text":"Checkout remote branch: git checkout -b <name> origin/<name> Checkout directory to given changeset: git checkout 3257289075289037592abcde253 -- path/to/the/folder/ Multiple working trees sharing one repository: git worktree add ../some/path/ branch Checkouts branch under ../some/path/ Compare file with branch: git diff mybranch otherbranch -- somefile Transfer single commit between branches: git checkout desired_branch git cherry-pick commit_to_be_transferred_here_by_hash git checkout orig_branch #rollback commit git reset --hard HEAD^","title":"Hands-on commands"},{"location":"basics/git/#branches","text":"","title":"Branches"},{"location":"basics/git/#checkout","text":"Remote to local: git checkout -b <local_branch_name> <remote_name>/<remote_branch_name>","title":"Checkout"},{"location":"basics/git/#delete","text":"Remote: git push <remote_name> --delete <branch_name> Locally: git branch -d <local_branch_name>","title":"Delete"},{"location":"basics/git/#rename","text":"First rename locally, then delete old branch on remote, finally push \"new\" git branch -m old_branch new_branch git push origin :old_branch git push --set-upstream origin new_branch","title":"Rename"},{"location":"basics/git/#submodules","text":"Project can depend on other projects. Root project is called superproject . Submodule is always a commit in other repostiory. In other words: submodule points to particular commit.","title":"Submodules"},{"location":"basics/git/#adding","text":"git submodule add https://repo.url... [path] add submodule to project at path . If no path is given then repo name is used.","title":"Adding"},{"location":"basics/git/#deleting","text":"git submodule deinit <submodule> git rm <submodule> rm -rf .git/modules/asubmodule","title":"Deleting"},{"location":"basics/git/#cloning","text":"By default submodules are not cloned. In order to clone repository that contains submodules: git clone <repo url> git submodule init git submodule update","title":"Cloning"},{"location":"basics/git/#upgrading","text":"In order to \"upgrade\" the submodule (actually submodule's commit): cd submodule_dir git checkout desired_branch git pull cd .. git add submodule_dir && git commit -m \"...\"","title":"Upgrading"},{"location":"basics/git/#other","text":"In order to change submodule's URL, edit .gitmodules files and run git submodule sync","title":"Other"},{"location":"basics/git/#useful-infographic","text":"","title":"Useful infographic"},{"location":"basics/git/#references","text":"http://www.saintsjd.com/2011/01/what-is-a-bare-git-repository/ http://gitolite.com/gcs.html#%281%29","title":"References"},{"location":"basics/memory/","text":"RAM Linux Memory Types physical memory - resource containing code and data. swap file - optional. Keeps (dirty) modified memory for later use if too many demands are made on physical memory. virtual memory - \"unlimited\" (...) No matter the memory type - all are managed as pages (typically 4096 bytes) Memory monitoring tools free (-m to show in MB) Header Description total indicates memory/physical RAM available for your machine. By default these numbers are in KB's used indicates memory/RAM used by system. This includes buffers and cached data size as well free indicates total unused RAM available for new process to run shared indicates shared memory. This column is obsolete and may be removed in future releases of free buffers indicates total RAM buffered by different applications in Linux cached indicates total RAM used for Caching of data for future purpose -/+ buffers/cache shows used column minus ( buffers + cached ) and free column plus ( buffers + cached ). Why is that? Because when memory used is getting up to limit, the buffers + cache will be freed and used by demanding applications. This show most accurate memory usage. New version of free : - buff/cache: sum of buffers and cached - available: not exactly free column plus ( buffers + cached ) top Header Description virt res shr %mem","title":"MEM"},{"location":"basics/memory/#ram","text":"","title":"RAM"},{"location":"basics/memory/#linux-memory-types","text":"physical memory - resource containing code and data. swap file - optional. Keeps (dirty) modified memory for later use if too many demands are made on physical memory. virtual memory - \"unlimited\" (...) No matter the memory type - all are managed as pages (typically 4096 bytes)","title":"Linux Memory Types"},{"location":"basics/memory/#memory-monitoring-tools","text":"free (-m to show in MB) Header Description total indicates memory/physical RAM available for your machine. By default these numbers are in KB's used indicates memory/RAM used by system. This includes buffers and cached data size as well free indicates total unused RAM available for new process to run shared indicates shared memory. This column is obsolete and may be removed in future releases of free buffers indicates total RAM buffered by different applications in Linux cached indicates total RAM used for Caching of data for future purpose -/+ buffers/cache shows used column minus ( buffers + cached ) and free column plus ( buffers + cached ). Why is that? Because when memory used is getting up to limit, the buffers + cache will be freed and used by demanding applications. This show most accurate memory usage. New version of free : - buff/cache: sum of buffers and cached - available: not exactly free column plus ( buffers + cached ) top Header Description virt res shr %mem","title":"Memory monitoring tools"},{"location":"basics/scripting/","text":"Command substitution `some command` plugs the some command output into some other context. Currently backticks are deprecated in favour of $(some command) form which: permits nesting $(some command $(some nested)) treats \\\\ differently Process substitution <(command_list) Feeds the output of processes into the stdin of another process (piping allows only one command output to be redirected into stdin of another process). The process list is run with its input or output connected to a FIFO or some file in /dev/fd. The name of this file is passed as an argument to the current command as the result of the expansion. Bash Nothing more than shell implementation Expansions Controls how parameters/expressions are expanded Variable expansion Introduced by $ e.g. $param_name . Variable may be enclosed in braces in order to separate it from adjacent characters Tilde expansion Begins with unquoted ~ , includes all following characters up to unquoted slash. Characters are treated as login name. Evaluation of ~login yields login user home directory. Indirect expansion todo Quotes Single quotes preserve everything. No expansion occurs Built-in variables Positional parameters * $# number of command line arguments * $* all of positional arguments (single word) Special * $? last command's exit code * $! most recently executed process ID Test constructs if statement tests wheter exit status of command is equal to 0 if test command; then #sth fi bash has alias for test command and is called [ so the statement becomes: if [ command ]; then fi With Bash 2.02 extended test statement was introduced: if [[ command ]]; then fi Extended test statement is not compliant with POSIX. It features: * No need to quote variables (in [ not quoted variables like \"something with spaces\" will yield error: too many arguments ) * Regular expression matching with =~ e.g. [[ $variable =~ .*string ]] * Wildcard matching e.g. [[ $(lxc-ls) == *\"desired_container\"* ]] * Chaining tests with and and or like: [[ ... && ... ]] (otherwise: [] && [] ) Negate To negate test condition: if ! [ ... ] Conditional expressions condition description -f file true if file exists and is regular file -z string true if length of string is zero -n string true if length of string is non-zero Exit codes Bash scripts can exit with error codes Some codes have special meaning code meaning 1 general catch-all 2 misuse of built-ins (e.g. syntax errors like missing keywords) 126 cannot execute command (permission issue or not executable script) 127 command not found (typo or command missing on $PATH ) 128 wrong exit argument 128+ n fatal signal n 130 termination via signal ( ctrl+c ) AWK print the quote sign awk 'BEGIN { print \"Here is a single quote <'\"'\"'>\" }' #this is concat of three strings '...' \"'\" '...' awk 'BEGIN { print \"Here is a single quote <'\\''>\" }' simple examples: #longest line in data file awk '{ if (length($0) > max) max = length($0) } END { print max }' data # Print every line that has at least one field awk 'NF > 0' data you can have multiple rules in awk (first goes first rule, then second, if line contains both patterns then will be processed two times) awk '/12/ { print $0 } /21/ { print $0 }' data awk env variables: - AWKPATH - contains awk programs - AWKLIBPATH - contains extensions (can be written in C/C++) References https://google.github.io/styleguide/shell.xml https://github.com/robbyrussell/oh-my-zsh/wiki/Coding-style-guide http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html https://www.kernel.org/doc/Documentation/process/coding-style.rst https://github.com/zsh-users/zsh-completions/blob/master/zsh-completions-howto.org http://www.tldp.org/LDP/abs/html/internalvariables.html http://mywiki.wooledge.org/BashFAQ http://mywiki.wooledge.org/BashPitfalls https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html http://wiki.bash-hackers.org/syntax/pe (http://wiki.bash-hackers.org/start) http://tldp.org/LDP/abs/html/exitcodes.html https://www.gnu.org/software/bash/manual/html_node/Tilde-Expansion.html","title":"Scripting"},{"location":"basics/scripting/#command-substitution","text":"`some command` plugs the some command output into some other context. Currently backticks are deprecated in favour of $(some command) form which: permits nesting $(some command $(some nested)) treats \\\\ differently","title":"Command substitution"},{"location":"basics/scripting/#process-substitution","text":"<(command_list) Feeds the output of processes into the stdin of another process (piping allows only one command output to be redirected into stdin of another process). The process list is run with its input or output connected to a FIFO or some file in /dev/fd. The name of this file is passed as an argument to the current command as the result of the expansion.","title":"Process substitution"},{"location":"basics/scripting/#bash","text":"Nothing more than shell implementation","title":"Bash"},{"location":"basics/scripting/#expansions","text":"Controls how parameters/expressions are expanded","title":"Expansions"},{"location":"basics/scripting/#variable-expansion","text":"Introduced by $ e.g. $param_name . Variable may be enclosed in braces in order to separate it from adjacent characters","title":"Variable expansion"},{"location":"basics/scripting/#tilde-expansion","text":"Begins with unquoted ~ , includes all following characters up to unquoted slash. Characters are treated as login name. Evaluation of ~login yields login user home directory.","title":"Tilde expansion"},{"location":"basics/scripting/#indirect-expansion","text":"todo","title":"Indirect expansion"},{"location":"basics/scripting/#quotes","text":"Single quotes preserve everything. No expansion occurs","title":"Quotes"},{"location":"basics/scripting/#built-in-variables","text":"Positional parameters * $# number of command line arguments * $* all of positional arguments (single word) Special * $? last command's exit code * $! most recently executed process ID","title":"Built-in variables"},{"location":"basics/scripting/#test-constructs","text":"if statement tests wheter exit status of command is equal to 0 if test command; then #sth fi bash has alias for test command and is called [ so the statement becomes: if [ command ]; then fi With Bash 2.02 extended test statement was introduced: if [[ command ]]; then fi Extended test statement is not compliant with POSIX. It features: * No need to quote variables (in [ not quoted variables like \"something with spaces\" will yield error: too many arguments ) * Regular expression matching with =~ e.g. [[ $variable =~ .*string ]] * Wildcard matching e.g. [[ $(lxc-ls) == *\"desired_container\"* ]] * Chaining tests with and and or like: [[ ... && ... ]] (otherwise: [] && [] )","title":"Test constructs"},{"location":"basics/scripting/#negate","text":"To negate test condition: if ! [ ... ]","title":"Negate"},{"location":"basics/scripting/#conditional-expressions","text":"condition description -f file true if file exists and is regular file -z string true if length of string is zero -n string true if length of string is non-zero","title":"Conditional expressions"},{"location":"basics/scripting/#exit-codes","text":"Bash scripts can exit with error codes Some codes have special meaning code meaning 1 general catch-all 2 misuse of built-ins (e.g. syntax errors like missing keywords) 126 cannot execute command (permission issue or not executable script) 127 command not found (typo or command missing on $PATH ) 128 wrong exit argument 128+ n fatal signal n 130 termination via signal ( ctrl+c )","title":"Exit codes"},{"location":"basics/scripting/#awk","text":"print the quote sign awk 'BEGIN { print \"Here is a single quote <'\"'\"'>\" }' #this is concat of three strings '...' \"'\" '...' awk 'BEGIN { print \"Here is a single quote <'\\''>\" }' simple examples: #longest line in data file awk '{ if (length($0) > max) max = length($0) } END { print max }' data # Print every line that has at least one field awk 'NF > 0' data you can have multiple rules in awk (first goes first rule, then second, if line contains both patterns then will be processed two times) awk '/12/ { print $0 } /21/ { print $0 }' data awk env variables: - AWKPATH - contains awk programs - AWKLIBPATH - contains extensions (can be written in C/C++)","title":"AWK"},{"location":"basics/scripting/#references","text":"https://google.github.io/styleguide/shell.xml https://github.com/robbyrussell/oh-my-zsh/wiki/Coding-style-guide http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html https://www.kernel.org/doc/Documentation/process/coding-style.rst https://github.com/zsh-users/zsh-completions/blob/master/zsh-completions-howto.org http://www.tldp.org/LDP/abs/html/internalvariables.html http://mywiki.wooledge.org/BashFAQ http://mywiki.wooledge.org/BashPitfalls https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html http://wiki.bash-hackers.org/syntax/pe (http://wiki.bash-hackers.org/start) http://tldp.org/LDP/abs/html/exitcodes.html https://www.gnu.org/software/bash/manual/html_node/Tilde-Expansion.html","title":"References"},{"location":"iaac/foreman/","text":"Basics Configuration Host creation screen Interface must be marked as managed in order to download os image. Provisioning Debian-based OS installation Named as: preseeding Example of preseed with custom partition setup d-i partman-auto/choose_recipe select expert d-i partman-auto/expert_recipe string \\ boot-root :: \\ 2048 2200 4096 ext4 \\ $primary{ } $bootable{ } \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ / } \\ . \\ 1024 1100 1500 ext4 \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ /home } \\ . \\ 1024 1100 1500 ext4 \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ /var } \\ . \\ 1024 1030 1056 linux-swap \\ method{ swap } format{ } \\ . Integration Saltstack Saltstack's grains map to foreman facts. Saltstack's pillar map to foreman parameters. References","title":"Foreman"},{"location":"iaac/foreman/#basics","text":"","title":"Basics"},{"location":"iaac/foreman/#configuration","text":"","title":"Configuration"},{"location":"iaac/foreman/#host-creation-screen","text":"Interface must be marked as managed in order to download os image.","title":"Host creation screen"},{"location":"iaac/foreman/#provisioning","text":"","title":"Provisioning"},{"location":"iaac/foreman/#debian-based-os-installation","text":"Named as: preseeding Example of preseed with custom partition setup d-i partman-auto/choose_recipe select expert d-i partman-auto/expert_recipe string \\ boot-root :: \\ 2048 2200 4096 ext4 \\ $primary{ } $bootable{ } \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ / } \\ . \\ 1024 1100 1500 ext4 \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ /home } \\ . \\ 1024 1100 1500 ext4 \\ method{ format } format{ } \\ use_filesystem{ } filesystem{ ext4 } \\ mountpoint{ /var } \\ . \\ 1024 1030 1056 linux-swap \\ method{ swap } format{ } \\ .","title":"Debian-based OS installation"},{"location":"iaac/foreman/#integration","text":"","title":"Integration"},{"location":"iaac/foreman/#saltstack","text":"Saltstack's grains map to foreman facts. Saltstack's pillar map to foreman parameters.","title":"Saltstack"},{"location":"iaac/foreman/#references","text":"","title":"References"},{"location":"iaac/helm/","text":"Helm is 'package' manager for Kubernetes. Helps with following \"bare\" Kubernetes issues: 1. Maintaining Kubernetes YAML files is tedious (a lot of in-yaml duplication, a lot of duplicated YAMLs for multiple environments) 2. Allows to express dependencies, surprisingly Kubernetes doesn't support this References https://docs.helm.sh/","title":"Helm"},{"location":"iaac/helm/#references","text":"https://docs.helm.sh/","title":"References"},{"location":"iaac/unattended/","text":"Unattended system provisioning Unattended installation comprises of two phases: 1. Installation of OS itself with automatic answers 2. Provisioning of installed OS (application installation, configuration etc.) OS installation Automated OS installation process utilizes PXE boot. 1. The UEFI/BIOS sends DHCP discover request 2. The DHCP offer must contain TFTP Server Name (option 66) and Filename (option 67) 3. BIOS/UEFI downloads the loader from TFTP server and runs it The process looks like (BIOS example) this: PXE booting relies on platform firmware, its configuration files are different for BIOS and its successor UEFI. The available UEFI loaders (for BIOS just send the pxelinux.0 file): 1. Syslinux. Can download images via TFTP, contains many bugs, e.g. the TFTP download of linux image may take long time and syslinux will terminate the download process with timeout. This breaks the whole process 2. GRUB. This must be pre-build grub image with PXE support that will continue to download the initrd/image from servers. 3. iPXE. The de-facto standard, open-source and most robust solution. Can download initrd/image from the HTTP servers thus making it the most reliable. BIOS Hardware initialization in not exactly standarized manner (as reverse engineered from IBM first implementation). After POST boots by reading and executing first sector on hard disk. Booting runs in 16-bit processor mode and has only 1MB of space to execute in. Has problems with parallel device initialization. Can boot only from hard disks of size less than 2.1TB Uses MBR partitioning scheme UEFI Has no limitations of BIOS, standarized (by Intel). Specifies following servies available for OS and OS loader: System table Boot time services Run time services Console Additional tables Boots by loading EFI program files. Uses GPT partitioning scheme References https://www.debian.org/releases/stretch/example-preseed.txt https://wikitech.wikimedia.org/wiki/PartMan https://wiki.debian.org/DebianInstaller/Preseed https://www.debian.org/releases/stable/amd64/ch03s06.html.en#UEFI http://fai-project.org/fai-guide/ https://www.youtube.com/watch?v=bNL1pd-rwCU https://www.howtogeek.com/56958/htg-explains-how-uefi-will-replace-the-bios/ https://superuser.com/questions/496026/what-is-the-difference-in-boot-with-bios-and-boot-with-uefi","title":"Unattended"},{"location":"iaac/unattended/#unattended-system-provisioning","text":"Unattended installation comprises of two phases: 1. Installation of OS itself with automatic answers 2. Provisioning of installed OS (application installation, configuration etc.)","title":"Unattended system provisioning"},{"location":"iaac/unattended/#os-installation","text":"Automated OS installation process utilizes PXE boot. 1. The UEFI/BIOS sends DHCP discover request 2. The DHCP offer must contain TFTP Server Name (option 66) and Filename (option 67) 3. BIOS/UEFI downloads the loader from TFTP server and runs it The process looks like (BIOS example) this: PXE booting relies on platform firmware, its configuration files are different for BIOS and its successor UEFI. The available UEFI loaders (for BIOS just send the pxelinux.0 file): 1. Syslinux. Can download images via TFTP, contains many bugs, e.g. the TFTP download of linux image may take long time and syslinux will terminate the download process with timeout. This breaks the whole process 2. GRUB. This must be pre-build grub image with PXE support that will continue to download the initrd/image from servers. 3. iPXE. The de-facto standard, open-source and most robust solution. Can download initrd/image from the HTTP servers thus making it the most reliable.","title":"OS installation"},{"location":"iaac/unattended/#bios","text":"Hardware initialization in not exactly standarized manner (as reverse engineered from IBM first implementation). After POST boots by reading and executing first sector on hard disk. Booting runs in 16-bit processor mode and has only 1MB of space to execute in. Has problems with parallel device initialization. Can boot only from hard disks of size less than 2.1TB Uses MBR partitioning scheme","title":"BIOS"},{"location":"iaac/unattended/#uefi","text":"Has no limitations of BIOS, standarized (by Intel). Specifies following servies available for OS and OS loader: System table Boot time services Run time services Console Additional tables Boots by loading EFI program files. Uses GPT partitioning scheme","title":"UEFI"},{"location":"iaac/unattended/#references","text":"https://www.debian.org/releases/stretch/example-preseed.txt https://wikitech.wikimedia.org/wiki/PartMan https://wiki.debian.org/DebianInstaller/Preseed https://www.debian.org/releases/stable/amd64/ch03s06.html.en#UEFI http://fai-project.org/fai-guide/ https://www.youtube.com/watch?v=bNL1pd-rwCU https://www.howtogeek.com/56958/htg-explains-how-uefi-will-replace-the-bios/ https://superuser.com/questions/496026/what-is-the-difference-in-boot-with-bios-and-boot-with-uefi","title":"References"},{"location":"iaac/vagrant/","text":"Create Box Create QCOW2 backed VM image using libvirt Execute following in the VM useradd vagrant mkdir ~vagrant/.ssh chmod 700 ~vagrant/.ssh echo \"ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key\" > ~vagrant/.ssh/authorized_keys chmod 600 ~vagrant/.ssh/authorized_keys chown -R vagrant.vagrant ~vagrant/.ssh # Sudoers sed -i -r 's/(.* requiretty)/#\\1/' /etc/sudoers cat > /etc/sudoers.d/90-vagrant-users <<EOF # User rules for vagrant vagrant ALL=(ALL) NOPASSWD:ALL EOF apt-get install rsync Configure only eth0 network interface, disable modern predictable network interface names Optionally disable unattended-upgrades Shutdown VM cp /var/lib/libvirt/images/yourqcowimage box.img Create the *.box using helper If the *.box will be uploaded to https://app.vagrantup.com/ generate the sha256 hash using: openssl sha256 box.img and provide it in the web interface References https://gilmatdub.wordpress.com/2014/08/08/howto-create-a-vagrant-image-box-for-libvirt-kvm/ https://github.com/vagrant-libvirt/vagrant-libvirt","title":"Vagrant"},{"location":"iaac/vagrant/#create-box","text":"Create QCOW2 backed VM image using libvirt Execute following in the VM useradd vagrant mkdir ~vagrant/.ssh chmod 700 ~vagrant/.ssh echo \"ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key\" > ~vagrant/.ssh/authorized_keys chmod 600 ~vagrant/.ssh/authorized_keys chown -R vagrant.vagrant ~vagrant/.ssh # Sudoers sed -i -r 's/(.* requiretty)/#\\1/' /etc/sudoers cat > /etc/sudoers.d/90-vagrant-users <<EOF # User rules for vagrant vagrant ALL=(ALL) NOPASSWD:ALL EOF apt-get install rsync Configure only eth0 network interface, disable modern predictable network interface names Optionally disable unattended-upgrades Shutdown VM cp /var/lib/libvirt/images/yourqcowimage box.img Create the *.box using helper If the *.box will be uploaded to https://app.vagrantup.com/ generate the sha256 hash using: openssl sha256 box.img and provide it in the web interface","title":"Create Box"},{"location":"iaac/vagrant/#references","text":"https://gilmatdub.wordpress.com/2014/08/08/howto-create-a-vagrant-image-box-for-libvirt-kvm/ https://github.com/vagrant-libvirt/vagrant-libvirt","title":"References"},{"location":"iaac/kubernetes/POD/","text":"Main execution unit of Kubernetes cluster kubectl get pods -o wide will inform about status, number of restarts, POD readiness or assigned node. Status The status roughly traverses: Pending -> Running -> Succeeded/Failed -> Completed -> CrashLoopBackOff (if policy says so) The POD default restartPolicy is Always , which means that if POD gets to Completed state, it will restart automatically. To specify container command and/or args: command , args . If the Docker runtime is used then the command will override ENTRYPOINT and args will override CMD Minimalistic POD yaml example: apiVersion: v1 kind: Pod metadata: labels: app: pod name: example spec: # restartPolicy: OnFailure containers: - image: httpd name: example # command: # - \"echo\" # - \"A\" Probe Such POD once reaches the Running state, is immediately marked as Ready ( kubectl get pod example ). This is due to the fact that example POD didn't provide readinessProbe - the mechanism which informs the Kubernetes that the POD is ready to accept the traffic. Example enriched with readinessProbe apiVersion: v1 kind: Pod metadata: labels: app: pod name: example spec: containers: - image: httpd name: example readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 There are two types of probes: 1. livenessProbe - inform that the POD needs to be restarted 2. readinessProbe - inform that the POD won't accept traffic Both of them use three different types of materializing checks: 1. exec execute arbitrary command 2. httpGET perform HTTP GET request and succeed if 2XX code received 3. tcpSocket open TCP socket and try to establish a connection (three-way handshake only) Both probes accept different thresholds, periods and inital delays. Using port-forward for accessing non-ready PODs bypasses the non-routing restriction. Resources POD resources may be limited to not use all of node available resources. Adjusted using requests and limits apiVersion: v1 kind: Pod metadata: name: example spec: containers: - image: httpd name: example resources: requests: cpu: 100m memory: 128Mi limits: cpu: 1000m memory: 256Mi requests Kubernetes Scheduler uses this value to place the POD on the best node. This is the value that the node will at least reserve for POD limits Init containers Run certain scripts/procedures during POD startup. Affects the POD status change path: Pending -> Init -> PodInitializing -> Running -> Succeeded/Failed -> Completed -> CrashLoopBackOff (if policy says so) apiVersion: v1 kind: Pod metadata: labels: app: pod name: example1 spec: containers: - image: httpd name: example1 initContainers: - image: debian:stretch-slim name: ini command: - \"sh\" - \"-c\" - \"sleep 5\" Debug with kubectl logs example1 -c ini Termination POD may be terminated not only upon user request but also: - the scheduler may decide to move the POD to other node - some auto-scaling mechanisms - upgrades Termination: 1. for all containers in POD: SIGTERM sent to the PID 1 2. terminationGracePeriodSeconds countdown starts 3. if the timeout occurs and container(s) is still alive the SIGKILL is send Tricks Throw-away debug container: kubectl run -it --rm name --image=the_image --restart=Never -- sh busybox: kubectl run -it --rm busybox --image=busybox --restart=Never -- sh debian: kubectl run -it --rm debian --image=debian --restart=Never -- bash","title":"PODs"},{"location":"iaac/kubernetes/POD/#status","text":"The status roughly traverses: Pending -> Running -> Succeeded/Failed -> Completed -> CrashLoopBackOff (if policy says so) The POD default restartPolicy is Always , which means that if POD gets to Completed state, it will restart automatically. To specify container command and/or args: command , args . If the Docker runtime is used then the command will override ENTRYPOINT and args will override CMD Minimalistic POD yaml example: apiVersion: v1 kind: Pod metadata: labels: app: pod name: example spec: # restartPolicy: OnFailure containers: - image: httpd name: example # command: # - \"echo\" # - \"A\"","title":"Status"},{"location":"iaac/kubernetes/POD/#probe","text":"Such POD once reaches the Running state, is immediately marked as Ready ( kubectl get pod example ). This is due to the fact that example POD didn't provide readinessProbe - the mechanism which informs the Kubernetes that the POD is ready to accept the traffic. Example enriched with readinessProbe apiVersion: v1 kind: Pod metadata: labels: app: pod name: example spec: containers: - image: httpd name: example readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 There are two types of probes: 1. livenessProbe - inform that the POD needs to be restarted 2. readinessProbe - inform that the POD won't accept traffic Both of them use three different types of materializing checks: 1. exec execute arbitrary command 2. httpGET perform HTTP GET request and succeed if 2XX code received 3. tcpSocket open TCP socket and try to establish a connection (three-way handshake only) Both probes accept different thresholds, periods and inital delays. Using port-forward for accessing non-ready PODs bypasses the non-routing restriction.","title":"Probe"},{"location":"iaac/kubernetes/POD/#resources","text":"POD resources may be limited to not use all of node available resources. Adjusted using requests and limits apiVersion: v1 kind: Pod metadata: name: example spec: containers: - image: httpd name: example resources: requests: cpu: 100m memory: 128Mi limits: cpu: 1000m memory: 256Mi","title":"Resources"},{"location":"iaac/kubernetes/POD/#requests","text":"Kubernetes Scheduler uses this value to place the POD on the best node. This is the value that the node will at least reserve for POD","title":"requests"},{"location":"iaac/kubernetes/POD/#limits","text":"","title":"limits"},{"location":"iaac/kubernetes/POD/#init-containers","text":"Run certain scripts/procedures during POD startup. Affects the POD status change path: Pending -> Init -> PodInitializing -> Running -> Succeeded/Failed -> Completed -> CrashLoopBackOff (if policy says so) apiVersion: v1 kind: Pod metadata: labels: app: pod name: example1 spec: containers: - image: httpd name: example1 initContainers: - image: debian:stretch-slim name: ini command: - \"sh\" - \"-c\" - \"sleep 5\" Debug with kubectl logs example1 -c ini","title":"Init containers"},{"location":"iaac/kubernetes/POD/#termination","text":"POD may be terminated not only upon user request but also: - the scheduler may decide to move the POD to other node - some auto-scaling mechanisms - upgrades Termination: 1. for all containers in POD: SIGTERM sent to the PID 1 2. terminationGracePeriodSeconds countdown starts 3. if the timeout occurs and container(s) is still alive the SIGKILL is send","title":"Termination"},{"location":"iaac/kubernetes/POD/#tricks","text":"Throw-away debug container: kubectl run -it --rm name --image=the_image --restart=Never -- sh busybox: kubectl run -it --rm busybox --image=busybox --restart=Never -- sh debian: kubectl run -it --rm debian --image=debian --restart=Never -- bash","title":"Tricks"},{"location":"iaac/kubernetes/admin/","text":"Kubernetes components Kubernetes Cluster spans over multiple nodes, the master (Control Plane) should be separated from worker Nodes kube-apiserver Master only component. Accepts user request. Stores resources definitions in etcd . etcd Master only component. Key-value store that is highly available. Used to store all cluster data. kube-scheduler Master only component. Materializes user requests, watch es the kube-apiserver, decides where and when schedule PODs. PODs definition may contain some data that affects kube-scheduler : - affinity/anti-affinity - nodeSelector - taints/tolerations - reservations/limits It is possible to write custom scheduler kube-controller-manager Master only component. Controllers execute routine tasks to synchronize desired state (typically called spec ) with observed state. Notable mentions: - Node Controller - monitors Node lifecycle, responds when the Node goes down - Replication Controller - manages *-controller s, e.g., deployment-controller - Endpoints Controller - populates Endpoint - Service Account Controller - creates accounts and access tokens for namespaces Full list of controllers kubelet Master/Worker component. Resides on every Node. Connects to the kube-apiserver . Starts the actual containers via the container runtime. Provides health-checks kube-proxy Master/Worker component. Main network component, watches the service s and materializes their rule on the Nodes (e.g. handles iptables ) References https://kubernetes.io/docs/concepts/overview/components/ https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html","title":"Admin"},{"location":"iaac/kubernetes/admin/#kubernetes-components","text":"Kubernetes Cluster spans over multiple nodes, the master (Control Plane) should be separated from worker Nodes","title":"Kubernetes components"},{"location":"iaac/kubernetes/admin/#kube-apiserver","text":"Master only component. Accepts user request. Stores resources definitions in etcd .","title":"kube-apiserver"},{"location":"iaac/kubernetes/admin/#etcd","text":"Master only component. Key-value store that is highly available. Used to store all cluster data.","title":"etcd"},{"location":"iaac/kubernetes/admin/#kube-scheduler","text":"Master only component. Materializes user requests, watch es the kube-apiserver, decides where and when schedule PODs. PODs definition may contain some data that affects kube-scheduler : - affinity/anti-affinity - nodeSelector - taints/tolerations - reservations/limits It is possible to write custom scheduler","title":"kube-scheduler"},{"location":"iaac/kubernetes/admin/#kube-controller-manager","text":"Master only component. Controllers execute routine tasks to synchronize desired state (typically called spec ) with observed state. Notable mentions: - Node Controller - monitors Node lifecycle, responds when the Node goes down - Replication Controller - manages *-controller s, e.g., deployment-controller - Endpoints Controller - populates Endpoint - Service Account Controller - creates accounts and access tokens for namespaces Full list of controllers","title":"kube-controller-manager"},{"location":"iaac/kubernetes/admin/#kubelet","text":"Master/Worker component. Resides on every Node. Connects to the kube-apiserver . Starts the actual containers via the container runtime. Provides health-checks","title":"kubelet"},{"location":"iaac/kubernetes/admin/#kube-proxy","text":"Master/Worker component. Main network component, watches the service s and materializes their rule on the Nodes (e.g. handles iptables )","title":"kube-proxy"},{"location":"iaac/kubernetes/admin/#references","text":"https://kubernetes.io/docs/concepts/overview/components/ https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html","title":"References"},{"location":"iaac/kubernetes/basics/","text":"Kubernetes is the container orchestration tool. Its primary job it to ensure that given container is running with regards to given constraints (e.g container X requires N mb of memory or container X must be replicated N times on different physical nodes). Kubernetes covers more use-cases: it is a platform for automatic deployment, scaling, HA and orchestration of containerized applications Terminology Kubernetes introduces multiple concepts. The main one is: the resource (aka. object) In order to list all of the Kubernetes resources use: kubectl api-resources Kubernetes Resources The handy command: kubectl explain <resource> , for nested fields: kubectl explain <resource>.<field> Node Worker machine, runs Services , capable of running Pods . Either VM or physical machine Pod Resource The 'execution' unit of Kubernetes. Representation of one or more application containers (Docker or rkt ) or shared resources for those containers. Shared resources are available for all containers in the POD. All containers within the POD share the same IP address and can communicate via loopback interface PODs are ephemeral. Namespace Virtual cluster, scope for names. In order to find out if given resource is affected by namespace use: kubectl api-resources --namespaced=true , all of the listed resources respect the namespace setting. Deployment Configuration how to create/update instances of the application Service Set of Pods with policy how to access them (e.g. load balancing or service discovery for pods ) Controller Pods manager (handles, e.g., pod replication). Given information about desired number of PODs ensures the desired number of them is running. Volume Local storage on the POD is ephemeral. When the POD is destroyed, the POD data is gone forever. In order to enable 'persistent' storage - use volumes. Non-resource concepts Labels Key-value pairs. Used to group together set of objects. Each object can have multiple labels, 'same' label can be attached to multiple objects Basics The PODs are the execution units submitted by the 'user', however creating, submitting PODs one by one would be tedious. This is solved by using e.g., deployment, statefulsets or daemonsets. They provide policies for scheduling multiple PODs. Containers within one POD share IP address. Tightly coupled containers should run within one Pod. Pod provides two kinds of shared (by pod's containers) resources: networking and storage . Services match a set of pods using labels and selectors. Services are published or discovered either via DNS or environmental variables. Services by default are visible within the cluster only and there is no way to access them from the outside of the Kubernetes cluster. In Kubernetes following networking rules hold true: 1. All containers/pods can communicate with all other containers/pods without NAT 2. All Nodes can communicate with all containers/pods (and vice-versa) without NAT 3. The IP that a container sees itself as is the same IP that others see it as Kubectl is used to interact with the cluster. If you have multiple clusters, list them with: kubectl config get-contexts , switch between them with: kubectl config use-context CONTEXT_NAME kubectl commands supports both: imperative and declarative management: - kubectl create -f your.yaml - imperative - kubectl apply -f your.yaml - declarative In order to get detailed information about any part of your deployment use kubectl describe <kind> . The term kind is defined in this manual (it can simply be a pod , service or deployment ) In order to debug what actually happens within Kubernetes cluster: kubectl get events --sort-by='{.lastTimestamp}' Setup Setup depends on the number of nodes used for cluster Single-node Use minikube . It uses either KVM or VirtualBox as docker host. In order to increase default limits of the VM, it must be destroyed first: minikube delete; minikube start --memory 12288 Multi-node TODO References https://kubernetes.io/docs/user-journeys/users/application-developer/foundational/ https://coreos.com/rkt/docs/latest/rkt-vs-other-projects.html https://vimeo.com/245778144/4d1d597c5e https://www.magalix.com/blog/kubernetes-cluster-networking-101 https://pracucci.com/graceful-shutdown-of-kubernetes-pods.html","title":"Basics"},{"location":"iaac/kubernetes/basics/#terminology","text":"Kubernetes introduces multiple concepts. The main one is: the resource (aka. object) In order to list all of the Kubernetes resources use: kubectl api-resources","title":"Terminology"},{"location":"iaac/kubernetes/basics/#kubernetes-resources","text":"The handy command: kubectl explain <resource> , for nested fields: kubectl explain <resource>.<field>","title":"Kubernetes Resources"},{"location":"iaac/kubernetes/basics/#node","text":"Worker machine, runs Services , capable of running Pods . Either VM or physical machine","title":"Node"},{"location":"iaac/kubernetes/basics/#pod","text":"Resource The 'execution' unit of Kubernetes. Representation of one or more application containers (Docker or rkt ) or shared resources for those containers. Shared resources are available for all containers in the POD. All containers within the POD share the same IP address and can communicate via loopback interface PODs are ephemeral.","title":"Pod"},{"location":"iaac/kubernetes/basics/#namespace","text":"Virtual cluster, scope for names. In order to find out if given resource is affected by namespace use: kubectl api-resources --namespaced=true , all of the listed resources respect the namespace setting.","title":"Namespace"},{"location":"iaac/kubernetes/basics/#deployment","text":"Configuration how to create/update instances of the application","title":"Deployment"},{"location":"iaac/kubernetes/basics/#service","text":"Set of Pods with policy how to access them (e.g. load balancing or service discovery for pods )","title":"Service"},{"location":"iaac/kubernetes/basics/#controller","text":"Pods manager (handles, e.g., pod replication). Given information about desired number of PODs ensures the desired number of them is running.","title":"Controller"},{"location":"iaac/kubernetes/basics/#volume","text":"Local storage on the POD is ephemeral. When the POD is destroyed, the POD data is gone forever. In order to enable 'persistent' storage - use volumes.","title":"Volume"},{"location":"iaac/kubernetes/basics/#non-resource-concepts","text":"","title":"Non-resource concepts"},{"location":"iaac/kubernetes/basics/#labels","text":"Key-value pairs. Used to group together set of objects. Each object can have multiple labels, 'same' label can be attached to multiple objects","title":"Labels"},{"location":"iaac/kubernetes/basics/#basics","text":"The PODs are the execution units submitted by the 'user', however creating, submitting PODs one by one would be tedious. This is solved by using e.g., deployment, statefulsets or daemonsets. They provide policies for scheduling multiple PODs. Containers within one POD share IP address. Tightly coupled containers should run within one Pod. Pod provides two kinds of shared (by pod's containers) resources: networking and storage . Services match a set of pods using labels and selectors. Services are published or discovered either via DNS or environmental variables. Services by default are visible within the cluster only and there is no way to access them from the outside of the Kubernetes cluster. In Kubernetes following networking rules hold true: 1. All containers/pods can communicate with all other containers/pods without NAT 2. All Nodes can communicate with all containers/pods (and vice-versa) without NAT 3. The IP that a container sees itself as is the same IP that others see it as Kubectl is used to interact with the cluster. If you have multiple clusters, list them with: kubectl config get-contexts , switch between them with: kubectl config use-context CONTEXT_NAME kubectl commands supports both: imperative and declarative management: - kubectl create -f your.yaml - imperative - kubectl apply -f your.yaml - declarative In order to get detailed information about any part of your deployment use kubectl describe <kind> . The term kind is defined in this manual (it can simply be a pod , service or deployment ) In order to debug what actually happens within Kubernetes cluster: kubectl get events --sort-by='{.lastTimestamp}'","title":"Basics"},{"location":"iaac/kubernetes/basics/#setup","text":"Setup depends on the number of nodes used for cluster","title":"Setup"},{"location":"iaac/kubernetes/basics/#single-node","text":"Use minikube . It uses either KVM or VirtualBox as docker host. In order to increase default limits of the VM, it must be destroyed first: minikube delete; minikube start --memory 12288","title":"Single-node"},{"location":"iaac/kubernetes/basics/#multi-node","text":"TODO","title":"Multi-node"},{"location":"iaac/kubernetes/basics/#references","text":"https://kubernetes.io/docs/user-journeys/users/application-developer/foundational/ https://coreos.com/rkt/docs/latest/rkt-vs-other-projects.html https://vimeo.com/245778144/4d1d597c5e https://www.magalix.com/blog/kubernetes-cluster-networking-101 https://pracucci.com/graceful-shutdown-of-kubernetes-pods.html","title":"References"},{"location":"iaac/kubernetes/controllers/","text":"Typically PODs are scheduled via higher-level abstraction that relies on controller s. Controllers 'find' PODs using .spec.selector Deployment Common method to create the groups of PODs with regards to number of instances running. \"Replaces\" ReplicaSet No POD location guarantees by default PODs will be suffixed with random string PODs started at the same time PODs are updated one after another (\"previous\" one must be READY in order to continue) by default. Refer to DeploymentStrategy ) for all options Recreate - Terminate all, start all RollingUpdate - Don't let all PODs go down, upgrades gradually StatefulSet Don't use if 3. is not required No POD location guarantees by default By default started one by one (previous one must be READY in order to continue). It is possible to start all at once (see podManagementPolicy ). Order of deletion is not specified. PODs have stable network identifier (predictable, ordered names instead of random hash). This includes PVC as well, the Nth POD will use Nth PVC. The headless Service is required (must be added manually) Updated in order (hi -> low), regardless of podManagementPolicy setting by default. Refer to StatefulSetUpdateStrategy OnDelete - no automatic action when update is triggered RollingUpdate - Don't let all PODs go down, upgrades gradually. StatefulSet includes partition parameter (default = 0). Only PODs with ID greater than or equal partition will be updated, the rest will not, even when manually deleted. Mind: https://github.com/kubernetes/kubernetes/issues/82612 (change cause message doesn't work for StatefulSet ) https://github.com/kubernetes/kubernetes/issues/67250 (cannot rollout undo statefulset from broken StatefulSet replica) https://github.com/kubernetes/website/issues/17842 (headless Service requirement clarification) DaemonSet The only allowed POD RestartPolicy is always POD location guarantees: one POD on each node PODs started at the same time Adding Nodes adds DaemonSet s PODs (with regards to tolerations , nodeSelector , etc. - see this doc ) PODs are updated one after another (\"previous\" one must be READY in order to continue) by default. Refer to DaemonSetUpdateStrategy RollingUpdate - Don't let all PODs go down, upgrades gradually. OnDelete - no automatic action when update is triggered ReplicaSet In general: replaced by kind: Deployment . Can 'capture' other manually created PODs if they match labels. Internally, the Deployment uses ReplicaSet No POD location guarantees by default Doesn't support updates Doesn't support rollout commands Starts and terminates all at once ReplicationController The ReplicationController simply ensures that the desired number of pods matches its label selector and are operational. Was replaced by ReplicaSet Job Schedules PODs and ensures that specified number of them completes successfully. It is possible to run PODs in parallel. The Job may be used for work queue processing , just remember to set .spec.completions to 1 and .spec.parallelism > 0 No POD location guarantees by default Doesn't support rollout commands By default runs only one POD Usage For interaction with selected controller kind the kubectl rollout command is used. The .spec.template must be changed in order to trigger update, otherwise the same deployment revision is used Information Command Status kubectl rollout status <kind, e.g.: deployment, statefulset> <name> History (The CHANGE-CAUSE is copied from annotation kubernetes.io/change-cause ), more details can be obtained using --revision kubectl rollout history <kind e.g. deployment> <name> [--revision=N] Rollback kubectl rollout undo <kind e.g. deployment> <name>","title":"Controllers"},{"location":"iaac/kubernetes/controllers/#deployment","text":"Common method to create the groups of PODs with regards to number of instances running. \"Replaces\" ReplicaSet No POD location guarantees by default PODs will be suffixed with random string PODs started at the same time PODs are updated one after another (\"previous\" one must be READY in order to continue) by default. Refer to DeploymentStrategy ) for all options Recreate - Terminate all, start all RollingUpdate - Don't let all PODs go down, upgrades gradually","title":"Deployment"},{"location":"iaac/kubernetes/controllers/#statefulset","text":"Don't use if 3. is not required No POD location guarantees by default By default started one by one (previous one must be READY in order to continue). It is possible to start all at once (see podManagementPolicy ). Order of deletion is not specified. PODs have stable network identifier (predictable, ordered names instead of random hash). This includes PVC as well, the Nth POD will use Nth PVC. The headless Service is required (must be added manually) Updated in order (hi -> low), regardless of podManagementPolicy setting by default. Refer to StatefulSetUpdateStrategy OnDelete - no automatic action when update is triggered RollingUpdate - Don't let all PODs go down, upgrades gradually. StatefulSet includes partition parameter (default = 0). Only PODs with ID greater than or equal partition will be updated, the rest will not, even when manually deleted. Mind: https://github.com/kubernetes/kubernetes/issues/82612 (change cause message doesn't work for StatefulSet ) https://github.com/kubernetes/kubernetes/issues/67250 (cannot rollout undo statefulset from broken StatefulSet replica) https://github.com/kubernetes/website/issues/17842 (headless Service requirement clarification)","title":"StatefulSet"},{"location":"iaac/kubernetes/controllers/#daemonset","text":"The only allowed POD RestartPolicy is always POD location guarantees: one POD on each node PODs started at the same time Adding Nodes adds DaemonSet s PODs (with regards to tolerations , nodeSelector , etc. - see this doc ) PODs are updated one after another (\"previous\" one must be READY in order to continue) by default. Refer to DaemonSetUpdateStrategy RollingUpdate - Don't let all PODs go down, upgrades gradually. OnDelete - no automatic action when update is triggered","title":"DaemonSet"},{"location":"iaac/kubernetes/controllers/#replicaset","text":"In general: replaced by kind: Deployment . Can 'capture' other manually created PODs if they match labels. Internally, the Deployment uses ReplicaSet No POD location guarantees by default Doesn't support updates Doesn't support rollout commands Starts and terminates all at once","title":"ReplicaSet"},{"location":"iaac/kubernetes/controllers/#replicationcontroller","text":"The ReplicationController simply ensures that the desired number of pods matches its label selector and are operational. Was replaced by ReplicaSet","title":"ReplicationController"},{"location":"iaac/kubernetes/controllers/#job","text":"Schedules PODs and ensures that specified number of them completes successfully. It is possible to run PODs in parallel. The Job may be used for work queue processing , just remember to set .spec.completions to 1 and .spec.parallelism > 0 No POD location guarantees by default Doesn't support rollout commands By default runs only one POD","title":"Job"},{"location":"iaac/kubernetes/controllers/#usage","text":"For interaction with selected controller kind the kubectl rollout command is used. The .spec.template must be changed in order to trigger update, otherwise the same deployment revision is used Information Command Status kubectl rollout status <kind, e.g.: deployment, statefulset> <name> History (The CHANGE-CAUSE is copied from annotation kubernetes.io/change-cause ), more details can be obtained using --revision kubectl rollout history <kind e.g. deployment> <name> [--revision=N] Rollback kubectl rollout undo <kind e.g. deployment> <name>","title":"Usage"},{"location":"iaac/kubernetes/service/","text":"Services create rules for accessing Pods Services route traffic to the ready Pods only Services matches Pods using selectors Services expose DNS name that is available across the cluster. The full service name: name.namespace.svc.cluster.local Default resolution policy in Kubernetes is ClusterFirst : - DNS query is routed to dnsmasq (running in kube-dns pod) - dnsmasq routes the request to: - kube-dns if the name ends with a cluster suffix - to the upstream DNS server otherwise Basics Name resolution is configured in /etc/resolv.conf . By default pods contain resolv.conf with roughly: nameserver 10.10.10.10 search namespace.svc.cluster.local svc.cluster.local cluster.local something.cloud.provider.specific options ndots:5 If the requested DNS name contains fewer than 5 dots then the search domains are checked. If there is no match then the name is treated as an absolute name The FQDN domains (the ones ending with . ) are always treated as absolute. Thus if the pod needs to resolve the name that is known to be external to the cluster: it is good to configure that name as FQDN in the application. The pod will make fewer (number of search option entries) DNS queries It is also possible to customize resolv.conf using dnsConfig pod's section Headless type: ClusterIP clusterIP: None No IP is allocated. DNS is configured: - if selectors are used then DNS returns all matching Pods - if selectors are not configured then: - DNS returns CNAME records for ExternalName -type Services - DNS returns records for and Endpoints that share a name with this Service, for all other types Using headless service it is possible to expose pods hostname for cluster availability (required: hostname and subdomain on the pod) ClusterIP type: ClusterIP Default Service type, guarantees unique IP across the cluster. This IP 'lives' only in iptables and is maintained by kube-proxy . By default uses round-robin to pick pods. NodePort type: NodePort Gives access from outside of the cluster. Opens socket with high port on every node thus allowing to access the service from outside of the cluster using node IP (with that high port) LoadBalancer type: LoadBalancer Gives access from outside of the cluster. This is cloud-specific. Creates the load-balancer-type resource in cloud provider","title":"Service"},{"location":"iaac/kubernetes/service/#basics","text":"Name resolution is configured in /etc/resolv.conf . By default pods contain resolv.conf with roughly: nameserver 10.10.10.10 search namespace.svc.cluster.local svc.cluster.local cluster.local something.cloud.provider.specific options ndots:5 If the requested DNS name contains fewer than 5 dots then the search domains are checked. If there is no match then the name is treated as an absolute name The FQDN domains (the ones ending with . ) are always treated as absolute. Thus if the pod needs to resolve the name that is known to be external to the cluster: it is good to configure that name as FQDN in the application. The pod will make fewer (number of search option entries) DNS queries It is also possible to customize resolv.conf using dnsConfig pod's section","title":"Basics"},{"location":"iaac/kubernetes/service/#headless","text":"type: ClusterIP clusterIP: None No IP is allocated. DNS is configured: - if selectors are used then DNS returns all matching Pods - if selectors are not configured then: - DNS returns CNAME records for ExternalName -type Services - DNS returns records for and Endpoints that share a name with this Service, for all other types Using headless service it is possible to expose pods hostname for cluster availability (required: hostname and subdomain on the pod)","title":"Headless"},{"location":"iaac/kubernetes/service/#clusterip","text":"type: ClusterIP Default Service type, guarantees unique IP across the cluster. This IP 'lives' only in iptables and is maintained by kube-proxy . By default uses round-robin to pick pods.","title":"ClusterIP"},{"location":"iaac/kubernetes/service/#nodeport","text":"type: NodePort Gives access from outside of the cluster. Opens socket with high port on every node thus allowing to access the service from outside of the cluster using node IP (with that high port)","title":"NodePort"},{"location":"iaac/kubernetes/service/#loadbalancer","text":"type: LoadBalancer Gives access from outside of the cluster. This is cloud-specific. Creates the load-balancer-type resource in cloud provider","title":"LoadBalancer"},{"location":"iaac/kubernetes/volumes/","text":"Essentialy - a directory that will be accessible by POD, preserving state independently of POD lifecycle. Example (simplest) usage of volume apiVersion: v1 kind: Pod metadata: labels: app: pod name: example spec: containers: - image: httpd name: example volumeMounts: - mountPath: /var/tmp name: vartmp volumes: - name: vartmp hostPath: path: /mnt/testing There are numerous volume types that can be used, hostPath is actually the most test-purpose only. It is possible to mount ConfigMap as a volume. Typically you have PersistentVolume (PV) resource (or have some Cloud provider prepare that for you) and you just mount it to the PODs. The persistentVolumeClaim (PVC) is used to represent the request for the storage (PV). There are two main types of Persistent Volumes provisioning: 1. Static It is the administrator that prepares volumes. Use selector in PVC, without providing storageClassName to use static provisioning Dynamic Based on StorageClass the PersistentVolume will be automatically created for given PVC. Use storageClassName . Example POD with PersistentVolume Statically provisioned ( StorageClass is not used at all) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer --- apiVersion: v1 kind: PersistentVolume metadata: name: example-pv labels: name: example-pv spec: capacity: storage: 1Mi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete ##storageClassName: local-storage #hostPath: # path: /mnt/testing local: path: /mnt/testing nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s1 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: example-pvc spec: ##storageClassName: local-storage selector: matchLabels: name: example-pv accessModes: - ReadWriteOnce resources: requests: storage: 1Mi --- apiVersion: v1 kind: Pod metadata: labels: app: pod name: example spec: containers: - image: httpd name: example volumeMounts: - mountPath: /var/tmp name: vartmp volumes: - name: vartmp persistentVolumeClaim: claimName: example-pvc The persistentVolumeReclaimPolicy tells what happens to PersistentVolume after released from claim. Defaults: Retain for statically provisioned PersistentVolume Delete for dynamically provisioned PersistentVolume The PersistentVolume must support different (than default) policy setting.","title":"Volumes"},{"location":"iaac/salt/basics/","text":"Salt is primarily a configuration management solution and remote execution engine. Thanks to the latter, many use-cases can (but not necessarily should) be covered using just this one tool. Basics Term Meaning minion slave, managed host sls salt state file , file that represents the state the system should end up in pillar sensitive data, tree like structure, targeted and securely send to matched minions grain minion data, static information, rarely refreshed on master, contain minion specific data only (like OS name) Full glossary Architecture Event-based, highly modular and highly customizable . It is fairly easy to add your own state modules as well as alter existing ones. Runs: * master-slave (separate master node that provisions minion nodes) * master-less (the node provisions itself) Supports both management modes: * push (the master sends the config updates to minions) * pull (the minions check with master for config updates) Can operate: * with agents (application installed on minion) * agent-less (no application installed on minion) Salt State Files express the desired... state on the provisioned minion. States are matched to minions using (primarily) top.sls file . Each state is then executed on targeted minion (slave). By default minions are targeted using minion id - special generated grain (its default value is the minion hostname). Detailed description how does the state execute on minion is provided in Modules section Information about how to interact with Salt can be found in usage and in scripts sections Details Salt architecture consists of multiple components , it is best to describe them using layered approach: - minion - master - transport - scripts - modules Usage This section contains only brief description of how to interact with Salt using CLI. For more complete overview refer to scripts . Execution Module execution (dissection of the salt command): salt '*' execution_module_name.function_name [arguments_list] [kwargs] - salt is the python script that accepts user commands and passes them to Salt Master process. - '*' selects minions which will execute user function. By default the shell-style globbing is used on minion id. - Find execution_module_name in docs or within Salt sources . Example: salt '*' test.ping State execution (or rather state application): Check what states are going to be applied: salt '*' state.show_top Execute all matching states from given environment: salt '*' state.highstate [saltenv=base] Execute single state: salt '*' state.apply <statename> [saltenv=<env>] References https://repo.saltstack.com/ https://docs.saltstack.com/en/getstarted/config/functions.html https://docs.saltstack.com/en/latest/topics/tutorials/states_pt1.html https://docs.saltstack.com/en/latest/ref/states/highstate.html Great presentation from @carsonoid https://docs.saltstack.com/en/latest/ref/configuration/master.html","title":"Basics"},{"location":"iaac/salt/basics/#basics","text":"Term Meaning minion slave, managed host sls salt state file , file that represents the state the system should end up in pillar sensitive data, tree like structure, targeted and securely send to matched minions grain minion data, static information, rarely refreshed on master, contain minion specific data only (like OS name) Full glossary","title":"Basics"},{"location":"iaac/salt/basics/#architecture","text":"Event-based, highly modular and highly customizable . It is fairly easy to add your own state modules as well as alter existing ones. Runs: * master-slave (separate master node that provisions minion nodes) * master-less (the node provisions itself) Supports both management modes: * push (the master sends the config updates to minions) * pull (the minions check with master for config updates) Can operate: * with agents (application installed on minion) * agent-less (no application installed on minion) Salt State Files express the desired... state on the provisioned minion. States are matched to minions using (primarily) top.sls file . Each state is then executed on targeted minion (slave). By default minions are targeted using minion id - special generated grain (its default value is the minion hostname). Detailed description how does the state execute on minion is provided in Modules section Information about how to interact with Salt can be found in usage and in scripts sections","title":"Architecture"},{"location":"iaac/salt/basics/#details","text":"Salt architecture consists of multiple components , it is best to describe them using layered approach: - minion - master - transport - scripts - modules","title":"Details"},{"location":"iaac/salt/basics/#usage","text":"This section contains only brief description of how to interact with Salt using CLI. For more complete overview refer to scripts . Execution Module execution (dissection of the salt command): salt '*' execution_module_name.function_name [arguments_list] [kwargs] - salt is the python script that accepts user commands and passes them to Salt Master process. - '*' selects minions which will execute user function. By default the shell-style globbing is used on minion id. - Find execution_module_name in docs or within Salt sources . Example: salt '*' test.ping State execution (or rather state application): Check what states are going to be applied: salt '*' state.show_top Execute all matching states from given environment: salt '*' state.highstate [saltenv=base] Execute single state: salt '*' state.apply <statename> [saltenv=<env>]","title":"Usage"},{"location":"iaac/salt/basics/#references","text":"https://repo.saltstack.com/ https://docs.saltstack.com/en/getstarted/config/functions.html https://docs.saltstack.com/en/latest/topics/tutorials/states_pt1.html https://docs.saltstack.com/en/latest/ref/states/highstate.html Great presentation from @carsonoid https://docs.saltstack.com/en/latest/ref/configuration/master.html","title":"References"},{"location":"iaac/salt/configuration/","text":"Configuration Written in YAML files, located in /etc/salt/master and /etc/salt/master.d/ Master configuration: /etc/salt/master Master configuration overrides: /etc/salt/master.d/myoverrides.conf Minion configuration: /etc/salt/minion Minion configuration overrides: /etc/salt/minion.d/myoverrides.conf Overrides require .conf suffix, otherwise they are not included and nothing is logged. States Defined using *.sls files, located in state directories listed in file_roots configuration option ( /srv/salt/ by default). The state directories are also called State Tree Example of state file contents: funny_user_setup: # state ID user.present: # <state module>.<function> - name: cool_guy # <function's> arguments - fullname: Cool Guy Jr - shell: /bin/bash - home: /home/cool_guy Given the above file_roots configuration this state file must be located in /srv/salt/the_state.sls or /srv/salt/the_state/init.sls , (the init.sls is treated as parent directory). The sls filename cannot contain dots (other than suffix .sls ). Filename with dots is expanded by salt as directories, e.g., the_state.something.sls will be understood as /srv/salt/the_state/something.sls . When both /srv/salt/the_state.sls or /srv/salt/the_state/init.sls exist, the the_state/init.sls is ignored. By default the sls file represents python dictionaries, lists, strings and numbers only. Salt also requires special file to exist in top level of State Tree . This special file is responsible for matching states to actual machines. It is called top.sls , example: base: 'machine': - the_state For more information about Top File structure and state definition refer to Targeting and Top file saltenv file_roots configuration contains environment specification. In order to specify multiple environments list them along with their directories containing states: file_roots: base: - /srv/salt/base dev: - /srv/salt/dev It is possible to specify directories multiple times in order to 'reuse' some states from other environments: file_roots: base: - /srv/salt/base dev: - /srv/salt/dev - /srv/salt/base Custom states In order to create your own states, put them in _states under State Tree root. This directory is configured to be synchronized to minions upon state.highstate or saltutil.sync_all calls. Pillar Defined in similar way as state files, located in directories listed in pillar_roots configuration option. There is no enforced pillar data syntax, given default renderer the pillar must be just provided as YAML file. Example pillar data file: some_user_data: mail: cool_guy@o2.io home_dir: /home/cool_guy The top.sls file is also expected to exist in pillar_roots top directory. Custom pillar Documentation refers to custom pillars as external pillars . It is possible to extend Pillar subsystem to fetch data from arbitrary sources. The requirement is to implement: def ext_pillar( minion_id, pillar, *args, **kwargs ): ... return pillar_dict The external pillar must be configured in Salt Master configuration first. Some already existing custom pillars deserve special mentions. git_pillar file_tree ext pillar: file_tree File becomes the value of key Under the root_dir you must have either hosts/minion_id folder or nodegroups/nodegroup folder Data is available to matching hosts. Example configuration: ext_pillar: - file_tree: root_dir: /path/to/root/directory keep_newline: - files/testdir/* gpg renderer The pillar values can be encrypted using GPG. The keypair (without passphrase) must be either setup or imported beforehand on the master node (or on masterless node). It is possible to change default location of gpgkeys using: gpg_keydir configuration option. To create encrypted secret: cat secret_file | gpg --armor --trust-model always -r pillargpg --encrypt --homedir ~/somewhere/the/keys Dunder dictionaries Some of the Salt modules are 'wrapped' within special dunder dictionaries. These special dictionaries provide access to Salt different modules. Dictionary name Available in Information __opts__ Loader modules configuration file contents __salt__ Execution, State, Returner, Runner, SDB modules execution modules __grains__ Execution, State, Returner, External Pillar modules minion grains data __pillar__ Execution, State, Returner modules pillar data __context__ Execution, State modules all-purpose dict, that exists during all state runs Using dunder dictionaries Jinja2+YAML Double underscores are omitted. apache: pkg.installed: - name: {{ salt['pillar.get']('pkgs:apache', 'httpd') }} {% for mnt in salt['cmd.run']('ls /dev/data/moose*').split() %} /mnt/moose{{ mnt[-1] }}: mount.mounted: - device: {{ mnt }} - fstype: xfs - mkmnt: True {% endfor %} py #!py def run(): states = {} swaps = __salt__['mount.swaps']() for swap, dev in swaps.items(): states[\"kubeadm_disable_swap_{}\".format(swap)] = { 'module.run': [ { 'mount.swapoff': [ { 'name': swap }, ]}, { 'require_in': [ { 'pkg': \"kubeadm\" } ]} ] } return states Fileserver All of the above configuration samples assumed filesystem to be used as primary sls storage. Actually fileserver is also a Salt module . Thus it is possible to create your own (place it in _fileserver directory under State Tree root). In order to enable different fileserver, switch them on in master configuration: fileserver_backend: - roots It is possible to specify multiple backends, they are merged together. The sls files that are same within backends are taken in the order of their definition ( fileserver_backend is a list after all). Example: fileserver_backend: - roots - gitfs If the top.sls file exists in both gitfs and roots top directories then the roots one will be used. gitfs Enabled with: fileserver_backend: - gitfs Allows to pull state and pillar definitions from git repositories. Reactor Part of the Salt , that starts different actions upon fired events. The configuration contains the list of handled events with different sls files run as a reaction. reactor: - 'salt/job/*/ret/*': - salt://do_something_after_job.sls - /full/path/to/file.sls By default this is configured in /etc/salt/master and /etc/salt/master.d/reactor.conf . Reactions can be specified using /full/path/notation or relative to State Tree : salt:// Thorium Cloud Extending Salt It is very easy to create your own Salt modules or even alter existing ones. All of the module changes must be synchronized to minions prior to use (this happens automatically only for state.highstate call). By default, extensions must be placed in directories in State Tree root, following naming convention: _<module_type> . It is also possible to define custom extensions in different places, given proper extension_modules configuration. Anything from salt module can be customized. Salt SSH No software needs to be installed on managed host, on the Salt Master side: salt-ssh package is required. Configure /etc/salt/roster file with remote host details, the working roster file: minion: host: minion.local user: dev passwd: password # required even if keys are used sudo: true tty: true minion_opts: log_level: debug log_level_logfile: debug log_file: ../../salt-ssh.log # nice trick from https://twitter.com/SaltTips/status/1146306964026253312 The remote host must have at least python-minimal installed, the passwd is always mandatory (since on the remote the sudo is required) Read more in Salt-SSH","title":"Configuration"},{"location":"iaac/salt/configuration/#configuration","text":"Written in YAML files, located in /etc/salt/master and /etc/salt/master.d/ Master configuration: /etc/salt/master Master configuration overrides: /etc/salt/master.d/myoverrides.conf Minion configuration: /etc/salt/minion Minion configuration overrides: /etc/salt/minion.d/myoverrides.conf Overrides require .conf suffix, otherwise they are not included and nothing is logged.","title":"Configuration"},{"location":"iaac/salt/configuration/#states","text":"Defined using *.sls files, located in state directories listed in file_roots configuration option ( /srv/salt/ by default). The state directories are also called State Tree Example of state file contents: funny_user_setup: # state ID user.present: # <state module>.<function> - name: cool_guy # <function's> arguments - fullname: Cool Guy Jr - shell: /bin/bash - home: /home/cool_guy Given the above file_roots configuration this state file must be located in /srv/salt/the_state.sls or /srv/salt/the_state/init.sls , (the init.sls is treated as parent directory). The sls filename cannot contain dots (other than suffix .sls ). Filename with dots is expanded by salt as directories, e.g., the_state.something.sls will be understood as /srv/salt/the_state/something.sls . When both /srv/salt/the_state.sls or /srv/salt/the_state/init.sls exist, the the_state/init.sls is ignored. By default the sls file represents python dictionaries, lists, strings and numbers only. Salt also requires special file to exist in top level of State Tree . This special file is responsible for matching states to actual machines. It is called top.sls , example: base: 'machine': - the_state For more information about Top File structure and state definition refer to Targeting and Top file","title":"States"},{"location":"iaac/salt/configuration/#saltenv","text":"file_roots configuration contains environment specification. In order to specify multiple environments list them along with their directories containing states: file_roots: base: - /srv/salt/base dev: - /srv/salt/dev It is possible to specify directories multiple times in order to 'reuse' some states from other environments: file_roots: base: - /srv/salt/base dev: - /srv/salt/dev - /srv/salt/base","title":"saltenv"},{"location":"iaac/salt/configuration/#custom-states","text":"In order to create your own states, put them in _states under State Tree root. This directory is configured to be synchronized to minions upon state.highstate or saltutil.sync_all calls.","title":"Custom states"},{"location":"iaac/salt/configuration/#pillar","text":"Defined in similar way as state files, located in directories listed in pillar_roots configuration option. There is no enforced pillar data syntax, given default renderer the pillar must be just provided as YAML file. Example pillar data file: some_user_data: mail: cool_guy@o2.io home_dir: /home/cool_guy The top.sls file is also expected to exist in pillar_roots top directory.","title":"Pillar"},{"location":"iaac/salt/configuration/#custom-pillar","text":"Documentation refers to custom pillars as external pillars . It is possible to extend Pillar subsystem to fetch data from arbitrary sources. The requirement is to implement: def ext_pillar( minion_id, pillar, *args, **kwargs ): ... return pillar_dict The external pillar must be configured in Salt Master configuration first. Some already existing custom pillars deserve special mentions.","title":"Custom pillar"},{"location":"iaac/salt/configuration/#git_pillar","text":"","title":"git_pillar"},{"location":"iaac/salt/configuration/#file_tree","text":"ext pillar: file_tree File becomes the value of key Under the root_dir you must have either hosts/minion_id folder or nodegroups/nodegroup folder Data is available to matching hosts. Example configuration: ext_pillar: - file_tree: root_dir: /path/to/root/directory keep_newline: - files/testdir/*","title":"file_tree"},{"location":"iaac/salt/configuration/#gpg-renderer","text":"The pillar values can be encrypted using GPG. The keypair (without passphrase) must be either setup or imported beforehand on the master node (or on masterless node). It is possible to change default location of gpgkeys using: gpg_keydir configuration option. To create encrypted secret: cat secret_file | gpg --armor --trust-model always -r pillargpg --encrypt --homedir ~/somewhere/the/keys","title":"gpg renderer"},{"location":"iaac/salt/configuration/#dunder-dictionaries","text":"Some of the Salt modules are 'wrapped' within special dunder dictionaries. These special dictionaries provide access to Salt different modules. Dictionary name Available in Information __opts__ Loader modules configuration file contents __salt__ Execution, State, Returner, Runner, SDB modules execution modules __grains__ Execution, State, Returner, External Pillar modules minion grains data __pillar__ Execution, State, Returner modules pillar data __context__ Execution, State modules all-purpose dict, that exists during all state runs","title":"Dunder dictionaries"},{"location":"iaac/salt/configuration/#using-dunder-dictionaries","text":"","title":"Using dunder dictionaries"},{"location":"iaac/salt/configuration/#jinja2yaml","text":"Double underscores are omitted. apache: pkg.installed: - name: {{ salt['pillar.get']('pkgs:apache', 'httpd') }} {% for mnt in salt['cmd.run']('ls /dev/data/moose*').split() %} /mnt/moose{{ mnt[-1] }}: mount.mounted: - device: {{ mnt }} - fstype: xfs - mkmnt: True {% endfor %}","title":"Jinja2+YAML"},{"location":"iaac/salt/configuration/#py","text":"#!py def run(): states = {} swaps = __salt__['mount.swaps']() for swap, dev in swaps.items(): states[\"kubeadm_disable_swap_{}\".format(swap)] = { 'module.run': [ { 'mount.swapoff': [ { 'name': swap }, ]}, { 'require_in': [ { 'pkg': \"kubeadm\" } ]} ] } return states","title":"py"},{"location":"iaac/salt/configuration/#fileserver","text":"All of the above configuration samples assumed filesystem to be used as primary sls storage. Actually fileserver is also a Salt module . Thus it is possible to create your own (place it in _fileserver directory under State Tree root). In order to enable different fileserver, switch them on in master configuration: fileserver_backend: - roots It is possible to specify multiple backends, they are merged together. The sls files that are same within backends are taken in the order of their definition ( fileserver_backend is a list after all). Example: fileserver_backend: - roots - gitfs If the top.sls file exists in both gitfs and roots top directories then the roots one will be used.","title":"Fileserver"},{"location":"iaac/salt/configuration/#gitfs","text":"Enabled with: fileserver_backend: - gitfs Allows to pull state and pillar definitions from git repositories.","title":"gitfs"},{"location":"iaac/salt/configuration/#reactor","text":"Part of the Salt , that starts different actions upon fired events. The configuration contains the list of handled events with different sls files run as a reaction. reactor: - 'salt/job/*/ret/*': - salt://do_something_after_job.sls - /full/path/to/file.sls By default this is configured in /etc/salt/master and /etc/salt/master.d/reactor.conf . Reactions can be specified using /full/path/notation or relative to State Tree : salt://","title":"Reactor"},{"location":"iaac/salt/configuration/#thorium","text":"","title":"Thorium"},{"location":"iaac/salt/configuration/#cloud","text":"","title":"Cloud"},{"location":"iaac/salt/configuration/#extending-salt","text":"It is very easy to create your own Salt modules or even alter existing ones. All of the module changes must be synchronized to minions prior to use (this happens automatically only for state.highstate call). By default, extensions must be placed in directories in State Tree root, following naming convention: _<module_type> . It is also possible to define custom extensions in different places, given proper extension_modules configuration. Anything from salt module can be customized.","title":"Extending Salt"},{"location":"iaac/salt/configuration/#salt-ssh","text":"No software needs to be installed on managed host, on the Salt Master side: salt-ssh package is required. Configure /etc/salt/roster file with remote host details, the working roster file: minion: host: minion.local user: dev passwd: password # required even if keys are used sudo: true tty: true minion_opts: log_level: debug log_level_logfile: debug log_file: ../../salt-ssh.log # nice trick from https://twitter.com/SaltTips/status/1146306964026253312 The remote host must have at least python-minimal installed, the passwd is always mandatory (since on the remote the sudo is required) Read more in Salt-SSH","title":"Salt SSH"},{"location":"iaac/salt/events-and-reactor/","text":"Reactor System Not a Module, configured separately , native part of Salt , used to react to different events. Reactor is the go-to place to create 'self-healing' or 'fully-automated' solutions. Reactions are matched on the Salt Master . The reaction sls files follow the same rules of compiling (they have default Jinja+YAML renderer), however they are limited in terms of available Dunder Dictionaries . Additional: data dictionary is available for reactions which contains event data. Reaction types The sls reaction files must contain reaction type in front of execution_module_name.function_name Local reaction Runs Execution Module on targeted minions (not necessarily a \"local\"). Example: highstate: local.state.highstate: - tgt: {{ data['id'] }} This example runs highstate on data['id'] minion Runner reaction Runs Runner Modules on the Salt Master . Centralized Salt Masters view allows to create complex flows easily. Most widely used Runner Module for this purpose is the Orchestrate Runner . redis_cluster_orchestrate: runner.state.orchestrate: - args: - mods: - redis.server._orchestrate - saltenv: server Orchestrate Runner In Salt terminology, the highstate is a collection of states applied to one minion. Collection of states applied to multiple minions with inter-minion dependencies could be called Orchestration . Orchestrate is more generic term than highstate . As this is Runner Module it can be directly called from CLI: salt-run state.orchestrate kubernetes._orchestrate.cluster saltenv=server Combined with Salt event system allows to create multi minion-aware reactions. state.orchestrate Module accepts mods argument, this is the sls file list with actual orchestration logic. Example of redis.server._orchestrate : refresh_pillar: salt.function: - name: saltutil.pillar_refresh_synchronous # saltutil.pillar_refresh_synchronous doesn't exist, see below for explanation - tgt: {{ salt['pillar.get'](\"redis:coordinator\") }} cluster_met: salt.state: - tgt: {{ salt['pillar.get'](\"redis:coordinator\") }} - sls: - \"redis.server._orchestrate.met\" - queue: True - require: - salt: refresh_pillar # some more logic # ... Orchestrate Runner accepts other sls'es evaluates them on Salt Master and invokes them on desired targets. These sls'es contain regular salt states/functions or even Runner Modules . To simplify: 1. Some situation triggers event 2. Event is propagated to Salt Master 3. Salt Master checks if it can find reaction 4. Reaction is rendered if found. 5. Reaction executes Runner Orchestrate Module if runner.state.orchestrate 6. Runner Orchestrate Module renders mods on the Salt Master 7. Runner Orchestrate Module executes functions on desired targets. The most typical orchestrate sls files will comprise mostly of salt.[function|state] calls as they accept the tgt parameter and thus can delegate the call to minions. Aforementioned example contains unfortunate 'gotcha' in Salt. Typically Salt Master may want Minions to refresh the Pillar data prior to invoking desired states. Thus calling saltutil.pillar_refresh prior to the states execution seems like viable solution. However it may (and usually will) not work, because pillar_refresh function actually doesn't refresh the Pillar on Salt Minion . This particular function is asynchronous by default - which is inconsistent with most of the states that are synchronous. Wheel reaction Runs Wheel Modules on the Salt Master Caller reaction Used for Masterless Minions. The minion must be properly configured . Runs Execution Modules on the minion Reactor state files limitations Matching and redering reaction sls files is done sequentially in single process. Because of this, the reaction sls files should contain very few reactions. Also heavy jinja logic within reaction sls files can choke whole Reactor System . Reactor doesn't support require or other requisite statements. Pillar and grain data are not available. Thus any time, some complex logic is required to handle event the state.orchestrate should be used as a reaction. Example of typical flow (logic moved to orchestrator)","title":"Events/Reactor"},{"location":"iaac/salt/events-and-reactor/#reactor-system","text":"Not a Module, configured separately , native part of Salt , used to react to different events. Reactor is the go-to place to create 'self-healing' or 'fully-automated' solutions. Reactions are matched on the Salt Master . The reaction sls files follow the same rules of compiling (they have default Jinja+YAML renderer), however they are limited in terms of available Dunder Dictionaries . Additional: data dictionary is available for reactions which contains event data.","title":"Reactor System"},{"location":"iaac/salt/events-and-reactor/#reaction-types","text":"The sls reaction files must contain reaction type in front of execution_module_name.function_name","title":"Reaction types"},{"location":"iaac/salt/events-and-reactor/#local-reaction","text":"Runs Execution Module on targeted minions (not necessarily a \"local\"). Example: highstate: local.state.highstate: - tgt: {{ data['id'] }} This example runs highstate on data['id'] minion","title":"Local reaction"},{"location":"iaac/salt/events-and-reactor/#runner-reaction","text":"Runs Runner Modules on the Salt Master . Centralized Salt Masters view allows to create complex flows easily. Most widely used Runner Module for this purpose is the Orchestrate Runner . redis_cluster_orchestrate: runner.state.orchestrate: - args: - mods: - redis.server._orchestrate - saltenv: server","title":"Runner reaction"},{"location":"iaac/salt/events-and-reactor/#orchestrate-runner","text":"In Salt terminology, the highstate is a collection of states applied to one minion. Collection of states applied to multiple minions with inter-minion dependencies could be called Orchestration . Orchestrate is more generic term than highstate . As this is Runner Module it can be directly called from CLI: salt-run state.orchestrate kubernetes._orchestrate.cluster saltenv=server Combined with Salt event system allows to create multi minion-aware reactions. state.orchestrate Module accepts mods argument, this is the sls file list with actual orchestration logic. Example of redis.server._orchestrate : refresh_pillar: salt.function: - name: saltutil.pillar_refresh_synchronous # saltutil.pillar_refresh_synchronous doesn't exist, see below for explanation - tgt: {{ salt['pillar.get'](\"redis:coordinator\") }} cluster_met: salt.state: - tgt: {{ salt['pillar.get'](\"redis:coordinator\") }} - sls: - \"redis.server._orchestrate.met\" - queue: True - require: - salt: refresh_pillar # some more logic # ... Orchestrate Runner accepts other sls'es evaluates them on Salt Master and invokes them on desired targets. These sls'es contain regular salt states/functions or even Runner Modules . To simplify: 1. Some situation triggers event 2. Event is propagated to Salt Master 3. Salt Master checks if it can find reaction 4. Reaction is rendered if found. 5. Reaction executes Runner Orchestrate Module if runner.state.orchestrate 6. Runner Orchestrate Module renders mods on the Salt Master 7. Runner Orchestrate Module executes functions on desired targets. The most typical orchestrate sls files will comprise mostly of salt.[function|state] calls as they accept the tgt parameter and thus can delegate the call to minions. Aforementioned example contains unfortunate 'gotcha' in Salt. Typically Salt Master may want Minions to refresh the Pillar data prior to invoking desired states. Thus calling saltutil.pillar_refresh prior to the states execution seems like viable solution. However it may (and usually will) not work, because pillar_refresh function actually doesn't refresh the Pillar on Salt Minion . This particular function is asynchronous by default - which is inconsistent with most of the states that are synchronous.","title":"Orchestrate Runner"},{"location":"iaac/salt/events-and-reactor/#wheel-reaction","text":"Runs Wheel Modules on the Salt Master","title":"Wheel reaction"},{"location":"iaac/salt/events-and-reactor/#caller-reaction","text":"Used for Masterless Minions. The minion must be properly configured . Runs Execution Modules on the minion","title":"Caller reaction"},{"location":"iaac/salt/events-and-reactor/#reactor-state-files-limitations","text":"Matching and redering reaction sls files is done sequentially in single process. Because of this, the reaction sls files should contain very few reactions. Also heavy jinja logic within reaction sls files can choke whole Reactor System . Reactor doesn't support require or other requisite statements. Pillar and grain data are not available. Thus any time, some complex logic is required to handle event the state.orchestrate should be used as a reaction. Example of typical flow (logic moved to orchestrator)","title":"Reactor state files limitations"},{"location":"iaac/salt/master/","text":"Responsible for managing the minions. Provides the states and the pillar data. States are send and rendered on minions, pillars are send and cached on minions. In order to match states and pillar data that should be send to given minions the targeting is used. Similarly, to Salt Minion the Salt Master also has ID, which is equal to hostname with _master suffix. It is also possible to override this setting in the master config: id Targeting and Top file Targeting is specifying which minions should execute given state as well as contain given pillar data. Typically targeting is contained in top.sls file. base: # environment 'minion_id': # target - some_state # states top.sls consists of three parts: - environment ( saltenv ) - main one, contains targets - target - contains states - states Environments aka saltenv Most often used to isolate operations within different... environments, e.g., dev , test , prod . Requires additional configuration Helps to: - Group minions by their role prod: 'prod*': - some_state However it is possible to achieve this without multiple environments (by adding proper grains on minions): base: 'role:prod': - match: grain - some_state The role grain on the minion has drawback. It is obvious by looking at such grain that it expresses membership. Thus when such minion is compromised it is trivial to change its value to something that may reveal too much data to attacker. The first approach is free of this flaw, if attacker tinkers with id grain, the minion would have to be accepted on Salt Master - Group states by their purpose. Helps to organize the states Targeting By default the shell-style globbing on minion id ( id is a grain data) is used, e.g.: salt 'minion_id' test.ping or base: 'minion_id': - some_state List of all targeting options Targeting with grains Grains are uploaded upon first contact and in general should not change, thus the grain targeting is safe. salt -G 'your:grain:path:value' test.ping , .e.g.: salt -G 'os:Debian' test.ping Targeting with pillar Pillar is actually cached not only on Salt Minion but on Salt Master too, in order to use pillar targeting, the pillar data must be refreshed on master: 1. salt '*' saltutil.pillar_refresh 2. salt -I 'some:pillar:value' test.ping , or salt -I 'some:pillar:value_prefix*' test.ping Compound targeting Allows to mix all of the options using slightly different syntax: salt -C 'G@os:Debian and I@redis:setup_type:cluster' test.ping Find all of the available prefixes to be used here States States can be defined using arbitrary syntax as long as proper renderer is present. In order to specify different renderer, use shebang with renderer name. #!py def run(): ... Default renderer uses Jinja2+YAML (order matters). States that are defined like in example wouldn't be of much use, they are too static. States should use pillar and grain data to allow flexibility of configuration. Pillar Based on everything the Salt Master already knows about the minion and minion grain data the Salt Master creates the pillar data and sends it over to minion. Pillar is managed similarly to state files. It contains its own top.sls with data to minion matching and the actual pillar data What to store in pillar data: - secrets - minion configuration - any data... this is the place where all of the variables and configs should be stored It is even possible to include Salt Master configuration files in the pillar data. In /etc/salt/master.d/custom.conf the setting: pillar_opts: True controls this. Environments aka pillarenv Similarly to saltenv , pillarenv exists. The purpose is almost the same: to group pillar files within environments. However... by default minion fetches pillar data from all matching environments, thus defeating the purpose of pillarenv . Setting pillarenv in the minion configuration changes this behavior to select only this one defined environment. CLI Inspect whole minion pillar data: salt 'my_minion' pillar.items Get pillar value: salt 'my_minion' pillar.get my:nested:or_not_key","title":"Master"},{"location":"iaac/salt/master/#targeting-and-top-file","text":"Targeting is specifying which minions should execute given state as well as contain given pillar data. Typically targeting is contained in top.sls file. base: # environment 'minion_id': # target - some_state # states top.sls consists of three parts: - environment ( saltenv ) - main one, contains targets - target - contains states - states","title":"Targeting and Top file"},{"location":"iaac/salt/master/#environments-aka-saltenv","text":"Most often used to isolate operations within different... environments, e.g., dev , test , prod . Requires additional configuration Helps to: - Group minions by their role prod: 'prod*': - some_state However it is possible to achieve this without multiple environments (by adding proper grains on minions): base: 'role:prod': - match: grain - some_state The role grain on the minion has drawback. It is obvious by looking at such grain that it expresses membership. Thus when such minion is compromised it is trivial to change its value to something that may reveal too much data to attacker. The first approach is free of this flaw, if attacker tinkers with id grain, the minion would have to be accepted on Salt Master - Group states by their purpose. Helps to organize the states","title":"Environments aka saltenv"},{"location":"iaac/salt/master/#targeting","text":"By default the shell-style globbing on minion id ( id is a grain data) is used, e.g.: salt 'minion_id' test.ping or base: 'minion_id': - some_state List of all targeting options","title":"Targeting"},{"location":"iaac/salt/master/#targeting-with-grains","text":"Grains are uploaded upon first contact and in general should not change, thus the grain targeting is safe. salt -G 'your:grain:path:value' test.ping , .e.g.: salt -G 'os:Debian' test.ping","title":"Targeting with grains"},{"location":"iaac/salt/master/#targeting-with-pillar","text":"Pillar is actually cached not only on Salt Minion but on Salt Master too, in order to use pillar targeting, the pillar data must be refreshed on master: 1. salt '*' saltutil.pillar_refresh 2. salt -I 'some:pillar:value' test.ping , or salt -I 'some:pillar:value_prefix*' test.ping","title":"Targeting with pillar"},{"location":"iaac/salt/master/#compound-targeting","text":"Allows to mix all of the options using slightly different syntax: salt -C 'G@os:Debian and I@redis:setup_type:cluster' test.ping Find all of the available prefixes to be used here","title":"Compound targeting"},{"location":"iaac/salt/master/#states","text":"States can be defined using arbitrary syntax as long as proper renderer is present. In order to specify different renderer, use shebang with renderer name. #!py def run(): ... Default renderer uses Jinja2+YAML (order matters). States that are defined like in example wouldn't be of much use, they are too static. States should use pillar and grain data to allow flexibility of configuration.","title":"States"},{"location":"iaac/salt/master/#pillar","text":"Based on everything the Salt Master already knows about the minion and minion grain data the Salt Master creates the pillar data and sends it over to minion. Pillar is managed similarly to state files. It contains its own top.sls with data to minion matching and the actual pillar data What to store in pillar data: - secrets - minion configuration - any data... this is the place where all of the variables and configs should be stored It is even possible to include Salt Master configuration files in the pillar data. In /etc/salt/master.d/custom.conf the setting: pillar_opts: True controls this.","title":"Pillar"},{"location":"iaac/salt/master/#environments-aka-pillarenv","text":"Similarly to saltenv , pillarenv exists. The purpose is almost the same: to group pillar files within environments. However... by default minion fetches pillar data from all matching environments, thus defeating the purpose of pillarenv . Setting pillarenv in the minion configuration changes this behavior to select only this one defined environment.","title":"Environments aka pillarenv"},{"location":"iaac/salt/master/#cli","text":"Inspect whole minion pillar data: salt 'my_minion' pillar.items Get pillar value: salt 'my_minion' pillar.get my:nested:or_not_key","title":"CLI"},{"location":"iaac/salt/minion/","text":"Agent daemon that runs on the remote (managed) node. Receives commands from the Salt Master and responds with the results, can operate autonomously as well. Runs directly on top of the OS (Windows, Linux, Mac), requires python (2.7 or 3.5). Identified by ID which is equal to hostname by default. Override in settings: id: desired_id If during the minion lifetime the hostname changes and there is no id setting in the config the minion ID is not updated Operation upon first minion start the ID is generated using following procedure minion generates keypair sends public key to the Salt Master waits for Salt Master to accept the key (there are multiple ways to accept the key, e.g., salt-key ) Salt Master generates symmetric key, encrypts it with Salt Minion public key and sends back Salt Minion has established secure connection Salt Minion sends grains via secure channel Salt Minion is ready to accept master requests, all further state file rendering happens on minion","title":"Minion"},{"location":"iaac/salt/minion/#operation","text":"upon first minion start the ID is generated using following procedure minion generates keypair sends public key to the Salt Master waits for Salt Master to accept the key (there are multiple ways to accept the key, e.g., salt-key ) Salt Master generates symmetric key, encrypts it with Salt Minion public key and sends back Salt Minion has established secure connection Salt Minion sends grains via secure channel Salt Minion is ready to accept master requests, all further state file rendering happens on minion","title":"Operation"},{"location":"iaac/salt/modules/","text":"Ambiguous term in salt documentation, overused to describe any piece of salt logic. The *.sls files contains state definitions, this is just the data file (YAML by default) the Salt translates into State Module execution Core Modules Execution Modules salt/modules Custom: salt://_modules The salt or salt-call command executes exactly the Execution Modules Contrary to State Modules , Execution Modules don't check any state, they just perform action. salt '*' cmd.run \"ls /\" cwd=/ cmd - the module name, the module name is either the salt/modules/cmd.py or salt/modules/any_name.py containing __virtualname__ = \"cmd\" run - the actual function definition, everything after function definition is the args and kwargs (for the actual function or for the salt state compiler) \"ls /\" - first positional argument of run function cwd=/ - keyword argument State Modules salt/states Custom: salt://_states Enforces desired states on the remote. Under the hood they usually use Execution Modules pkgs_pip: pip.installed: - name: pip_packages - pkgs: - google-auth - reload_modules: True pkgs_pip - state ID pip - State Module name, maps directly to either salt/states/pip.py or salt/states/any_name.py containing __virtualname__ = \"pip\" installed - the actual function name, naming convention: past tense. - name: pip_packages the first positional argument, by convention called name . When the name is not passed the state ID is used as the name Ordering states Salt supports two ordering modes: 1. Definition order : in the order of appearance in top.sls file, thus it is the filename that determines order. Then the states are executed by the order of appearance in the state file itself. The include d files that were defined after including file are executed first. inculde takes precedence. 2. Lexicographic order : states are sorted by: their name , function and then by state ID . Enable with configuration option: state_auto_order: False , the include statement in sls files doesn't affect the order at all. Both options respect the requisite statements . The requisites take precedence over configured ordering. Full list of requisite statements and their usage . If multiple states happen to be assigned same order number (the internal number that formally determines execution order), then Salt fallbacks to Lexicographic ordering . Evaluating states By default sls files are written using in YAML format with Jinja templates. Both engines are not aware of each other. Jinja templating starts before validating YAML. In other words: 1. Jinja must produce valid YAML file 2. YAML file must be a valid highstate data structure 3. Highstate is compiled to lowstate (lowstate is a sorted list of state executions) 4. Salt executes the list in the order The full evaluation and execution order: Jinja -> YAML -> highstate -> lowstate -> execution It is very easy to misuse jinja. When the state starts to be unreadable, it is possible candidate to switch to different renderer (usually #!py ). However user desired logic may be too complex still. Then writing custom Execution Module or State Module is a better idea. Additionally as the State Tree grows, it is easy to fall into following trap (depicted by example): clone_repo: git.latest: - name: https://github.com/kiemlicz/util - target: /tmp/util/ - branch: master {% for f in '/tmp/util/' | list_files %} {% if f == '/tmp/util/README.md' %} add_developer_{{ f }}: file.append: - name: {{ f }} - text: \"added contributor: bla@o2.pl\" - require: - git: clone_repo {% endif %} {% endfor %} User assumed that the jinja will 'see' clone_repo changes. It is not true. Jinja is evaluated first, thus when this sls file is applied first time, effectively the YAML looks like: clone_repo: git.latest: - name: https://github.com/kiemlicz/util - target: /tmp/util/ - branch: master However during next run, the jinja will 'see' the changes (as they are already applied). Thus the output YAML will become: clone_repo: git.latest: - name: https://github.com/kiemlicz/util - target: /tmp/util/ - branch: master add_developer_/tmp/util/README.md: file.append: - name: /tmp/util/README.md - text: \"added contributor: bla@o2.pl\" - require: - git: clone_repo There are couple of options how to overcome such situation, most common involve: - writing custom Execution Module or State Module - using Slots (if you know what you are doing) Slots Relatively new Salt feature, allows to store the result of Execution Module and use it in next Modules (during same run). Example: dnsutils: pkg.latest: - name: dnsutils find_domain: # works cmd.run: - name: \"nslookup google.com\" the_state_id: # fails test.show_notification: - name: some name - text: \"the server ip: {{ salt['cmd.run'](\"nslookup google.com\") }}\" Will fail because not-yet-existing command output is used as part of state definition. Using slots we can overcome this: dnsutils: pkg.latest: - name: dnsutils find_domain: # works cmd.run: - name: \"nslookup google.com\" the_state_id: # works test.show_notification: - name: some name - text: __slot__:salt:cmd.run(\"nslookup google.com\") Idempotence It is tempting to wonder: is the state execution idempotent? Is depends, e.g., the states like: run_scripts: cmd.script: - name: do_dangerous_stuff.sh are not guaranteed to be idempotent because everything depends on underlying: do_dangerous_stuff.sh script, so in general it is wise to assume that such state is not idempotent. Moreover adding jinja constructs that modify underlying system also doesn't help (not advised though). However Salt provides requisite constructs that can be added to (not all) states: onlyif or unless Thus it is possible to make Salt states idempotent Best practices Check these two documents, they provide excellent details about how to create state properly: - best practices - formulas best practices Data Modules Contains runtime configuration, variables, secret data... data... Data\\Authority Salt Master Salt Minion Other Secrets Pillar SDB SDB Config Pillar Grains SDB Grains Modules salt/grains Custom: salt://_grains Minion specific data, can be also specified in configuration file grains: {} , or set by master. Grains are refreshed on a very limited basis and are largely static data. If there is some minion specific data that needs to be updated on the master then the Salt Mine is the place to go. Pillar Modules salt/pillar Custom: salt://_pillar Salt Master is authoritative over pillar data. Pushes pillar to minions that cache it. Minion may request pillar data on its own. SDB Modules salt/sdb Custom: salt://_sdb Used when neither Salt Master nor Salt Minion is authoritative over data. It could be used to pull secrets from HashiCorp Vault or other keystores. If it is Salt Minion that makes the call to sdb it calls directly the third party entity. It is possible to use SDB modules in the Salt Master/Minion config files: client_id: sdb://module_name/secret_client_id However it's impossible to bootstrap Salt with custom SDB modules used in config already. During the Salt Minion/Master startup the full config is read and parsed, thus any sdb://<profile>/key are evaluated Use of custom SDB modules requires preceding: sync_sdb , which doesn't happen during initial bootstrap Event Modules and Reactor System Salt Master and Salt Minion have their own event buses. Depending on the Module's function used to fire event, event may or may not be propagated to other event buses (e.g. from Minion to Master and vice-versa). Event always comprises of two things: 1. event tag 2. data dictionary Salt event system that uses Event Modules is described in separate section Beacon Modules salt/beacons Custom: salt://_beacons Way to notify the master about anything. Works like a probe/sensor, e.g. disk is going full. Notifications use the Salt Minion 's event bus and are propagated to Salt Master . Internally beacons work in the following way: - minion's scheduler starts the beacon module's beacon function - the function fetches desired data in the most lightweight way possible - data is forwarded to Salt Minion event bus Queue Modules salt/queues Custom: n/a Helps to handle the events, sometimes it is desirable to enqueue incoming events and pop them sequentially instead of allowing asynchronous reactions to happen. Engine Modules salt/engines Custom: salt://_engines Can be run on Salt Master or Salt Minion , once started: runs forever in separate process. Commonly used to integrate with external systems (like sending the notifications to slack) or fetching the external systems data under the Salt infrastructure. Thorium Modules - experimental salt/thorium Custom: salt://_thorium Primarily created to add event aggregation, requires additional configuration . Sometimes it is desirable to start a reaction once the set of N minions complete their highstate logic, not every time each of the minions completes. This can be achieved with Thorium. Example of thorium state file that fires the event only when two salt/custom/event are received: something: reg.list: - add: \"some_field\" - match: 'salt/custom/event' check.len_eq: - value: 2 send_when: runner.cmd: - func: event.send - arg: - thor/works - require: - check: something Result Modules TODO Output Modules Result Modules Admin Modules Wheel Modules salt/wheel Custom: n/a Dealing with Salt infrastructure itself, e.g., accept Salt Minion key. Runner Modules salt/runners Custom: salt://_runners ( runner_dirs configuration option) Used exclusively by salt-run command. They are pure Salt Master Modules, designed to run on master only. Cache Modules salt/cache Custom: salt://_cache Netapi Modules salt/netapi Modules that expose Salt API, require configuration to enable Python client API This is not the part of Salt's netapi , but a library for interaction with Salt Same that is used by Salt commands Integration Modules TODO Utility Modules TODO","title":"Modules"},{"location":"iaac/salt/modules/#core-modules","text":"","title":"Core Modules"},{"location":"iaac/salt/modules/#execution-modules","text":"salt/modules Custom: salt://_modules The salt or salt-call command executes exactly the Execution Modules Contrary to State Modules , Execution Modules don't check any state, they just perform action. salt '*' cmd.run \"ls /\" cwd=/ cmd - the module name, the module name is either the salt/modules/cmd.py or salt/modules/any_name.py containing __virtualname__ = \"cmd\" run - the actual function definition, everything after function definition is the args and kwargs (for the actual function or for the salt state compiler) \"ls /\" - first positional argument of run function cwd=/ - keyword argument","title":"Execution Modules"},{"location":"iaac/salt/modules/#state-modules","text":"salt/states Custom: salt://_states Enforces desired states on the remote. Under the hood they usually use Execution Modules pkgs_pip: pip.installed: - name: pip_packages - pkgs: - google-auth - reload_modules: True pkgs_pip - state ID pip - State Module name, maps directly to either salt/states/pip.py or salt/states/any_name.py containing __virtualname__ = \"pip\" installed - the actual function name, naming convention: past tense. - name: pip_packages the first positional argument, by convention called name . When the name is not passed the state ID is used as the name","title":"State Modules"},{"location":"iaac/salt/modules/#ordering-states","text":"Salt supports two ordering modes: 1. Definition order : in the order of appearance in top.sls file, thus it is the filename that determines order. Then the states are executed by the order of appearance in the state file itself. The include d files that were defined after including file are executed first. inculde takes precedence. 2. Lexicographic order : states are sorted by: their name , function and then by state ID . Enable with configuration option: state_auto_order: False , the include statement in sls files doesn't affect the order at all. Both options respect the requisite statements . The requisites take precedence over configured ordering. Full list of requisite statements and their usage . If multiple states happen to be assigned same order number (the internal number that formally determines execution order), then Salt fallbacks to Lexicographic ordering .","title":"Ordering states"},{"location":"iaac/salt/modules/#evaluating-states","text":"By default sls files are written using in YAML format with Jinja templates. Both engines are not aware of each other. Jinja templating starts before validating YAML. In other words: 1. Jinja must produce valid YAML file 2. YAML file must be a valid highstate data structure 3. Highstate is compiled to lowstate (lowstate is a sorted list of state executions) 4. Salt executes the list in the order The full evaluation and execution order: Jinja -> YAML -> highstate -> lowstate -> execution It is very easy to misuse jinja. When the state starts to be unreadable, it is possible candidate to switch to different renderer (usually #!py ). However user desired logic may be too complex still. Then writing custom Execution Module or State Module is a better idea. Additionally as the State Tree grows, it is easy to fall into following trap (depicted by example): clone_repo: git.latest: - name: https://github.com/kiemlicz/util - target: /tmp/util/ - branch: master {% for f in '/tmp/util/' | list_files %} {% if f == '/tmp/util/README.md' %} add_developer_{{ f }}: file.append: - name: {{ f }} - text: \"added contributor: bla@o2.pl\" - require: - git: clone_repo {% endif %} {% endfor %} User assumed that the jinja will 'see' clone_repo changes. It is not true. Jinja is evaluated first, thus when this sls file is applied first time, effectively the YAML looks like: clone_repo: git.latest: - name: https://github.com/kiemlicz/util - target: /tmp/util/ - branch: master However during next run, the jinja will 'see' the changes (as they are already applied). Thus the output YAML will become: clone_repo: git.latest: - name: https://github.com/kiemlicz/util - target: /tmp/util/ - branch: master add_developer_/tmp/util/README.md: file.append: - name: /tmp/util/README.md - text: \"added contributor: bla@o2.pl\" - require: - git: clone_repo There are couple of options how to overcome such situation, most common involve: - writing custom Execution Module or State Module - using Slots (if you know what you are doing)","title":"Evaluating states"},{"location":"iaac/salt/modules/#slots","text":"Relatively new Salt feature, allows to store the result of Execution Module and use it in next Modules (during same run). Example: dnsutils: pkg.latest: - name: dnsutils find_domain: # works cmd.run: - name: \"nslookup google.com\" the_state_id: # fails test.show_notification: - name: some name - text: \"the server ip: {{ salt['cmd.run'](\"nslookup google.com\") }}\" Will fail because not-yet-existing command output is used as part of state definition. Using slots we can overcome this: dnsutils: pkg.latest: - name: dnsutils find_domain: # works cmd.run: - name: \"nslookup google.com\" the_state_id: # works test.show_notification: - name: some name - text: __slot__:salt:cmd.run(\"nslookup google.com\")","title":"Slots"},{"location":"iaac/salt/modules/#idempotence","text":"It is tempting to wonder: is the state execution idempotent? Is depends, e.g., the states like: run_scripts: cmd.script: - name: do_dangerous_stuff.sh are not guaranteed to be idempotent because everything depends on underlying: do_dangerous_stuff.sh script, so in general it is wise to assume that such state is not idempotent. Moreover adding jinja constructs that modify underlying system also doesn't help (not advised though). However Salt provides requisite constructs that can be added to (not all) states: onlyif or unless Thus it is possible to make Salt states idempotent","title":"Idempotence"},{"location":"iaac/salt/modules/#best-practices","text":"Check these two documents, they provide excellent details about how to create state properly: - best practices - formulas best practices","title":"Best practices"},{"location":"iaac/salt/modules/#data-modules","text":"Contains runtime configuration, variables, secret data... data... Data\\Authority Salt Master Salt Minion Other Secrets Pillar SDB SDB Config Pillar Grains SDB","title":"Data Modules"},{"location":"iaac/salt/modules/#grains-modules","text":"salt/grains Custom: salt://_grains Minion specific data, can be also specified in configuration file grains: {} , or set by master. Grains are refreshed on a very limited basis and are largely static data. If there is some minion specific data that needs to be updated on the master then the Salt Mine is the place to go.","title":"Grains Modules"},{"location":"iaac/salt/modules/#pillar-modules","text":"salt/pillar Custom: salt://_pillar Salt Master is authoritative over pillar data. Pushes pillar to minions that cache it. Minion may request pillar data on its own.","title":"Pillar Modules"},{"location":"iaac/salt/modules/#sdb-modules","text":"salt/sdb Custom: salt://_sdb Used when neither Salt Master nor Salt Minion is authoritative over data. It could be used to pull secrets from HashiCorp Vault or other keystores. If it is Salt Minion that makes the call to sdb it calls directly the third party entity. It is possible to use SDB modules in the Salt Master/Minion config files: client_id: sdb://module_name/secret_client_id However it's impossible to bootstrap Salt with custom SDB modules used in config already. During the Salt Minion/Master startup the full config is read and parsed, thus any sdb://<profile>/key are evaluated Use of custom SDB modules requires preceding: sync_sdb , which doesn't happen during initial bootstrap","title":"SDB Modules"},{"location":"iaac/salt/modules/#event-modules-and-reactor-system","text":"Salt Master and Salt Minion have their own event buses. Depending on the Module's function used to fire event, event may or may not be propagated to other event buses (e.g. from Minion to Master and vice-versa). Event always comprises of two things: 1. event tag 2. data dictionary Salt event system that uses Event Modules is described in separate section","title":"Event Modules and Reactor System"},{"location":"iaac/salt/modules/#beacon-modules","text":"salt/beacons Custom: salt://_beacons Way to notify the master about anything. Works like a probe/sensor, e.g. disk is going full. Notifications use the Salt Minion 's event bus and are propagated to Salt Master . Internally beacons work in the following way: - minion's scheduler starts the beacon module's beacon function - the function fetches desired data in the most lightweight way possible - data is forwarded to Salt Minion event bus","title":"Beacon Modules"},{"location":"iaac/salt/modules/#queue-modules","text":"salt/queues Custom: n/a Helps to handle the events, sometimes it is desirable to enqueue incoming events and pop them sequentially instead of allowing asynchronous reactions to happen.","title":"Queue Modules"},{"location":"iaac/salt/modules/#engine-modules","text":"salt/engines Custom: salt://_engines Can be run on Salt Master or Salt Minion , once started: runs forever in separate process. Commonly used to integrate with external systems (like sending the notifications to slack) or fetching the external systems data under the Salt infrastructure.","title":"Engine Modules"},{"location":"iaac/salt/modules/#thorium-modules-experimental","text":"salt/thorium Custom: salt://_thorium Primarily created to add event aggregation, requires additional configuration . Sometimes it is desirable to start a reaction once the set of N minions complete their highstate logic, not every time each of the minions completes. This can be achieved with Thorium. Example of thorium state file that fires the event only when two salt/custom/event are received: something: reg.list: - add: \"some_field\" - match: 'salt/custom/event' check.len_eq: - value: 2 send_when: runner.cmd: - func: event.send - arg: - thor/works - require: - check: something","title":"Thorium Modules - experimental"},{"location":"iaac/salt/modules/#result-modules","text":"TODO","title":"Result Modules"},{"location":"iaac/salt/modules/#output-modules","text":"","title":"Output Modules"},{"location":"iaac/salt/modules/#result-modules_1","text":"","title":"Result Modules"},{"location":"iaac/salt/modules/#admin-modules","text":"","title":"Admin Modules"},{"location":"iaac/salt/modules/#wheel-modules","text":"salt/wheel Custom: n/a Dealing with Salt infrastructure itself, e.g., accept Salt Minion key.","title":"Wheel Modules"},{"location":"iaac/salt/modules/#runner-modules","text":"salt/runners Custom: salt://_runners ( runner_dirs configuration option) Used exclusively by salt-run command. They are pure Salt Master Modules, designed to run on master only.","title":"Runner Modules"},{"location":"iaac/salt/modules/#cache-modules","text":"salt/cache Custom: salt://_cache","title":"Cache Modules"},{"location":"iaac/salt/modules/#netapi-modules","text":"salt/netapi Modules that expose Salt API, require configuration to enable","title":"Netapi Modules"},{"location":"iaac/salt/modules/#python-client-api","text":"This is not the part of Salt's netapi , but a library for interaction with Salt Same that is used by Salt commands","title":"Python client API"},{"location":"iaac/salt/modules/#integration-modules","text":"TODO","title":"Integration Modules"},{"location":"iaac/salt/modules/#utility-modules","text":"TODO","title":"Utility Modules"},{"location":"iaac/salt/salt-SSH/","text":"Salt-SSH usage shouldn't be different from the 'standard' Salt Master , Salt Minion usage The minimal config ( /etc/salt/roster ) file: the.hostname.com: host: 192.168.1.2 user: user salt-ssh will upload /etc/salt/pki/master/ssh/salt-ssh.rsa to the the.hostname.com so that public key authentication is used later on. Upon first connection, the user password will be required to provide in the prompt Permission denied for host the.hostname.com, do you want to deploy the salt-ssh key? (password required): [Y/n] Password for user@the.hostname.com: Previous roster file is very limited, such config won't allow to execute any state that requires elevated privileges. In order to fix this the user must belong to sudo group and have NOPASSWD setting in sudoers file. As of current: 2019.2.2 version both settings are required Thin dir salt-ssh uploads some environmental data to the remote and places it under thin_dir (default: /var/tmp - contrary to /tmp from doc ) It contains salt-call along with packaged custom modules, lowstate and grains. Log Using -l <log level> doesn't provide logs from remote host, thus if some modules are missing on remote, user won't know it. This is somewhat consistent with Salt Master - Minion deployments where Salt Minion logs provide more details about issues during state execution In oder to gather logs from remote: the.hostname.com: host: 192.168.1.2 user: user minion_opts: log_level: debug log_level_logfile: debug log_file: ../../salt-ssh.log Putting absolute path as log_file doesn't allow to gather logs, this must be relative path","title":"SSH"},{"location":"iaac/salt/salt-SSH/#thin-dir","text":"salt-ssh uploads some environmental data to the remote and places it under thin_dir (default: /var/tmp - contrary to /tmp from doc ) It contains salt-call along with packaged custom modules, lowstate and grains.","title":"Thin dir"},{"location":"iaac/salt/salt-SSH/#log","text":"Using -l <log level> doesn't provide logs from remote host, thus if some modules are missing on remote, user won't know it. This is somewhat consistent with Salt Master - Minion deployments where Salt Minion logs provide more details about issues during state execution In oder to gather logs from remote: the.hostname.com: host: 192.168.1.2 user: user minion_opts: log_level: debug log_level_logfile: debug log_file: ../../salt-ssh.log Putting absolute path as log_file doesn't allow to gather logs, this must be relative path","title":"Log"},{"location":"iaac/salt/scripts/","text":"Means of interaction with Salt @carsonoid groups the scripts by their purpose. Action scripts Interaction with Salt Master or Salt Minion salt salt '*' test.ping salt runs on master, accepts the user command, applies them via salt-master process (salt -> salt-master) --transport--> salt-minions salt-call salt-call runs the execution modules directly, doesn't require running salt-minion at all , unless --local option is passed it still requires salt-master connection salt-run salt-run runs on the master, talks to the salt-master process, allows to run runner modules. runner modules allows to orchestrate multi-minion installations salt-ssh Doesn't use Salt Minion process at all, requires only SSH daemon running. Installs python binaries on the remote minion. It is much slower than using salt-minion More detailed description of salt-ssh can be found here salt-cloud Runs on the master. Requires additional configuration of: - cloud provider connection - image profiles (of the vm) Connects to desired cloud provider, allocates the resources, uses salt-ssh to provision image after its creation. Daemon scripts Tools to extend access to salt salt-api Runs on the salt-master and communicates with salt-master process. Exposes multiple connectors and ACL. Using curl, access API like so: curl -sSk https://salt.local:9191/login -H 'Accept: application/x-yaml' -d username=saltuser -d password=saltpassword -d eauth=auto salt-proxy Pretends to be salt-minion, allows provisioning of the devices that cannot run salt-minion or cannot be connected via SSH. Salt-master is not aware of the proxy. salt-syndic Runs on multiple salt-masters and proxies traffic from multiple masters to one desired (uber) salt-master. Utility scripts Salt itself management salt-key Runs on salt-master, manages minion keys. List all of the keys: salt-key -L Accept key: salt-key -a minion_id salt-cp Copy files from salt-master to salt-minion (other way around as well) spm Salt Package Manager, standarization over multiple salt formulas. Currently adding formulas requires changes in Salt Master configuration. With spm this is no longer necessary. salt-extend Boilerplate generator for custom modules. salt-unity Wrapper over any other script: salt-unity key -L . This is useful for tools that require listing allowed user commands (like sudoers file)","title":"Scripts"},{"location":"iaac/salt/scripts/#action-scripts","text":"Interaction with Salt Master or Salt Minion","title":"Action scripts"},{"location":"iaac/salt/scripts/#salt","text":"salt '*' test.ping salt runs on master, accepts the user command, applies them via salt-master process (salt -> salt-master) --transport--> salt-minions","title":"salt"},{"location":"iaac/salt/scripts/#salt-call","text":"salt-call runs the execution modules directly, doesn't require running salt-minion at all , unless --local option is passed it still requires salt-master connection","title":"salt-call"},{"location":"iaac/salt/scripts/#salt-run","text":"salt-run runs on the master, talks to the salt-master process, allows to run runner modules. runner modules allows to orchestrate multi-minion installations","title":"salt-run"},{"location":"iaac/salt/scripts/#salt-ssh","text":"Doesn't use Salt Minion process at all, requires only SSH daemon running. Installs python binaries on the remote minion. It is much slower than using salt-minion More detailed description of salt-ssh can be found here","title":"salt-ssh"},{"location":"iaac/salt/scripts/#salt-cloud","text":"Runs on the master. Requires additional configuration of: - cloud provider connection - image profiles (of the vm) Connects to desired cloud provider, allocates the resources, uses salt-ssh to provision image after its creation.","title":"salt-cloud"},{"location":"iaac/salt/scripts/#daemon-scripts","text":"Tools to extend access to salt","title":"Daemon scripts"},{"location":"iaac/salt/scripts/#salt-api","text":"Runs on the salt-master and communicates with salt-master process. Exposes multiple connectors and ACL. Using curl, access API like so: curl -sSk https://salt.local:9191/login -H 'Accept: application/x-yaml' -d username=saltuser -d password=saltpassword -d eauth=auto","title":"salt-api"},{"location":"iaac/salt/scripts/#salt-proxy","text":"Pretends to be salt-minion, allows provisioning of the devices that cannot run salt-minion or cannot be connected via SSH. Salt-master is not aware of the proxy.","title":"salt-proxy"},{"location":"iaac/salt/scripts/#salt-syndic","text":"Runs on multiple salt-masters and proxies traffic from multiple masters to one desired (uber) salt-master.","title":"salt-syndic"},{"location":"iaac/salt/scripts/#utility-scripts","text":"Salt itself management","title":"Utility scripts"},{"location":"iaac/salt/scripts/#salt-key","text":"Runs on salt-master, manages minion keys. List all of the keys: salt-key -L Accept key: salt-key -a minion_id","title":"salt-key"},{"location":"iaac/salt/scripts/#salt-cp","text":"Copy files from salt-master to salt-minion (other way around as well)","title":"salt-cp"},{"location":"iaac/salt/scripts/#spm","text":"Salt Package Manager, standarization over multiple salt formulas. Currently adding formulas requires changes in Salt Master configuration. With spm this is no longer necessary.","title":"spm"},{"location":"iaac/salt/scripts/#salt-extend","text":"Boilerplate generator for custom modules.","title":"salt-extend"},{"location":"iaac/salt/scripts/#salt-unity","text":"Wrapper over any other script: salt-unity key -L . This is useful for tools that require listing allowed user commands (like sudoers file)","title":"salt-unity"},{"location":"iaac/salt/transport/","text":"Salt accommodates multiple configurable transports (communication between master - minions): - TCP - ZeroMQ (default) - RAET (Rapid Asynchronous Event Transport) Architecture (ZeroMQ transport) There is no bi-directional communication using one channel only Master communicates with minions via Pub/Sub bus - this is broadcast type. Internally named Publisher , uses TCP port 4505 Thus every minion receives the master requests (the filtering happens on the minion). Master computes the number of expected replies (if using wildcards or computation is impossible, master will always assume all cached minions could reply). Minions sends their requests and replies via Direct bus , this channel is private for master-minion pair. Internally named ReqServer , uses TCP port 4506 For example, all of the following calls are insecure: salt '*' grains.set some:password 'afm4o' , salt 'minion' grains.set some:password 'afm4o' , salt 'minion' state.apply db.setup pillar='{\"some\": {\"password\": \"afm4o\"}}' On the other hand: salt 'minionX' saltutil.refresh_pillar is secure. Minion (and possibly all other minions) will receive the request to refresh the pillar data. However only minionX will establish private secure channel with master, which will use to fetch it's own private pillar data. Detailed job flow User issues command on the CLI, salt 'minion' test.ping salt uses LocalClient class for connection with Salt Master 's ReqServer on TCP port 4506 Job is issued over established connection ReqServer passes the job to worker processes ( MWorker ) on the Salt Master Worker validates the job (e.g. is user allowed to perform it) Worker send the publish command to all minions. Publish command represents the job to be executed. Worker does this by sending an event on Salt Master event bus. In the form of: salt/job/jid/new , where jid is a generated job ID. From Salt Master event bus, event is encrypted and transferred to actual Publisher that sends the message to all connected minions Minions already have session established with Salt Master's Publisher (port 4505), where they await commands. Minions decrypt the message Minions check if the message is targeted for them Job is executed The result of the job execution is encrypted and send back to master to ReqServer TCP port 4506. ReqServer forwards the result to MWorker's MWorker decrypts the received result, forwards it to Salt Master event bus. One of the Salt Master event bus listeners is a LocalClient that has been waiting for this result LocalClient stores the result, waits until all expected minions reply (or timeout occurs) Result is displayed back to CLI","title":"Transport"},{"location":"iaac/salt/transport/#architecture-zeromq-transport","text":"There is no bi-directional communication using one channel only Master communicates with minions via Pub/Sub bus - this is broadcast type. Internally named Publisher , uses TCP port 4505 Thus every minion receives the master requests (the filtering happens on the minion). Master computes the number of expected replies (if using wildcards or computation is impossible, master will always assume all cached minions could reply). Minions sends their requests and replies via Direct bus , this channel is private for master-minion pair. Internally named ReqServer , uses TCP port 4506 For example, all of the following calls are insecure: salt '*' grains.set some:password 'afm4o' , salt 'minion' grains.set some:password 'afm4o' , salt 'minion' state.apply db.setup pillar='{\"some\": {\"password\": \"afm4o\"}}' On the other hand: salt 'minionX' saltutil.refresh_pillar is secure. Minion (and possibly all other minions) will receive the request to refresh the pillar data. However only minionX will establish private secure channel with master, which will use to fetch it's own private pillar data.","title":"Architecture (ZeroMQ transport)"},{"location":"iaac/salt/transport/#detailed-job-flow","text":"User issues command on the CLI, salt 'minion' test.ping salt uses LocalClient class for connection with Salt Master 's ReqServer on TCP port 4506 Job is issued over established connection ReqServer passes the job to worker processes ( MWorker ) on the Salt Master Worker validates the job (e.g. is user allowed to perform it) Worker send the publish command to all minions. Publish command represents the job to be executed. Worker does this by sending an event on Salt Master event bus. In the form of: salt/job/jid/new , where jid is a generated job ID. From Salt Master event bus, event is encrypted and transferred to actual Publisher that sends the message to all connected minions Minions already have session established with Salt Master's Publisher (port 4505), where they await commands. Minions decrypt the message Minions check if the message is targeted for them Job is executed The result of the job execution is encrypted and send back to master to ReqServer TCP port 4506. ReqServer forwards the result to MWorker's MWorker decrypts the received result, forwards it to Salt Master event bus. One of the Salt Master event bus listeners is a LocalClient that has been waiting for this result LocalClient stores the result, waits until all expected minions reply (or timeout occurs) Result is displayed back to CLI","title":"Detailed job flow"},{"location":"net/IPsec/","text":"IPsec IP-IP tunnelling type. Suite of protocols for securing network communication. Uses orthogonal concepts Concept: AH vs ESP Main L3 protocols, IP packet's proto field point to either AH (51) or ESP(50) AH (51) Authentication only, doesn't provide encryption. Auth (hash-based) is computed using all IP header fields (but TTL and header checksum) Notable headers: - Security Parameter Index: 32bit identifier, used by recipient to fetch the security context associated with the packet - Authentication Data: calculated hash value, mismatched hash value means the packet is discarded Incompatible with NATs since IP addresses are used to compute the hash, intermediate parties doing NAT don't know the secret key to recompute the hash. Thus the receiving site drops the packets. ESP (50) Provides encryption and flow identifier (Security Parameter Index - SPI) Concept: Tunnel vs Transport Informs what parts of IP packet to encrypt. The 'implementation' difference: next header field in AH or ESP header. | next header (symbolic name) | mode | |-|-| | ip | tunnel mode | | AH or ESP | transport mode | Transport Provides encryption or authentication (or both). The IP header is not encrypted. The routing information is not modified (IP header is left unchanged). The IP header determines the policy to be used for the packet. Typically, used for a host to host IPsec. Example for AH mode | IP header | AH header | TCP | Typically, used to secure communication between hosts Tunnel The entire IP packet is encapsulated. Implication of that is following: the encapsulated source/destination addresses may be different than these in IP header (routing information is changed). | IP header | AH header | IP header | TCP | Typically used to secure communication between networks Concept: IKE vs manual Mechanism of negotiating keys IKE In Linux typically implemented via pluto or charon (part of StrongSwan userspace application) Authenticates both VPN 'ends' to each other. IKE is encapsulated in UDP port 500. IKE detects NAT and switches to UDP port 4500 when NAT is found . IKA_SA is kept in userspace, other SA and SP are sent to kernel via netlink socket, the tunnel is considered \"up\". Manual charon provides the IKE protocol to derive the key used for encryption or authentication. In order to do that manually, check: https://github.com/kiemlicz/util/blob/master/net/ipsec_functions#L4 Concept: main mode vs aggressive TODO Processing Description of the IPsec processing on the Linux OS After negotiating IKE_SA the SA and SP for actual VPN connection are negotiated and injected into kernel. The IPsec processing uses XFRM framework (the part of kernel), it is \"hooked\" into packet flow ( xfrm states), allows for various 'transforms': There are actually three XFRM lookup calls (diagram shows only two). The use of either of them is determined by packet destination address: - dir in input policy, selector for already decrypted packages - dir out output policy, selector for packets to be encrypted - dir fwd forward policy, selector for already decrypted packages with non-local destination xfrm/socket lookup Multiple things happen here 1. ESP packets are matched against SAD (by {src IP, dst IP, SPI}). If matched then packed is passed to decode 1. already decrypted packets are matched against SPD ( dir in input policy), if the match is found then the packet is processed further, otherwise droppped xfrm decode Decrypt and decapsulate based on SA xfrm encode Encrypt and encapsulate based on SA xfrm fwd lookup Not shown in packet flow diagram. Relevant only for already decrypted packets. Packets are matched against SPD ( dir fwd forward policy, if the match is found then the packet is processed further (forwarded), otherwise droppped xfrm lookup Checks SPD ( dir out output policy), if match is found then the packet is moved to xfrm encode Security Policy Database (SPD) What to encrypt, e.g, \"all packets from 10.0.0.0/13 IPsec encrypt\" ip xfrm policy dumps all policies. Typically, each IPsec connection will have at least three policies (in, out and fwd). A volatile DB (doesn't persist to disk) SAD How to encrypt, or rather how to apply the security transformations ip xfrm state dumps all security associations, dumps the master key as well. A volatile DB (doesn't persist to disk) Packet flow Sender side (simplified): 0. Send packet 1. ip_route_output_flow() check the routing information ip route get <dst> 2. xfrm_lookup_route() find IPsec SPDs (finds IPsec policy) 3. if no policy found: just send 4. if policy found, get xfrm_state . If the state is not yet established, drop UDP packet (TCP is not dropped) and establish the xfrm_state Received side (simplified): 0. Incoming packet 1. Decide if packet is for local process or not, if not then forward 2. if local and ESP packet then go into XFRM 3. find SA using SPI, validate keys, decrypt 4. submit decrypted IP packet back to IP stack References One of the best IPsec descriptions IPsec in Linux kernel Slides Great Linux IPsec implementation description","title":"IPsec"},{"location":"net/IPsec/#ipsec","text":"IP-IP tunnelling type. Suite of protocols for securing network communication. Uses orthogonal concepts","title":"IPsec"},{"location":"net/IPsec/#concept-ah-vs-esp","text":"Main L3 protocols, IP packet's proto field point to either AH (51) or ESP(50)","title":"Concept: AH vs ESP"},{"location":"net/IPsec/#ah-51","text":"Authentication only, doesn't provide encryption. Auth (hash-based) is computed using all IP header fields (but TTL and header checksum) Notable headers: - Security Parameter Index: 32bit identifier, used by recipient to fetch the security context associated with the packet - Authentication Data: calculated hash value, mismatched hash value means the packet is discarded Incompatible with NATs since IP addresses are used to compute the hash, intermediate parties doing NAT don't know the secret key to recompute the hash. Thus the receiving site drops the packets.","title":"AH (51)"},{"location":"net/IPsec/#esp-50","text":"Provides encryption and flow identifier (Security Parameter Index - SPI)","title":"ESP (50)"},{"location":"net/IPsec/#concept-tunnel-vs-transport","text":"Informs what parts of IP packet to encrypt. The 'implementation' difference: next header field in AH or ESP header. | next header (symbolic name) | mode | |-|-| | ip | tunnel mode | | AH or ESP | transport mode |","title":"Concept: Tunnel vs Transport"},{"location":"net/IPsec/#transport","text":"Provides encryption or authentication (or both). The IP header is not encrypted. The routing information is not modified (IP header is left unchanged). The IP header determines the policy to be used for the packet. Typically, used for a host to host IPsec. Example for AH mode | IP header | AH header | TCP | Typically, used to secure communication between hosts","title":"Transport"},{"location":"net/IPsec/#tunnel","text":"The entire IP packet is encapsulated. Implication of that is following: the encapsulated source/destination addresses may be different than these in IP header (routing information is changed). | IP header | AH header | IP header | TCP | Typically used to secure communication between networks","title":"Tunnel"},{"location":"net/IPsec/#concept-ike-vs-manual","text":"Mechanism of negotiating keys","title":"Concept: IKE vs manual"},{"location":"net/IPsec/#ike","text":"In Linux typically implemented via pluto or charon (part of StrongSwan userspace application) Authenticates both VPN 'ends' to each other. IKE is encapsulated in UDP port 500. IKE detects NAT and switches to UDP port 4500 when NAT is found . IKA_SA is kept in userspace, other SA and SP are sent to kernel via netlink socket, the tunnel is considered \"up\".","title":"IKE"},{"location":"net/IPsec/#manual","text":"charon provides the IKE protocol to derive the key used for encryption or authentication. In order to do that manually, check: https://github.com/kiemlicz/util/blob/master/net/ipsec_functions#L4","title":"Manual"},{"location":"net/IPsec/#concept-main-mode-vs-aggressive","text":"TODO","title":"Concept: main mode vs aggressive"},{"location":"net/IPsec/#processing","text":"Description of the IPsec processing on the Linux OS After negotiating IKE_SA the SA and SP for actual VPN connection are negotiated and injected into kernel. The IPsec processing uses XFRM framework (the part of kernel), it is \"hooked\" into packet flow ( xfrm states), allows for various 'transforms': There are actually three XFRM lookup calls (diagram shows only two). The use of either of them is determined by packet destination address: - dir in input policy, selector for already decrypted packages - dir out output policy, selector for packets to be encrypted - dir fwd forward policy, selector for already decrypted packages with non-local destination","title":"Processing"},{"location":"net/IPsec/#xfrmsocket-lookup","text":"Multiple things happen here 1. ESP packets are matched against SAD (by {src IP, dst IP, SPI}). If matched then packed is passed to decode 1. already decrypted packets are matched against SPD ( dir in input policy), if the match is found then the packet is processed further, otherwise droppped","title":"xfrm/socket lookup"},{"location":"net/IPsec/#xfrm-decode","text":"Decrypt and decapsulate based on SA","title":"xfrm decode"},{"location":"net/IPsec/#xfrm-encode","text":"Encrypt and encapsulate based on SA","title":"xfrm encode"},{"location":"net/IPsec/#xfrm-fwd-lookup","text":"Not shown in packet flow diagram. Relevant only for already decrypted packets. Packets are matched against SPD ( dir fwd forward policy, if the match is found then the packet is processed further (forwarded), otherwise droppped","title":"xfrm fwd lookup"},{"location":"net/IPsec/#xfrm-lookup","text":"Checks SPD ( dir out output policy), if match is found then the packet is moved to xfrm encode","title":"xfrm lookup"},{"location":"net/IPsec/#security-policy-database-spd","text":"What to encrypt, e.g, \"all packets from 10.0.0.0/13 IPsec encrypt\" ip xfrm policy dumps all policies. Typically, each IPsec connection will have at least three policies (in, out and fwd). A volatile DB (doesn't persist to disk)","title":"Security Policy Database (SPD)"},{"location":"net/IPsec/#sad","text":"How to encrypt, or rather how to apply the security transformations ip xfrm state dumps all security associations, dumps the master key as well. A volatile DB (doesn't persist to disk)","title":"SAD"},{"location":"net/IPsec/#packet-flow","text":"Sender side (simplified): 0. Send packet 1. ip_route_output_flow() check the routing information ip route get <dst> 2. xfrm_lookup_route() find IPsec SPDs (finds IPsec policy) 3. if no policy found: just send 4. if policy found, get xfrm_state . If the state is not yet established, drop UDP packet (TCP is not dropped) and establish the xfrm_state Received side (simplified): 0. Incoming packet 1. Decide if packet is for local process or not, if not then forward 2. if local and ESP packet then go into XFRM 3. find SA using SPI, validate keys, decrypt 4. submit decrypted IP packet back to IP stack","title":"Packet flow"},{"location":"net/IPsec/#references","text":"One of the best IPsec descriptions IPsec in Linux kernel Slides Great Linux IPsec implementation description","title":"References"},{"location":"net/LoRa/","text":"Low power, long range, wide-area network (LPWAN network category) Uses radio frequencies: - 433 MHz - 868 MHz Europe - 915 MHz Australia, North America - 867 MHz India - 923 MHz Asia Range: According to spec claims: 15km, according to wikipedia for rural areas ~10km, depends on obstacles. Data rates: 0.3 kbps - 50kbps Topology Star of stars (extended star) Node (end device) -<LORA>- Gateway -<IP connection>- Network Server (Cloud) -<IP connection>- Application Server Bi-directional communication, multicast support. More than one gateway can relay the end device traffic (network server handles the redundancy). End device Sends small amounts of data, infrequently over long distances. Gateway Receives packets from Lora network and forwards it to the Network Server Network server Removes the optional redundancy, chooses most appropriate gateway to send the acknowedgements. Handles the security (decrypts the packet) and end device activation. The network servers may instruct the gateways to change end devices' data rates to conserve power (ADR). Application server Consumes end device data (decrypts as well). Sends data to end devices Physical Layer The term: LoRa, per se defines the physical layer. Proprietary protocol derived from CSS . Can be compared to ISO/OSI L1 Communication Layer Described by the LoRaWAN protocol. Hard to compare with any ISO/OSI layer since it descibes multiple network components. For end device to communicate with application, the registration within Network server is requried (this process is actually called \"Activation\"). To use raw LoRa no registration is required. Activation is granted based on Device EUI. Two activation methods exist Over The Air Authentication (OTAA) From network obtain Application EUI and Application Key. The end device joins the network using Application EUI, Application Key and Device EUI The underlying network and application session key is auto-generated. Authentication By Personalization (ABP) The end device joins the network using manually pre-configured network session key, application session key and Device EUI LoRaWAN defines different endpoint classes for different application needs Class A Initiated by end device. Asynchronous: if the device has data to send: it sends. During that transmission, windows for downlink transmission (from the server) are added. No requirements for wake-ups, device can behave however it wants. Downlink can only follow uplink. Class B Class A + end device opens scheduled downlink slots. This allows deterministic communication to the device. Class C Class A + end device always open for downlink requests. This allows servers to initated communication to the device at any time. Security References https://en.wikipedia.org/wiki/LoRa https://docs.pycom.io/firmwareapi/pycom/network/lora/ https://docs.pycom.io/gettingstarted/registration/lora/ https://docs.pycom.io/tutorials/networks/lora/ https://lora-alliance.org/about-lorawan/ https://www.hindawi.com/journals/wcmc/2017/6590713/ https://www.thethingsindustries.com/news/what-lorawan-network-server/ https://www.chirpstack.io/project/architecture/","title":"LoRa"},{"location":"net/LoRa/#topology","text":"Star of stars (extended star) Node (end device) -<LORA>- Gateway -<IP connection>- Network Server (Cloud) -<IP connection>- Application Server Bi-directional communication, multicast support. More than one gateway can relay the end device traffic (network server handles the redundancy).","title":"Topology"},{"location":"net/LoRa/#end-device","text":"Sends small amounts of data, infrequently over long distances.","title":"End device"},{"location":"net/LoRa/#gateway","text":"Receives packets from Lora network and forwards it to the Network Server","title":"Gateway"},{"location":"net/LoRa/#network-server","text":"Removes the optional redundancy, chooses most appropriate gateway to send the acknowedgements. Handles the security (decrypts the packet) and end device activation. The network servers may instruct the gateways to change end devices' data rates to conserve power (ADR).","title":"Network server"},{"location":"net/LoRa/#application-server","text":"Consumes end device data (decrypts as well). Sends data to end devices","title":"Application server"},{"location":"net/LoRa/#physical-layer","text":"The term: LoRa, per se defines the physical layer. Proprietary protocol derived from CSS . Can be compared to ISO/OSI L1","title":"Physical Layer"},{"location":"net/LoRa/#communication-layer","text":"Described by the LoRaWAN protocol. Hard to compare with any ISO/OSI layer since it descibes multiple network components. For end device to communicate with application, the registration within Network server is requried (this process is actually called \"Activation\"). To use raw LoRa no registration is required. Activation is granted based on Device EUI. Two activation methods exist","title":"Communication Layer"},{"location":"net/LoRa/#over-the-air-authentication-otaa","text":"From network obtain Application EUI and Application Key. The end device joins the network using Application EUI, Application Key and Device EUI The underlying network and application session key is auto-generated.","title":"Over The Air Authentication (OTAA)"},{"location":"net/LoRa/#authentication-by-personalization-abp","text":"The end device joins the network using manually pre-configured network session key, application session key and Device EUI LoRaWAN defines different endpoint classes for different application needs","title":"Authentication By Personalization (ABP)"},{"location":"net/LoRa/#class-a","text":"Initiated by end device. Asynchronous: if the device has data to send: it sends. During that transmission, windows for downlink transmission (from the server) are added. No requirements for wake-ups, device can behave however it wants. Downlink can only follow uplink.","title":"Class A"},{"location":"net/LoRa/#class-b","text":"Class A + end device opens scheduled downlink slots. This allows deterministic communication to the device.","title":"Class B"},{"location":"net/LoRa/#class-c","text":"Class A + end device always open for downlink requests. This allows servers to initated communication to the device at any time.","title":"Class C"},{"location":"net/LoRa/#security","text":"","title":"Security"},{"location":"net/LoRa/#references","text":"https://en.wikipedia.org/wiki/LoRa https://docs.pycom.io/firmwareapi/pycom/network/lora/ https://docs.pycom.io/gettingstarted/registration/lora/ https://docs.pycom.io/tutorials/networks/lora/ https://lora-alliance.org/about-lorawan/ https://www.hindawi.com/journals/wcmc/2017/6590713/ https://www.thethingsindustries.com/news/what-lorawan-network-server/ https://www.chirpstack.io/project/architecture/","title":"References"},{"location":"net/SSH/","text":"Implementations OpenSSH Client configurations sourcing order (first obtained value is used): User options ~/.ssh/config /etc/ssh/ssh_config Debian-based OSes send LANG and LC_* variables over SSH (this is implemetation feature). If the server is configured to receive them ( AcceptEnv option in OpenSSH ) then locale must exist on the server-side (must be generated). If the session hangs e.g. server breaks in order to release client use sequence: Enter , ~ , . Execute local script on remote machine: ssh user@remote 'bash -s' < ./local.script.sh Execute local script on remote machine and pass variables: ssh user@remote VAR1=\"something\" VAR2=\"something2\" 'bash -s' < ./local.script.sh If the script requires sudo privileges simply replace 'bash -s' with 'sudo bash -s' but if the script also accepts variables then sudo must be invoked with '-E' flag: 'sudo -E bash -s' . Otherwise variables wouldn't get passed to shell","title":"SSH"},{"location":"net/SSH/#implementations","text":"","title":"Implementations"},{"location":"net/SSH/#openssh","text":"Client configurations sourcing order (first obtained value is used): User options ~/.ssh/config /etc/ssh/ssh_config Debian-based OSes send LANG and LC_* variables over SSH (this is implemetation feature). If the server is configured to receive them ( AcceptEnv option in OpenSSH ) then locale must exist on the server-side (must be generated). If the session hangs e.g. server breaks in order to release client use sequence: Enter , ~ , . Execute local script on remote machine: ssh user@remote 'bash -s' < ./local.script.sh Execute local script on remote machine and pass variables: ssh user@remote VAR1=\"something\" VAR2=\"something2\" 'bash -s' < ./local.script.sh If the script requires sudo privileges simply replace 'bash -s' with 'sudo bash -s' but if the script also accepts variables then sudo must be invoked with '-E' flag: 'sudo -E bash -s' . Otherwise variables wouldn't get passed to shell","title":"OpenSSH"},{"location":"net/SSL/","text":"Basics Term Description Connection Is a transport providing suitable type of service, connection is transient, associated with one session Session Association between client and server. Created by the handshake protocol. Contains security parameters that are shared between multiple connections. They are used to avoid expensive negotiation of new security parameters for each connection Flight Chunk of logically grouped data. Exchanged during handshake. Messages from the same flight may be placed in same Record Record DTLS message fragment that must fit within single IP packet. Contains sequence number and epoch. Conveyed by Record protocol TLS Protocol directly above layer 4 ISO/OSI. Uses reliable transport only (TCP in general). Main goal of TLS is to provide secure connection between parties. Properties: Identity exchange via use of public keys (certificates). They\u2019re used to verify counterparts during initialization of communication. Then the session key (symmetric key) is agreed * Privacy: data is encrypted via symmetric * cryptography (key is negotiated during TLS Handshake Protocol ) * Reliability, uses integrity checks via secure hash functions All TLS messages are encrypted, even handshake (but with NULL protocol - so they are plain text). Description TLS is composed of two sub-protocols (layers), identified by Content Type field. 1. TLS Record Protocol (encapsulates handshake protocol) 2. TLS Handshake Protocol Privacy and reliability is ensured by lower layer - TLS Record Protocol Authentication and encryption algorithm negotiation is ensured by upper layer - TLS Handshake Protocol Handshake protocol Designed to authenticate peers with each other using asymmetric cryptography (one way authentication is required, mutual is optional) Shared secret negotiation (for latter symmetric cryptography) - in general: session negotiation Handshake protocol consists of three sub-protocols: Handshake Protocol Creates the session (context of whole communication) ``` Client sends ClientHello -----------------------> Server responds with ServerHello <----------------------- [or server responds with Alert if version or algorithms(ciphers) contained in ClientHello doesn't match with server's] <----------------------- ``` There are buggy implementations of server's that close connections without sending any alert messages At this point both sides have established following parameters: * ProtocolVersion * SessionID * CipherSuite * CompressionMethod Server sends its Certificate and ServerKeyExchange <----------------------- Optional Server may request client\u2019s certificate (CertificateRequest) <----------------------- Optional Server sends ServerHelloDone <----------------------- Client sends its Certificate and ClientKeyExchange -----------------------> Client sends CertificateVerify -----------------------> Optional, send if the certificate that client has sent \u201chad the signing ability\u201d (all certificates besides ones containing fixed DH parameters). CertificateVerify message contains signature of all sent/received handshake messages so far by the client. Hash and signature used in computation must be the one of those present in supported_signature_algorithms (from CertificateRequest ). Client creates signature using its private key, server verifies it using client\u2019s public key. Client sends ChangeCipherSpec -----------------------> With cipher it had set as pending. This cipher becomes current (explained in Change Cipher sub-section) Client sends Finished -----------------------> Uses new ciphers to send Finished Server sends its own ChangeCipherSpec <----------------------- Server sends Finished message (using new ciphers) <----------------------- Alert Protocol Indicates failures, associated session identifier must be invalidated. May be used to indicate connection end (via Alert(close_notify) ). Change Cipher Receiver of this message must instruct the Record Layer ( Record Protocol ) to immediately copy the read pending state into current state. Sender of this message must immediately instruct the record layer to copy pending write state to current write state. Record Protocol Compression/decompression, division into blocks, reassembly. Used by Handshake Protocol. Maintains connection state - encryption algorithm, compression algorithm and MAC algorithm. Receiving unexpected record type results in Alert(UnexpectedMessage) . Contains information about compression, MAC and encryption for: - current read/write states - pending read/write states Current are used for record processing. To become current: 1. the pending is first agreed upon in Handshake Protocol 2. the change cipher spec message makes it current DTLS TLS over datagram protocols TLS in its original form cannot be used on top of datagram transport like UDP as: - Decrypting of individual records could be impossible. If record N is not received, then integrity check for record N+1 will fail as relies on previous sequence number - Records depend on each other. Cryptographic context is retained between records (as stream ciphers are used). - Handshake protocol could fail as requires all messages to be reliably delivered (no messages must be lost during handshake phase) in defined order. Mechanisms for fitting TLS into UDP Message loss protection: - Retransmission, if expected other-side message (for handshake phase) doesn't arrive within given time then the message is retransmitted. - Stream ciphers are prohibited as they are stateful and loosing packages breaks them (missing record disallows decryption of packets with next sequence number). Message reordering protection: - Each message is assigned explicit sequence number. This way peer can determine if the message it receives is the next message it awaits. - Each message is also assigned epoch number. Epoch is incremented with every ChangeCipherSpec message. Usually message from previous epoch can be discarded. DTLS record must fit in single datagram (in order to avoid IP fragmentation). As e.g. handshake messages are bigger than max record size they can be fragmented in multiple records. Summary of TLS handshake changes for DTLS Stateless cookie exchange Message loss and reordering handling Retransmission timers added Further considerations As DTLS can use UDP the client-server relation is changed to more peer-to-peer like. Meaning that it should be possible for both parties to act like client and server simultaneously. This is useful feature. DTLS sessions can be long-lived thus e.g. when one side (that acted like server) has lost DTLS state it should be able to establish new one by sending ClientHello immediately after detecting failure References RFC handshake flow RFC CertificateVerify details DTLS RFC . Mind that this RFC is presented as series of diffs from TLS RFC Stream cipher","title":"SSL"},{"location":"net/SSL/#basics","text":"Term Description Connection Is a transport providing suitable type of service, connection is transient, associated with one session Session Association between client and server. Created by the handshake protocol. Contains security parameters that are shared between multiple connections. They are used to avoid expensive negotiation of new security parameters for each connection Flight Chunk of logically grouped data. Exchanged during handshake. Messages from the same flight may be placed in same Record Record DTLS message fragment that must fit within single IP packet. Contains sequence number and epoch. Conveyed by Record protocol","title":"Basics"},{"location":"net/SSL/#tls","text":"Protocol directly above layer 4 ISO/OSI. Uses reliable transport only (TCP in general). Main goal of TLS is to provide secure connection between parties. Properties: Identity exchange via use of public keys (certificates). They\u2019re used to verify counterparts during initialization of communication. Then the session key (symmetric key) is agreed * Privacy: data is encrypted via symmetric * cryptography (key is negotiated during TLS Handshake Protocol ) * Reliability, uses integrity checks via secure hash functions All TLS messages are encrypted, even handshake (but with NULL protocol - so they are plain text).","title":"TLS"},{"location":"net/SSL/#description","text":"TLS is composed of two sub-protocols (layers), identified by Content Type field. 1. TLS Record Protocol (encapsulates handshake protocol) 2. TLS Handshake Protocol Privacy and reliability is ensured by lower layer - TLS Record Protocol Authentication and encryption algorithm negotiation is ensured by upper layer - TLS Handshake Protocol","title":"Description"},{"location":"net/SSL/#handshake-protocol","text":"Designed to authenticate peers with each other using asymmetric cryptography (one way authentication is required, mutual is optional) Shared secret negotiation (for latter symmetric cryptography) - in general: session negotiation Handshake protocol consists of three sub-protocols: Handshake Protocol Creates the session (context of whole communication) ``` Client sends ClientHello -----------------------> Server responds with ServerHello <----------------------- [or server responds with Alert if version or algorithms(ciphers) contained in ClientHello doesn't match with server's] <----------------------- ``` There are buggy implementations of server's that close connections without sending any alert messages At this point both sides have established following parameters: * ProtocolVersion * SessionID * CipherSuite * CompressionMethod Server sends its Certificate and ServerKeyExchange <----------------------- Optional Server may request client\u2019s certificate (CertificateRequest) <----------------------- Optional Server sends ServerHelloDone <----------------------- Client sends its Certificate and ClientKeyExchange -----------------------> Client sends CertificateVerify -----------------------> Optional, send if the certificate that client has sent \u201chad the signing ability\u201d (all certificates besides ones containing fixed DH parameters). CertificateVerify message contains signature of all sent/received handshake messages so far by the client. Hash and signature used in computation must be the one of those present in supported_signature_algorithms (from CertificateRequest ). Client creates signature using its private key, server verifies it using client\u2019s public key. Client sends ChangeCipherSpec -----------------------> With cipher it had set as pending. This cipher becomes current (explained in Change Cipher sub-section) Client sends Finished -----------------------> Uses new ciphers to send Finished Server sends its own ChangeCipherSpec <----------------------- Server sends Finished message (using new ciphers) <----------------------- Alert Protocol Indicates failures, associated session identifier must be invalidated. May be used to indicate connection end (via Alert(close_notify) ). Change Cipher Receiver of this message must instruct the Record Layer ( Record Protocol ) to immediately copy the read pending state into current state. Sender of this message must immediately instruct the record layer to copy pending write state to current write state.","title":"Handshake protocol"},{"location":"net/SSL/#record-protocol","text":"Compression/decompression, division into blocks, reassembly. Used by Handshake Protocol. Maintains connection state - encryption algorithm, compression algorithm and MAC algorithm. Receiving unexpected record type results in Alert(UnexpectedMessage) . Contains information about compression, MAC and encryption for: - current read/write states - pending read/write states Current are used for record processing. To become current: 1. the pending is first agreed upon in Handshake Protocol 2. the change cipher spec message makes it current","title":"Record Protocol"},{"location":"net/SSL/#dtls","text":"TLS over datagram protocols TLS in its original form cannot be used on top of datagram transport like UDP as: - Decrypting of individual records could be impossible. If record N is not received, then integrity check for record N+1 will fail as relies on previous sequence number - Records depend on each other. Cryptographic context is retained between records (as stream ciphers are used). - Handshake protocol could fail as requires all messages to be reliably delivered (no messages must be lost during handshake phase) in defined order.","title":"DTLS"},{"location":"net/SSL/#mechanisms-for-fitting-tls-into-udp","text":"Message loss protection: - Retransmission, if expected other-side message (for handshake phase) doesn't arrive within given time then the message is retransmitted. - Stream ciphers are prohibited as they are stateful and loosing packages breaks them (missing record disallows decryption of packets with next sequence number). Message reordering protection: - Each message is assigned explicit sequence number. This way peer can determine if the message it receives is the next message it awaits. - Each message is also assigned epoch number. Epoch is incremented with every ChangeCipherSpec message. Usually message from previous epoch can be discarded. DTLS record must fit in single datagram (in order to avoid IP fragmentation). As e.g. handshake messages are bigger than max record size they can be fragmented in multiple records.","title":"Mechanisms for fitting TLS into UDP"},{"location":"net/SSL/#summary-of-tls-handshake-changes-for-dtls","text":"Stateless cookie exchange Message loss and reordering handling Retransmission timers added","title":"Summary of TLS handshake changes for DTLS"},{"location":"net/SSL/#further-considerations","text":"As DTLS can use UDP the client-server relation is changed to more peer-to-peer like. Meaning that it should be possible for both parties to act like client and server simultaneously. This is useful feature. DTLS sessions can be long-lived thus e.g. when one side (that acted like server) has lost DTLS state it should be able to establish new one by sending ClientHello immediately after detecting failure","title":"Further considerations"},{"location":"net/SSL/#references","text":"RFC handshake flow RFC CertificateVerify details DTLS RFC . Mind that this RFC is presented as series of diffs from TLS RFC Stream cipher","title":"References"},{"location":"net/net-cfg/","text":"Basics IP Meaning Usual name 127.0.0.1 Loopback address, primarly for debugging and connecting to local servers localhost 127.0.1.1 Mapping of hostname to IP in case network is not available. For systems with permanent IP, permanent IP should be used instead of 127.0.1.1 $(hostname) To get/set system name use: hostname To get DNS domain name use: dnsdomainname To set DNS domain name (or rather FQDN : Fully Qualified Domain Name, which consists of hostname concatenated with domain name) use fqdn aliases for 127.0.1.1 in /etc/hosts (e.g. 127.0.1.1 myhostname.my.domain.com myhostname ). Setting DNS domain may cause troubles in case of multi-interface nodes. Debian-based OSes net config Static network configuration in /etc/network/interfaces : auto eth0 iface eth0 inet static address 192.0.2.7 netmask 255.255.255.0 gateway 192.0.2.254 DNS servers IP addresses reside in /etc/resolv.conf Usage of NetworkManager NetworkManager can read /etc/network/interfaces file if configured to do so via /etc/NetworkManager/NetworkManager.conf [ifupdown] managed=true To disable NetworkManager for given interface: [main] plugins=ifupdown,keyfile [ifupdown] managed=true #this section allows network manager to stop managing specified interface [keyfile] unmanaged-devices=mac:aa:bb:cc:dd:ee:ff References Debian network configuration","title":"Configuration"},{"location":"net/net-cfg/#basics","text":"IP Meaning Usual name 127.0.0.1 Loopback address, primarly for debugging and connecting to local servers localhost 127.0.1.1 Mapping of hostname to IP in case network is not available. For systems with permanent IP, permanent IP should be used instead of 127.0.1.1 $(hostname) To get/set system name use: hostname To get DNS domain name use: dnsdomainname To set DNS domain name (or rather FQDN : Fully Qualified Domain Name, which consists of hostname concatenated with domain name) use fqdn aliases for 127.0.1.1 in /etc/hosts (e.g. 127.0.1.1 myhostname.my.domain.com myhostname ). Setting DNS domain may cause troubles in case of multi-interface nodes.","title":"Basics"},{"location":"net/net-cfg/#debian-based-oses-net-config","text":"Static network configuration in /etc/network/interfaces : auto eth0 iface eth0 inet static address 192.0.2.7 netmask 255.255.255.0 gateway 192.0.2.254 DNS servers IP addresses reside in /etc/resolv.conf","title":"Debian-based OSes net config"},{"location":"net/net-cfg/#usage-of-networkmanager","text":"NetworkManager can read /etc/network/interfaces file if configured to do so via /etc/NetworkManager/NetworkManager.conf [ifupdown] managed=true To disable NetworkManager for given interface: [main] plugins=ifupdown,keyfile [ifupdown] managed=true #this section allows network manager to stop managing specified interface [keyfile] unmanaged-devices=mac:aa:bb:cc:dd:ee:ff","title":"Usage of NetworkManager"},{"location":"net/net-cfg/#references","text":"Debian network configuration","title":"References"},{"location":"net/net-debug/","text":"Basics Normally packets received must have destination MAC address equal to NIC's MAC address. Exceptions: 1. Broadcast destination: 0xFFFFFFFFFFFF 2. Multicast destination: for IPv4 0x01..... , for IPv6 0x3333.... Tcpdump Tool based on libpcap for packet capture. Traffic dumping takes place at specific points in time: * Incoming traffic: wire -> NIC -> tcpdump -> netfilter/iptables -> application * Outgoing traffic: application -> iptables -> tcpdump -> NIC -> wire Dumping traffic with MAC of NIC Dump local traffic using tcpdump: tcpdump -i eth0 -w /tmp/outfile.pcap host 1.1.1.1 Dump traffic on remote (eth0) host and visualize it locally with wireshark: Without access to tcpdump binary on remote mkfifo /tmp/dump ssh user@remote \"sudo tcpdump -s0 -U -n -w - -i eth0 'not port 22'\" > /tmp/dump wireshark -k -i <(cat /tmp/dump) Having user access to tcpdump binary on remote, it is as simple as: ssh -C user@remote \"tcpdump -i any -s0 -U -w - host 1.2.3.4\" | wireshark -k -i - If sudo is available remotely, perform: groupadd pcap usermod -a -G pcap $USER chgrp pcap /usr/sbin/tcpdump chmod 750 /usr/sbin/tcpdump setcap cap_net_raw,cap_net_admin=eip /usr/sbin/tcpdump Dumping any traffic aka. sniffing Latency Different sources of latencies: - DNS - ping latency, round-trip time, may not reflect actual request latency (ICMP priority may be lower) - connection latency, e.g. TCP three-way handshake time (retransmits if backlog full on the receiving side) - first-byte latency, time from establishing connection to receiving the first byte - round-trip time, signal propagation and processing times at intermediate hops - connection life span, keep-alive connection to reduce handshake overhead References https://www.wains.be/pub/networking/tcpdump_advanced_filters.txt https://peternixon.net/news/2012/01/28/configure-tcpdump-work-non-root-user-opensuse-using-file-system-capabilities/","title":"Debugging"},{"location":"net/net-debug/#basics","text":"Normally packets received must have destination MAC address equal to NIC's MAC address. Exceptions: 1. Broadcast destination: 0xFFFFFFFFFFFF 2. Multicast destination: for IPv4 0x01..... , for IPv6 0x3333....","title":"Basics"},{"location":"net/net-debug/#tcpdump","text":"Tool based on libpcap for packet capture. Traffic dumping takes place at specific points in time: * Incoming traffic: wire -> NIC -> tcpdump -> netfilter/iptables -> application * Outgoing traffic: application -> iptables -> tcpdump -> NIC -> wire","title":"Tcpdump"},{"location":"net/net-debug/#dumping-traffic-with-mac-of-nic","text":"Dump local traffic using tcpdump: tcpdump -i eth0 -w /tmp/outfile.pcap host 1.1.1.1 Dump traffic on remote (eth0) host and visualize it locally with wireshark: Without access to tcpdump binary on remote mkfifo /tmp/dump ssh user@remote \"sudo tcpdump -s0 -U -n -w - -i eth0 'not port 22'\" > /tmp/dump wireshark -k -i <(cat /tmp/dump) Having user access to tcpdump binary on remote, it is as simple as: ssh -C user@remote \"tcpdump -i any -s0 -U -w - host 1.2.3.4\" | wireshark -k -i - If sudo is available remotely, perform: groupadd pcap usermod -a -G pcap $USER chgrp pcap /usr/sbin/tcpdump chmod 750 /usr/sbin/tcpdump setcap cap_net_raw,cap_net_admin=eip /usr/sbin/tcpdump","title":"Dumping traffic with MAC of NIC"},{"location":"net/net-debug/#dumping-any-traffic-aka-sniffing","text":"","title":"Dumping any traffic aka. sniffing"},{"location":"net/net-debug/#latency","text":"Different sources of latencies: - DNS - ping latency, round-trip time, may not reflect actual request latency (ICMP priority may be lower) - connection latency, e.g. TCP three-way handshake time (retransmits if backlog full on the receiving side) - first-byte latency, time from establishing connection to receiving the first byte - round-trip time, signal propagation and processing times at intermediate hops - connection life span, keep-alive connection to reduce handshake overhead","title":"Latency"},{"location":"net/net-debug/#references","text":"https://www.wains.be/pub/networking/tcpdump_advanced_filters.txt https://peternixon.net/news/2012/01/28/configure-tcpdump-work-non-root-user-opensuse-using-file-system-capabilities/","title":"References"},{"location":"net/net-link-layer/","text":"ISO/OSI Layer 2 Data link access, responsible for access and data transfer through physical medium Defines the transport unit: frame and mechanism of send/receive. Consists of two components: Logical Link Control (LLC) and Media Access Control (MAC) Layer 2 implementations: Ethernet, WiFi, Token Ring, PPP MAC divides the data into frames provides addressing provides medium access differs depending on layer 1 Addressing Non hierarchinal, flat, 6 Bytes. Address types: - unicast - multicast 01:... (lsb of most significant byte eqal to 1) - broadcast FF:FF:FF:FF:FF:FF First 3 Bytes denote NIC producent Carrier Sense Multiple Access with Collision Detection - CSMA/CD Ethernet protocol governing data transmission. Carrier Sense Each host monitors the medium. If the medium is busy then no transmission happens. Multiple Access If the medium is free, wait IFG ( inter frame gap ) time and begin transmission. Between every two frames there is at least IFG idle time. Since multiple hosts may detect that the medium is free, they may start transmission simultaneously causing collision. Collision Detection If the collision is detected, apply to all colliding hosts 1. finish sending preamble (if applies) 1. send jam sequence (32 bits) 1. stop sending 1. backoff 1. retry send In Ethernet segment the collision can't occur after sending 64B Ethernet network segment size In order to calculate Ethernet segment size, following assumptions are made: 1. the Ethernet frame cannot be send without guarantee that no collision occurs. 1. minimal frame size is 64B 1. time to send minimal frame is called the time slot 1. the time slot must be sufficiently long to: detect collision and send jam sequence in the maximum-sized network segment Time to send minimal frame is 51.2 us in 10Mb/s Ethernet network Packet travels roughly 2/3 c time ~= 200 000 km/s In order to find maximum allowed segment size: - assume that collision occurs at the farthest point - the host at that point immediately detects collision and sends the jam sequence - the jam sequence must reach the sender within slot time and within same time the sender must receive it: 51.2us == 2(propagation time) + jam sequence send time + jam sequence receive time Assume that jam sequence send time == jam sequence receive time == 9.6us (longer than sole jam sequence) The propagation time becomes: 16us and given propagation speed of 2/3 c the maximum theoretical size is 3.2km. In reality due to intermediate equipment it is less than 3.2km LLC retransmission gathering data from MAC error detection Frame 802.3 preamble (8B) destination address (6B) source address (6B) Type (2B) Data (46B-1500B) Frame Sequence Check (2B) - - - 0800 - IPv4, 08100 - 802.1q zero padded if less than 46B CRC References","title":"Link layer"},{"location":"net/net-link-layer/#isoosi-layer-2","text":"Data link access, responsible for access and data transfer through physical medium Defines the transport unit: frame and mechanism of send/receive. Consists of two components: Logical Link Control (LLC) and Media Access Control (MAC) Layer 2 implementations: Ethernet, WiFi, Token Ring, PPP","title":"ISO/OSI Layer 2"},{"location":"net/net-link-layer/#mac","text":"divides the data into frames provides addressing provides medium access differs depending on layer 1","title":"MAC"},{"location":"net/net-link-layer/#addressing","text":"Non hierarchinal, flat, 6 Bytes. Address types: - unicast - multicast 01:... (lsb of most significant byte eqal to 1) - broadcast FF:FF:FF:FF:FF:FF First 3 Bytes denote NIC producent","title":"Addressing"},{"location":"net/net-link-layer/#carrier-sense-multiple-access-with-collision-detection-csmacd","text":"Ethernet protocol governing data transmission.","title":"Carrier Sense Multiple Access with Collision Detection - CSMA/CD"},{"location":"net/net-link-layer/#carrier-sense","text":"Each host monitors the medium. If the medium is busy then no transmission happens.","title":"Carrier Sense"},{"location":"net/net-link-layer/#multiple-access","text":"If the medium is free, wait IFG ( inter frame gap ) time and begin transmission. Between every two frames there is at least IFG idle time. Since multiple hosts may detect that the medium is free, they may start transmission simultaneously causing collision.","title":"Multiple Access"},{"location":"net/net-link-layer/#collision-detection","text":"If the collision is detected, apply to all colliding hosts 1. finish sending preamble (if applies) 1. send jam sequence (32 bits) 1. stop sending 1. backoff 1. retry send In Ethernet segment the collision can't occur after sending 64B","title":"Collision Detection"},{"location":"net/net-link-layer/#ethernet-network-segment-size","text":"In order to calculate Ethernet segment size, following assumptions are made: 1. the Ethernet frame cannot be send without guarantee that no collision occurs. 1. minimal frame size is 64B 1. time to send minimal frame is called the time slot 1. the time slot must be sufficiently long to: detect collision and send jam sequence in the maximum-sized network segment Time to send minimal frame is 51.2 us in 10Mb/s Ethernet network Packet travels roughly 2/3 c time ~= 200 000 km/s In order to find maximum allowed segment size: - assume that collision occurs at the farthest point - the host at that point immediately detects collision and sends the jam sequence - the jam sequence must reach the sender within slot time and within same time the sender must receive it: 51.2us == 2(propagation time) + jam sequence send time + jam sequence receive time Assume that jam sequence send time == jam sequence receive time == 9.6us (longer than sole jam sequence) The propagation time becomes: 16us and given propagation speed of 2/3 c the maximum theoretical size is 3.2km. In reality due to intermediate equipment it is less than 3.2km","title":"Ethernet network segment size"},{"location":"net/net-link-layer/#llc","text":"retransmission gathering data from MAC error detection","title":"LLC"},{"location":"net/net-link-layer/#frame-8023","text":"preamble (8B) destination address (6B) source address (6B) Type (2B) Data (46B-1500B) Frame Sequence Check (2B) - - - 0800 - IPv4, 08100 - 802.1q zero padded if less than 46B CRC","title":"Frame 802.3"},{"location":"net/net-link-layer/#references","text":"","title":"References"},{"location":"net/routing/","text":"The routing table lists remote destinations (subnets) with the address of next-hop known to be closer to destination. On Linux hosts the routing table decides whether the packet should be forwarded or processed locally. Traditionally the Linux packet forwarding selects the route based on packet's destination IP. In order to enable forwarding to non-local destination: sysctl -w net.ipv4.ip_forward=1 Routing table ip route show will display main routing table. Example output: root@vm1:~# ip r s default via 192.168.1.1 dev ens3 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev ens3 proto kernel scope link src 192.168.1.106 Dissection: default equals to 0.0.0.0/0 dev <interface> denotes to which interface given route is pinned via <IP> next hop, may depend on route type src <IP> source address to prefer when using route proto how the route was installed proto info redirect installed due to ICMP redirect kernel installed by the kernel during autoconfiguration boot installed during the bootup sequence static installed by administrator ra installed by Router Discovery Protocol scope determines how valid given address is, if not present then global is assumed scope info global valid everywhere site valid only within given site (IPv6) link valid only for given device host valid only within host, not routable Routing policy It is possible to route packet not only based on destination IP, but rather using some additional information, e.g. source address, protocol or packet size. In order to do that following kernel options must be enabled ( y ) > cat /boot/config-$(uname -r) | grep CONFIG_IP_ADVANCED_ROUTER CONFIG_IP_ADVANCED_ROUTER=y > cat /boot/config-$(uname -r) | grep CONFIG_XFRM CONFIG_XFRM=y Policies can route selectively traffic using different paths. Policy is picked before routing decision. Policy can determine which routing table to use (it is possible to have multiple routing tables). RPDB Different routing tables are picked based on defined selector. ip rule list shows which route table will be used for particular set of information > ip rule list > #default output 0: from all lookup local 32766: from all lookup main 32767: from all lookup default > #other host > ip rule list 0: from all lookup local 9: from all fwmark 0x2 lookup o2 10: from all fwmark 0x1 lookup o1 220: from all lookup 220 32766: from all lookup main 32767: from all lookup default Dissection: 0: - first column shows entry priority. Lower the number, higher the priority from all - the selector part: any packet in this case from all fwmark 0x1 - the selector part: any packet with iptables mark 0x1 lookup tablename - the action part: use tablename routing table Rules are scanned starting from the lowest number, selector is applied to {src address, dst address, in interface, tos, fwmark} in order to add routing table to the RPDB (Routing Policy Database): echo \"100 o1\" >> /etc/iproute2/rt_tables Display routing table entries: ip r s t tablename Debugging ip route get 1.2.3.4 will display which route will be used for particular destination. ip route get 1.2.3.4 mark 1 will display which route will be used for particular destination with some additional information ( mark in this example). References https://tldp.org/HOWTO/pdf/Adv-Routing-HOWTO.pdf https://silo.tips/download/advanced-routing-scenarios-policy-based-routing-concepts-and-linux-implementatio https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg http://linux-ip.net/html/tools-ip-rule.html Available IP rules","title":"Routing"},{"location":"net/routing/#routing-table","text":"ip route show will display main routing table. Example output: root@vm1:~# ip r s default via 192.168.1.1 dev ens3 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev ens3 proto kernel scope link src 192.168.1.106 Dissection: default equals to 0.0.0.0/0 dev <interface> denotes to which interface given route is pinned via <IP> next hop, may depend on route type src <IP> source address to prefer when using route proto how the route was installed proto info redirect installed due to ICMP redirect kernel installed by the kernel during autoconfiguration boot installed during the bootup sequence static installed by administrator ra installed by Router Discovery Protocol scope determines how valid given address is, if not present then global is assumed scope info global valid everywhere site valid only within given site (IPv6) link valid only for given device host valid only within host, not routable","title":"Routing table"},{"location":"net/routing/#routing-policy","text":"It is possible to route packet not only based on destination IP, but rather using some additional information, e.g. source address, protocol or packet size. In order to do that following kernel options must be enabled ( y ) > cat /boot/config-$(uname -r) | grep CONFIG_IP_ADVANCED_ROUTER CONFIG_IP_ADVANCED_ROUTER=y > cat /boot/config-$(uname -r) | grep CONFIG_XFRM CONFIG_XFRM=y Policies can route selectively traffic using different paths. Policy is picked before routing decision. Policy can determine which routing table to use (it is possible to have multiple routing tables).","title":"Routing policy"},{"location":"net/routing/#rpdb","text":"Different routing tables are picked based on defined selector. ip rule list shows which route table will be used for particular set of information > ip rule list > #default output 0: from all lookup local 32766: from all lookup main 32767: from all lookup default > #other host > ip rule list 0: from all lookup local 9: from all fwmark 0x2 lookup o2 10: from all fwmark 0x1 lookup o1 220: from all lookup 220 32766: from all lookup main 32767: from all lookup default Dissection: 0: - first column shows entry priority. Lower the number, higher the priority from all - the selector part: any packet in this case from all fwmark 0x1 - the selector part: any packet with iptables mark 0x1 lookup tablename - the action part: use tablename routing table Rules are scanned starting from the lowest number, selector is applied to {src address, dst address, in interface, tos, fwmark} in order to add routing table to the RPDB (Routing Policy Database): echo \"100 o1\" >> /etc/iproute2/rt_tables Display routing table entries: ip r s t tablename","title":"RPDB"},{"location":"net/routing/#debugging","text":"ip route get 1.2.3.4 will display which route will be used for particular destination. ip route get 1.2.3.4 mark 1 will display which route will be used for particular destination with some additional information ( mark in this example).","title":"Debugging"},{"location":"net/routing/#references","text":"https://tldp.org/HOWTO/pdf/Adv-Routing-HOWTO.pdf https://silo.tips/download/advanced-routing-scenarios-policy-based-routing-concepts-and-linux-implementatio https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg http://linux-ip.net/html/tools-ip-rule.html Available IP rules","title":"References"},{"location":"net/sockets/","text":"Basics TCP/UDP connection is identified by so called 5-tuple: (protocol, source address, source port, destination address, destination port) Protocol is set when socket() is called Source address and source port is set when bind() is called Destination address and destination prot is set when connect() is called (even for UDP) In order to bind to any address, user can specify address 0.0.0.0 or :: . In practice it means: all IP addresses of all local interfaces. During the succeeding connect() call the OS will choose proper source IP based on destination address and contents of routing table. In order to bind to any ephemeral port, user can specify port = 0 , then the OS will choose the port By default no two sockets can be bound to same (source address, source port) , e.g., if any socket is bound to 0.0.0.0:21 , then no other address can be bound to port 21 Syscalls syscall info socket(domain, type, protocol) returns socket file descriptor (fd) bind(fd, *addr, addrlen) bind to address, returns error code listen(fd, backlog) mark socket as passive (this is: as a socket accepting connections), backlog determines the maximum length to which the queue of pending connections for fd may grow send() adds the data to the send buffer, it doesn't necessarily mean that the data has been sent out. In case of UDP it is possible that the data is send immediately, but for TCP it is unlikely Socket options Socket behavior can be changed, moreover options meaning differ with every OS SO_REUSEADDR Enabled prior to binding. Theoretically SO_REUSEADDR has effect only on wildcard addresses and affects possibility of binding to 'taken' address. SO_REUSEADDR socketA socketB result (of socketB bind() ) true/false 192.168.1.1:21 192.168.1.1:21 EADDRINUSE true/false 192.168.1.1:21 10.0.0.1:21 OK false 0.0.0.0:21 192.168.1.1:21 EADDRINUSE true 0.0.0.0:21 192.168.1.1:21 OK true/false 0.0.0.0:21 0.0.0.0:21 EADDRINUSE Impact: 1. For TCP sockets: the TCP socket after close() call, finally transitions to the TIME_WAIT (waiting for the data in socket buffers to be send). The amount of time the socket stays in this state is determined by linger time (OS and socket level - SO_LINGER - configurable option). If SO_REUSEADDR is not set for such socket then closed TCP socket is still considered bound (up to linger time ). However if SO_REUSEADDR is set, it is possible to bind to such (not-yet-fully) closed socket (without waiting linger time ) 2. For TCP sockets: it is possible that socket reusing address connects to the same destination address and port. Thus the 5-tuple is duplicated and connect() will fail with EADDRINUSE 3. For UDP sockets: multiple multicast (one-to-many) sockets with SO_REUSEADDR may be bound to same combination of multicast address and port (same behavior like SO_REUSEPORT ) SO_REUSEPORT Allows arbitrary number of sockets to bind to exactly the same source address and port as long as all prior bound sockets also had SO_REUSEPORT set. For Linux (>= 3.9): all sockets that want to share the same combination of address and port must belong to processes that share the same effective user ID. Bind options INADDR_ANY Equal to calling bind for address \"0.0.0.0\" ( sin_addr.s_addr ), which specifies all available interfaces. TCP socket states Backlog states how many connections can be queued before handing over (via accept call) to the application. TCP three-way handshake has SYN RECEIVED (SYN received and SYN+ACK sent) intermediate state, which transits to ESTABLISHED after receiving the ACK from client. Thus backlog can be implemented in two ways: The implementation uses a single queue, the size of which is determined by the backlog argument of the listen syscall. When a SYN packet is received, it sends back a SYN/ACK packet and adds the connection to the queue. When the corresponding ACK is received, the connection changes its state to ESTABLISHED and becomes eligible for handover to the application. This means that the queue can contain connections in two different states: SYN RECEIVED and ESTABLISHED. Only connections in the latter state can be returned to the application by the accept syscall. In this approach if the queue is full then any consecutive three-way handshake will be unsuccessful (client syn will be dropped). Not used in linux The implementation uses two queues, a SYN queue (or incomplete connection queue) and an accept queue (or complete connection queue). Connections in state SYN RECEIVED are added to the SYN queue and later moved to the accept queue when their state changes to ESTABLISHED, i.e. when the ACK packet in the 3-way handshake is received. As the name implies, the accept call is then implemented simply to consume connections from the accept queue. In this case, the backlog argument of the listen syscall determines the size of the accept queue (second queue). Linux default. To set the max length of incomplete queue (first queue) use: /proc/sys/net/ipv4/tcp_max_syn_backlog . What happens when accept queue is full and client ACK arrives? Depends on /proc/sys/net/ipv4/tcp_abort_on_overflow it may clean the incomplete connection (drop) References http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html https://github.com/torvalds/linux/blob/master/net/ipv4/tcp_ipv4.c https://stackoverflow.com/questions/14388706/socket-options-so-reuseaddr-and-so-reuseport-how-do-they-differ-do-they-mean-t","title":"Sockets"},{"location":"net/sockets/#basics","text":"TCP/UDP connection is identified by so called 5-tuple: (protocol, source address, source port, destination address, destination port) Protocol is set when socket() is called Source address and source port is set when bind() is called Destination address and destination prot is set when connect() is called (even for UDP) In order to bind to any address, user can specify address 0.0.0.0 or :: . In practice it means: all IP addresses of all local interfaces. During the succeeding connect() call the OS will choose proper source IP based on destination address and contents of routing table. In order to bind to any ephemeral port, user can specify port = 0 , then the OS will choose the port By default no two sockets can be bound to same (source address, source port) , e.g., if any socket is bound to 0.0.0.0:21 , then no other address can be bound to port 21","title":"Basics"},{"location":"net/sockets/#syscalls","text":"syscall info socket(domain, type, protocol) returns socket file descriptor (fd) bind(fd, *addr, addrlen) bind to address, returns error code listen(fd, backlog) mark socket as passive (this is: as a socket accepting connections), backlog determines the maximum length to which the queue of pending connections for fd may grow send() adds the data to the send buffer, it doesn't necessarily mean that the data has been sent out. In case of UDP it is possible that the data is send immediately, but for TCP it is unlikely","title":"Syscalls"},{"location":"net/sockets/#socket-options","text":"Socket behavior can be changed, moreover options meaning differ with every OS","title":"Socket options"},{"location":"net/sockets/#so_reuseaddr","text":"Enabled prior to binding. Theoretically SO_REUSEADDR has effect only on wildcard addresses and affects possibility of binding to 'taken' address. SO_REUSEADDR socketA socketB result (of socketB bind() ) true/false 192.168.1.1:21 192.168.1.1:21 EADDRINUSE true/false 192.168.1.1:21 10.0.0.1:21 OK false 0.0.0.0:21 192.168.1.1:21 EADDRINUSE true 0.0.0.0:21 192.168.1.1:21 OK true/false 0.0.0.0:21 0.0.0.0:21 EADDRINUSE Impact: 1. For TCP sockets: the TCP socket after close() call, finally transitions to the TIME_WAIT (waiting for the data in socket buffers to be send). The amount of time the socket stays in this state is determined by linger time (OS and socket level - SO_LINGER - configurable option). If SO_REUSEADDR is not set for such socket then closed TCP socket is still considered bound (up to linger time ). However if SO_REUSEADDR is set, it is possible to bind to such (not-yet-fully) closed socket (without waiting linger time ) 2. For TCP sockets: it is possible that socket reusing address connects to the same destination address and port. Thus the 5-tuple is duplicated and connect() will fail with EADDRINUSE 3. For UDP sockets: multiple multicast (one-to-many) sockets with SO_REUSEADDR may be bound to same combination of multicast address and port (same behavior like SO_REUSEPORT )","title":"SO_REUSEADDR"},{"location":"net/sockets/#so_reuseport","text":"Allows arbitrary number of sockets to bind to exactly the same source address and port as long as all prior bound sockets also had SO_REUSEPORT set. For Linux (>= 3.9): all sockets that want to share the same combination of address and port must belong to processes that share the same effective user ID.","title":"SO_REUSEPORT"},{"location":"net/sockets/#bind-options","text":"","title":"Bind options"},{"location":"net/sockets/#inaddr_any","text":"Equal to calling bind for address \"0.0.0.0\" ( sin_addr.s_addr ), which specifies all available interfaces.","title":"INADDR_ANY"},{"location":"net/sockets/#tcp-socket-states","text":"Backlog states how many connections can be queued before handing over (via accept call) to the application. TCP three-way handshake has SYN RECEIVED (SYN received and SYN+ACK sent) intermediate state, which transits to ESTABLISHED after receiving the ACK from client. Thus backlog can be implemented in two ways: The implementation uses a single queue, the size of which is determined by the backlog argument of the listen syscall. When a SYN packet is received, it sends back a SYN/ACK packet and adds the connection to the queue. When the corresponding ACK is received, the connection changes its state to ESTABLISHED and becomes eligible for handover to the application. This means that the queue can contain connections in two different states: SYN RECEIVED and ESTABLISHED. Only connections in the latter state can be returned to the application by the accept syscall. In this approach if the queue is full then any consecutive three-way handshake will be unsuccessful (client syn will be dropped). Not used in linux The implementation uses two queues, a SYN queue (or incomplete connection queue) and an accept queue (or complete connection queue). Connections in state SYN RECEIVED are added to the SYN queue and later moved to the accept queue when their state changes to ESTABLISHED, i.e. when the ACK packet in the 3-way handshake is received. As the name implies, the accept call is then implemented simply to consume connections from the accept queue. In this case, the backlog argument of the listen syscall determines the size of the accept queue (second queue). Linux default. To set the max length of incomplete queue (first queue) use: /proc/sys/net/ipv4/tcp_max_syn_backlog . What happens when accept queue is full and client ACK arrives? Depends on /proc/sys/net/ipv4/tcp_abort_on_overflow it may clean the incomplete connection (drop)","title":"TCP socket states"},{"location":"net/sockets/#references","text":"http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html https://github.com/torvalds/linux/blob/master/net/ipv4/tcp_ipv4.c https://stackoverflow.com/questions/14388706/socket-options-so-reuseaddr-and-so-reuseport-how-do-they-differ-do-they-mean-t","title":"References"},{"location":"net/tunneling/","text":"In Linux there are three types of tunneling (attaching network to network) Mind that tunnels introduce headers overhead which must be accounted for when setting the link MTU. IP in IP IP packet encapsulated inside IP GRE Very similar to IPIP but supports multicast Userland Anything allowing to connect two networks that 'live' outside the kernel References https://tldp.org/HOWTO/pdf/Adv-Routing-HOWTO.pdf https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels/","title":"Tunneling"},{"location":"net/tunneling/#ip-in-ip","text":"IP packet encapsulated inside IP","title":"IP in IP"},{"location":"net/tunneling/#gre","text":"Very similar to IPIP but supports multicast","title":"GRE"},{"location":"net/tunneling/#userland","text":"Anything allowing to connect two networks that 'live' outside the kernel","title":"Userland"},{"location":"net/tunneling/#references","text":"https://tldp.org/HOWTO/pdf/Adv-Routing-HOWTO.pdf https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels/","title":"References"},{"location":"os/init/","text":"Init process Main, first started process (typically id = 1, daemon) Started by kernel, if start is unsuccessful, kernel panic occurs. Multiple init implementations Upstart systemd Maintains userspace applications Each application is described in unit files. They use more declarative syntax instead of bash scripts. Typically located in /etc/systemd/system unit file Contains information nearly about anything: a service, socket, device, mount point, etc. File name is used for control (via systemctl ), though unit file can declare aliases ( Alias= ) that can be used too. service declaration Typically systemd controls one service but it is possible to spawn and control multiple instances of service . In order to enable multi instance management: 1. create service file with @.service suffix 2. in unit file use %i placeholder for \"some application instance\" References","title":"PID 1"},{"location":"os/init/#init-process","text":"Main, first started process (typically id = 1, daemon) Started by kernel, if start is unsuccessful, kernel panic occurs. Multiple init implementations","title":"Init process"},{"location":"os/init/#upstart","text":"","title":"Upstart"},{"location":"os/init/#systemd","text":"Maintains userspace applications Each application is described in unit files. They use more declarative syntax instead of bash scripts. Typically located in /etc/systemd/system","title":"systemd"},{"location":"os/init/#unit-file","text":"Contains information nearly about anything: a service, socket, device, mount point, etc. File name is used for control (via systemctl ), though unit file can declare aliases ( Alias= ) that can be used too.","title":"unit file"},{"location":"os/init/#service-declaration","text":"Typically systemd controls one service but it is possible to spawn and control multiple instances of service . In order to enable multi instance management: 1. create service file with @.service suffix 2. in unit file use %i placeholder for \"some application instance\"","title":"service declaration"},{"location":"os/init/#references","text":"","title":"References"},{"location":"os/kernel/","text":"Compilation Upgrade After installing new kernel dependent (\"external\") modules needs to be rebuild. In Debian family dkms is responsible for detecting dependent modules and rebuilding them. To find which modules needs to be rebuild after upgrade, use: dkms status Troubleshooting dkms may yield: Error! Could not locate dkms.conf file. File: does not exist. It means that your installation contains old kernel modules, not properly removed/upgraded. Kernel modules are usually located in /var/lib/dkms/ (or in /usr/src ). List them: for i in /var/lib/dkms/*/[^k]*/source; do [ -e \"$i\" ] || echo \"$i\";done Remove those that are no longer installed (old ones): rm -rf /var/lib/dkms/something/old_version References https://forums.virtualbox.org/viewtopic.php?f=7&t=76493# https://wiki.debian.org/KernelDKMS","title":"Kernel"},{"location":"os/kernel/#compilation","text":"","title":"Compilation"},{"location":"os/kernel/#upgrade","text":"After installing new kernel dependent (\"external\") modules needs to be rebuild. In Debian family dkms is responsible for detecting dependent modules and rebuilding them. To find which modules needs to be rebuild after upgrade, use: dkms status","title":"Upgrade"},{"location":"os/kernel/#troubleshooting","text":"dkms may yield: Error! Could not locate dkms.conf file. File: does not exist. It means that your installation contains old kernel modules, not properly removed/upgraded. Kernel modules are usually located in /var/lib/dkms/ (or in /usr/src ). List them: for i in /var/lib/dkms/*/[^k]*/source; do [ -e \"$i\" ] || echo \"$i\";done Remove those that are no longer installed (old ones): rm -rf /var/lib/dkms/something/old_version","title":"Troubleshooting"},{"location":"os/kernel/#references","text":"https://forums.virtualbox.org/viewtopic.php?f=7&t=76493# https://wiki.debian.org/KernelDKMS","title":"References"},{"location":"os/package-management/","text":"Packages Typically packages are nothing more that some kind of archive. However this archive besides the actual files to be copied to the underlying system contains a lot of additional configuration scripts. DEB AR archive with following 3 top-level files included: 1. control.tar.gz - control files. Theoretically it must contain following files: - control kind of key-value file. Contains information about packaged data (like: author, some description, dependencies) Fields (some of them): Source - name of the source package Package - name of the binary package Section - mainly for front-ends that display packages grouped into categories Priority - how important for the user the package is. For front-ends use (mainly when the apt -like software selects default packages for user or solves conflicts) Architecture - if this package is architecture dependent ( any ) or not ( all - e.g. Java applications) Depends - the package will not be installed unless the packages it depends on are installed - changelog dpkg uses this files to obtain version information, distribution, urgency etc of the package - copyright copyright and license Some of optional files: - conffiles list of packaged application configuration files. When upgrading package you'll be asked what to do with any file listed here that changed on the filesystem - package.cron.* e.g. my_app.cron.hourly will be installed as /etc/cron.hourly/my_app and will run every hour - dirs specifies any directories which we need but which are not created by the normal installation procedure - md5sums used to verify if installed files have been modified 2. data.tar.gz (since Debian 8: data.tar.xz ) - actual files 3. debian-binary RPM todo APT/Aptitude Highest priority number means highest priority of package. Cheat sheet command description aptitude versions the_package , apt-cache policy the_package , dpkg-query -l \\| grep the_package show available/installed package version apt-cache depends the_package show package dependencies apt-get install --only-upgrade the_package upgrade/downgrade only one specific package apt-get autoremove --purge Remove dangling packages aptitude autoclean remove old cached packages aptitude clean remove cached packages dpkg -S /bin/ping show to which package does the file belong to Configuration APT configuration can be dumped with: apt-config dump To reload configuration changes (either repositories changes or apt.conf.d/* changes use: apt-get update Preferences In general, same package may be defined in multiple repositories with different versions (or not). The mechanism that determines which package version will be installed is called preferences (consult: man apt_preferences ). Used to pick package version when apt list files ( /etc/apt/sources.list or /etc/apt/sources.list.d/*.list ) contains references to more than one distribution eg. contains both stable and testing repositories. 1. Normally without using apt_preferences the apt will pick the package coming from the first entry in sources.list . 2. If no preferences exists or contains no entry for given package then the package priority is the priority of distro. If preference exists for given package then its value is used. 3. To \u201cforce\u201d usage of some other distro, use -t flag (specify target release), eg. apt-get install -t experimental some-package . If -t is given then check man apt_preferences for algorithm that assign priority for package. It roughly specifies: - If target release contains in release files \u201cNotAutomatic: yes\u201d but not \u201cButAutomaticUpgrades: yes\u201d . Then the priority is 1 - If package is installed or if target release\u2019s release file contains \u201cNotAutomatic: yes\u201d and \u201cButAutomaticUpgrades: yes\u201d . Then the priority is 100 - If the package is not installed and do not belong to the target release. Then the priority is 500 - If the package is not installed and belong to the target release. Then the priority is 990. 4. Installs version with highest priority (highest number). Please mind that if you have configured package pinning and e.g. your stable is configured to have priority of 995 (or anything greater than 990) then -t will have no effect . You can always verify with apt-cache policy -t target-release package which exact version of package is going to be installed Override preferences The package is always installed when package version (or target repository) is provided. To provide version use = e.g. apt-get install firefox-esr=60.6.1esr-1~deb9u1 To provide target repository use forward slash \\ e.g. apt-get install firefox-esr/stretch-updates Preferences files Located in /etc/apt/preferences.d Parsed in alphanumeric ascending order Need to obey convention: 1. No filename extension or .pref 2. Filename chars allowed: alphanumeric, hyphen, underscore and period The file itself contains records separated by blank lines. Preference refers to the mentioned package(s) and the mentioned package(s) only (doesn't affect its dependencies). This is one of the reasons why this mechanism is kind of \"discouraged\". Typical pref file: Package: * Pin: release o=Debian,a=testing Pin-Priority: 900 Configuration options: Option Meaning Pin-Priority The priority Package To which package does the rule apply to. Can be regex Pin More complex rule to match packages on. Using Pin it is possible to point the desired repository that will be used to fetch package Pin Possible Pin options (and they should be defined) are rather poorly documented. Using Pin it is possible to point the exact repository to be used for given package(s). Pin to version Without regards to repository holding the perl package, this Pin will assign 1001 priority to every perl package version matching 5.8* Package: perl Pin: version 5.8* Pin-Priority: 1001 Pin to origin Assiging the priority by the repository URL only. Package: * Pin: origin ftp.de.debian.org Pin-Priority: 980 Pin to Release file To get the values that it is possible to pin to , download the desired repository's Release file, e.g.: Archive: Debian_9.0 Codename: Debian_9.0 Origin: obs://build.opensuse.org/isv:ownCloud:desktop/Debian_9.0 Label: isv:ownCloud:desktop Architectures: amd64 Date: Thu Mar 21 19:36:06 2019 Description: qt depenendencies for client 2.5.x (Debian_9.0) MD5Sum: ... SHA1: ... SHA256: ... In order to use Origin field: Package: * Pin: release o=obs://build.opensuse.org/isv:ownCloud:desktop/Debian_9.0 Pin-Priority: 995 The fields: Release file field Pin option Archive , Suite a Codename n Version v Component c Origin o Label l Entries can be concatenated with , e.g.: Pin: release o=Debian Mozilla Team,c=iceweasel-aurora In order to debug what package version is going to be installed use: apt-cache policy the_package Package configuration To display package configuration files use: debconf-show the_package To set package configuration option use (wireshark example, using here-string): debconf-set-selections <<< 'wireshark-common wireshark-common/install-setuid boolean false' References https://wiki.debian.org/AptPreferences https://wiki.debian.org/UnattendedUpgrades https://www.debian.org/doc/manuals/repository-howto/repository-howto https://dug.net.pl/tekst/163/priorytety_pakietow_(apt_pinning__pin_priority)/ https://www.debian.org/doc/manuals/maint-guide/index.en.html https://www.debian.org/doc/debian-policy/index.html","title":"Packages"},{"location":"os/package-management/#packages","text":"Typically packages are nothing more that some kind of archive. However this archive besides the actual files to be copied to the underlying system contains a lot of additional configuration scripts.","title":"Packages"},{"location":"os/package-management/#deb","text":"AR archive with following 3 top-level files included: 1. control.tar.gz - control files. Theoretically it must contain following files: - control kind of key-value file. Contains information about packaged data (like: author, some description, dependencies) Fields (some of them): Source - name of the source package Package - name of the binary package Section - mainly for front-ends that display packages grouped into categories Priority - how important for the user the package is. For front-ends use (mainly when the apt -like software selects default packages for user or solves conflicts) Architecture - if this package is architecture dependent ( any ) or not ( all - e.g. Java applications) Depends - the package will not be installed unless the packages it depends on are installed - changelog dpkg uses this files to obtain version information, distribution, urgency etc of the package - copyright copyright and license Some of optional files: - conffiles list of packaged application configuration files. When upgrading package you'll be asked what to do with any file listed here that changed on the filesystem - package.cron.* e.g. my_app.cron.hourly will be installed as /etc/cron.hourly/my_app and will run every hour - dirs specifies any directories which we need but which are not created by the normal installation procedure - md5sums used to verify if installed files have been modified 2. data.tar.gz (since Debian 8: data.tar.xz ) - actual files 3. debian-binary","title":"DEB"},{"location":"os/package-management/#rpm","text":"todo","title":"RPM"},{"location":"os/package-management/#aptaptitude","text":"Highest priority number means highest priority of package.","title":"APT/Aptitude"},{"location":"os/package-management/#cheat-sheet","text":"command description aptitude versions the_package , apt-cache policy the_package , dpkg-query -l \\| grep the_package show available/installed package version apt-cache depends the_package show package dependencies apt-get install --only-upgrade the_package upgrade/downgrade only one specific package apt-get autoremove --purge Remove dangling packages aptitude autoclean remove old cached packages aptitude clean remove cached packages dpkg -S /bin/ping show to which package does the file belong to","title":"Cheat sheet"},{"location":"os/package-management/#configuration","text":"APT configuration can be dumped with: apt-config dump To reload configuration changes (either repositories changes or apt.conf.d/* changes use: apt-get update","title":"Configuration"},{"location":"os/package-management/#preferences","text":"In general, same package may be defined in multiple repositories with different versions (or not). The mechanism that determines which package version will be installed is called preferences (consult: man apt_preferences ). Used to pick package version when apt list files ( /etc/apt/sources.list or /etc/apt/sources.list.d/*.list ) contains references to more than one distribution eg. contains both stable and testing repositories. 1. Normally without using apt_preferences the apt will pick the package coming from the first entry in sources.list . 2. If no preferences exists or contains no entry for given package then the package priority is the priority of distro. If preference exists for given package then its value is used. 3. To \u201cforce\u201d usage of some other distro, use -t flag (specify target release), eg. apt-get install -t experimental some-package . If -t is given then check man apt_preferences for algorithm that assign priority for package. It roughly specifies: - If target release contains in release files \u201cNotAutomatic: yes\u201d but not \u201cButAutomaticUpgrades: yes\u201d . Then the priority is 1 - If package is installed or if target release\u2019s release file contains \u201cNotAutomatic: yes\u201d and \u201cButAutomaticUpgrades: yes\u201d . Then the priority is 100 - If the package is not installed and do not belong to the target release. Then the priority is 500 - If the package is not installed and belong to the target release. Then the priority is 990. 4. Installs version with highest priority (highest number). Please mind that if you have configured package pinning and e.g. your stable is configured to have priority of 995 (or anything greater than 990) then -t will have no effect . You can always verify with apt-cache policy -t target-release package which exact version of package is going to be installed","title":"Preferences"},{"location":"os/package-management/#override-preferences","text":"The package is always installed when package version (or target repository) is provided. To provide version use = e.g. apt-get install firefox-esr=60.6.1esr-1~deb9u1 To provide target repository use forward slash \\ e.g. apt-get install firefox-esr/stretch-updates","title":"Override preferences"},{"location":"os/package-management/#preferences-files","text":"Located in /etc/apt/preferences.d Parsed in alphanumeric ascending order Need to obey convention: 1. No filename extension or .pref 2. Filename chars allowed: alphanumeric, hyphen, underscore and period The file itself contains records separated by blank lines. Preference refers to the mentioned package(s) and the mentioned package(s) only (doesn't affect its dependencies). This is one of the reasons why this mechanism is kind of \"discouraged\". Typical pref file: Package: * Pin: release o=Debian,a=testing Pin-Priority: 900 Configuration options: Option Meaning Pin-Priority The priority Package To which package does the rule apply to. Can be regex Pin More complex rule to match packages on. Using Pin it is possible to point the desired repository that will be used to fetch package","title":"Preferences files"},{"location":"os/package-management/#pin","text":"Possible Pin options (and they should be defined) are rather poorly documented. Using Pin it is possible to point the exact repository to be used for given package(s).","title":"Pin"},{"location":"os/package-management/#pin-to-version","text":"Without regards to repository holding the perl package, this Pin will assign 1001 priority to every perl package version matching 5.8* Package: perl Pin: version 5.8* Pin-Priority: 1001","title":"Pin to version"},{"location":"os/package-management/#pin-to-origin","text":"Assiging the priority by the repository URL only. Package: * Pin: origin ftp.de.debian.org Pin-Priority: 980","title":"Pin to origin"},{"location":"os/package-management/#pin-to-release-file","text":"To get the values that it is possible to pin to , download the desired repository's Release file, e.g.: Archive: Debian_9.0 Codename: Debian_9.0 Origin: obs://build.opensuse.org/isv:ownCloud:desktop/Debian_9.0 Label: isv:ownCloud:desktop Architectures: amd64 Date: Thu Mar 21 19:36:06 2019 Description: qt depenendencies for client 2.5.x (Debian_9.0) MD5Sum: ... SHA1: ... SHA256: ... In order to use Origin field: Package: * Pin: release o=obs://build.opensuse.org/isv:ownCloud:desktop/Debian_9.0 Pin-Priority: 995 The fields: Release file field Pin option Archive , Suite a Codename n Version v Component c Origin o Label l Entries can be concatenated with , e.g.: Pin: release o=Debian Mozilla Team,c=iceweasel-aurora In order to debug what package version is going to be installed use: apt-cache policy the_package","title":"Pin to Release file"},{"location":"os/package-management/#package-configuration","text":"To display package configuration files use: debconf-show the_package To set package configuration option use (wireshark example, using here-string): debconf-set-selections <<< 'wireshark-common wireshark-common/install-setuid boolean false'","title":"Package configuration"},{"location":"os/package-management/#references","text":"https://wiki.debian.org/AptPreferences https://wiki.debian.org/UnattendedUpgrades https://www.debian.org/doc/manuals/repository-howto/repository-howto https://dug.net.pl/tekst/163/priorytety_pakietow_(apt_pinning__pin_priority)/ https://www.debian.org/doc/manuals/maint-guide/index.en.html https://www.debian.org/doc/debian-policy/index.html","title":"References"},{"location":"os/fs/btrfs/","text":"Setup Preliminary: 1. Kernel with btrfs support (located at /lib/modules/$(uname -r)/kernel/fs/btrfs/btrfs.ko ) 2. Userspace tools: apt-get install btrfs-tools (mainly for filesystem creation, conversion, etc.) In order to create filesystem on device (even not partitioned one): mkfs.btrfs /dev/sdb /dev/sdc /dev/sdd References https://www.howtoforge.com/a-beginners-guide-to-btrfs https://wiki.debian.org/Btrfs http://marc.merlins.org/perso/btrfs/post_2014-05-04_Fixing-Btrfs-Filesystem-Full-Problems.html https://btrfs.wiki.kernel.org/index.php/Balance_Filters","title":"BTRFS"},{"location":"os/fs/btrfs/#setup","text":"Preliminary: 1. Kernel with btrfs support (located at /lib/modules/$(uname -r)/kernel/fs/btrfs/btrfs.ko ) 2. Userspace tools: apt-get install btrfs-tools (mainly for filesystem creation, conversion, etc.) In order to create filesystem on device (even not partitioned one): mkfs.btrfs /dev/sdb /dev/sdc /dev/sdd","title":"Setup"},{"location":"os/fs/btrfs/#references","text":"https://www.howtoforge.com/a-beginners-guide-to-btrfs https://wiki.debian.org/Btrfs http://marc.merlins.org/perso/btrfs/post_2014-05-04_Fixing-Btrfs-Filesystem-Full-Problems.html https://btrfs.wiki.kernel.org/index.php/Balance_Filters","title":"References"},{"location":"os/fs/cifs/","text":"Mount In order to mount CIFS share on linux, following options are possible: mount with explicit password mount with password in plaintext in file with chmod 600 use pam_mount: /etc/security/pam_mount.conf.xml <?xml version=\"1.0\" encoding=\"utf-8\" ?> <!DOCTYPE pam_mount SYSTEM \"pam_mount.conf.xml.dtd\"> <pam_mount> ... uncomment this: <luserconf name=\".pam_mount.conf.xml\" /> ... </pam_mount> and create ~/.pam_mount.conf.xml with: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <pam_mount> <volume fstype=\"cifs\" server=\"server_host\" user=\"*\" path=\"%(USER)\" mountpoint=\"/mnt/%(USER)\" options=\"nosuid,nodev\" /> ... </pam_mount> this requires that cifs share for given user has same password as unix user","title":"CIFS"},{"location":"os/fs/cifs/#mount","text":"In order to mount CIFS share on linux, following options are possible: mount with explicit password mount with password in plaintext in file with chmod 600 use pam_mount: /etc/security/pam_mount.conf.xml <?xml version=\"1.0\" encoding=\"utf-8\" ?> <!DOCTYPE pam_mount SYSTEM \"pam_mount.conf.xml.dtd\"> <pam_mount> ... uncomment this: <luserconf name=\".pam_mount.conf.xml\" /> ... </pam_mount> and create ~/.pam_mount.conf.xml with: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <pam_mount> <volume fstype=\"cifs\" server=\"server_host\" user=\"*\" path=\"%(USER)\" mountpoint=\"/mnt/%(USER)\" options=\"nosuid,nodev\" /> ... </pam_mount> this requires that cifs share for given user has same password as unix user","title":"Mount"},{"location":"security/crypto/","text":"Standards Certificates (keys) are stored in wide range of formats. ASN.1 defines the structure of the key/certificate which can later be saved as binary (DER) or 'textual' (PEM) PEM Base64 translation of the x509 ASN.1 keys placed between well-known delimeters (e.g. -----BEGIN PRIVATE KEY----- ) DER x509 ASN.1 keys Formats PKCS#1 PKCS#8 Certificates Asymmetric, public key cryptography using trusted institution certifying ownership of public key. Setting key-pair (with self-signed CA) for server consists of following steps: create private key for CA create self-signed CA cert create private key for server create csr (certificate signing request) for server sign csr using CA Multiple parameters asked during certificate creation can be specified using config files: https://www.openssl.org/docs/manmaster/apps/config.html References https://tls.mbed.org/kb/cryptography/asn1-key-structures-in-der-and-pem","title":"Standards"},{"location":"security/crypto/#standards","text":"Certificates (keys) are stored in wide range of formats. ASN.1 defines the structure of the key/certificate which can later be saved as binary (DER) or 'textual' (PEM)","title":"Standards"},{"location":"security/crypto/#pem","text":"Base64 translation of the x509 ASN.1 keys placed between well-known delimeters (e.g. -----BEGIN PRIVATE KEY----- )","title":"PEM"},{"location":"security/crypto/#der","text":"x509 ASN.1 keys","title":"DER"},{"location":"security/crypto/#formats","text":"","title":"Formats"},{"location":"security/crypto/#pkcs1","text":"","title":"PKCS#1"},{"location":"security/crypto/#pkcs8","text":"","title":"PKCS#8"},{"location":"security/crypto/#certificates","text":"Asymmetric, public key cryptography using trusted institution certifying ownership of public key. Setting key-pair (with self-signed CA) for server consists of following steps: create private key for CA create self-signed CA cert create private key for server create csr (certificate signing request) for server sign csr using CA Multiple parameters asked during certificate creation can be specified using config files: https://www.openssl.org/docs/manmaster/apps/config.html","title":"Certificates"},{"location":"security/crypto/#references","text":"https://tls.mbed.org/kb/cryptography/asn1-key-structures-in-der-and-pem","title":"References"},{"location":"uncategorized/bench_io/","text":"Tools dd hdparm iotop FIO Tool for benchmarking IO. Can spawn multiple threads performing different kind of work. Options worth mentioning: direct=<bool> - uses non-buffered IO ( O_DIRECT ) Testing scenarios Random read-write fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=random_read_write.fio --bs=4k --iodepth=64 --size=4G --readwrite=randrw --rwmixread=75 References https://dotlayer.com/how-to-use-fio-to-measure-disk-performance-in-linux/ https://wiki.mikejung.biz/Benchmarking#Fio_Test_Options_and_Examples","title":"IO benchmark"},{"location":"uncategorized/bench_io/#tools","text":"dd hdparm iotop","title":"Tools"},{"location":"uncategorized/bench_io/#fio","text":"Tool for benchmarking IO. Can spawn multiple threads performing different kind of work. Options worth mentioning: direct=<bool> - uses non-buffered IO ( O_DIRECT )","title":"FIO"},{"location":"uncategorized/bench_io/#testing-scenarios","text":"","title":"Testing scenarios"},{"location":"uncategorized/bench_io/#random-read-write","text":"fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=random_read_write.fio --bs=4k --iodepth=64 --size=4G --readwrite=randrw --rwmixread=75","title":"Random read-write"},{"location":"uncategorized/bench_io/#references","text":"https://dotlayer.com/how-to-use-fio-to-measure-disk-performance-in-linux/ https://wiki.mikejung.biz/Benchmarking#Fio_Test_Options_and_Examples","title":"References"},{"location":"uncategorized/kde/","text":"Krunner Desktop entries are located in: - /usr/share/applications - ~/.local/share/applications/ References https://userbase.kde.org/Plasma/Krunner","title":"KDE"},{"location":"uncategorized/kde/#krunner","text":"Desktop entries are located in: - /usr/share/applications - ~/.local/share/applications/","title":"Krunner"},{"location":"uncategorized/kde/#references","text":"https://userbase.kde.org/Plasma/Krunner","title":"References"},{"location":"virt/KVM/","text":"libvirt Requirements Install with: aptitude install qemu-kvm libvirt-clients libvirt-daemon-system In order to permit user for vm management adduser <youruser> libvirt adduser <youruser> libvirt-qemu Configuration Add storage pool virsh -c qemu:///system pool-define-as $pool_name dir --target $location References https://wiki.debian.org/KVM https://libvirt.org/docs.html http://rabexc.org/posts/how-to-get-started-with-libvirt-on","title":"KVM"},{"location":"virt/KVM/#libvirt","text":"","title":"libvirt"},{"location":"virt/KVM/#requirements","text":"Install with: aptitude install qemu-kvm libvirt-clients libvirt-daemon-system In order to permit user for vm management adduser <youruser> libvirt adduser <youruser> libvirt-qemu","title":"Requirements"},{"location":"virt/KVM/#configuration","text":"Add storage pool virsh -c qemu:///system pool-define-as $pool_name dir --target $location","title":"Configuration"},{"location":"virt/KVM/#references","text":"https://wiki.debian.org/KVM https://libvirt.org/docs.html http://rabexc.org/posts/how-to-get-started-with-libvirt-on","title":"References"},{"location":"virt/LXC/","text":"Container's filesystem root location: /var/lib/lxc/<container name> Container creation scripts/templates for given distro: /usr/share/lxc/templates To verify if LXC is supported on current OS/kernel run: lxc-checkconfig Debian may have some issues with memory control via cgroups. Check kernel support: cat /boot/config-$(uname -r) | grep CONFIG_MEMCG If output contains both CONFIG_MEMCG=y and CONFIG_MEMCG_DISABLED=y means that memory cgroups must be explicitly enabled by kernel parameter ( cgroup_enable=memory ) Networking Described in configuration file lxc-create -f <config_file> Create bridge interface on host OS and link to container: lxc.network.type = veth lxc.network.flags = up lxc.network.link = br0 lxc.network.type = veth lxc.network.flags = up lxc.network.link = br1 lxc.network.type = empty For \"isolated\" bridge interface for containers, configure lxc-net . Refer to 4 References https://wiki.debian.org/LXC https://www.stgraber.org/2013/12/20/lxc-1-0-blog-post-series/ https://www.flockport.com/guides/ https://wiki.debian.org/LXC/SimpleBridge http://man7.org/linux/man-pages/man5/lxc.container.conf.5.html https://wiki.debian.org/BridgeNetworkConnections","title":"LXC"},{"location":"virt/LXC/#networking","text":"Described in configuration file lxc-create -f <config_file> Create bridge interface on host OS and link to container: lxc.network.type = veth lxc.network.flags = up lxc.network.link = br0 lxc.network.type = veth lxc.network.flags = up lxc.network.link = br1 lxc.network.type = empty For \"isolated\" bridge interface for containers, configure lxc-net . Refer to 4","title":"Networking"},{"location":"virt/LXC/#references","text":"https://wiki.debian.org/LXC https://www.stgraber.org/2013/12/20/lxc-1-0-blog-post-series/ https://www.flockport.com/guides/ https://wiki.debian.org/LXC/SimpleBridge http://man7.org/linux/man-pages/man5/lxc.container.conf.5.html https://wiki.debian.org/BridgeNetworkConnections","title":"References"},{"location":"virt/VFIO/","text":"Virtual Function I/O Set of technologies that allows to 'push' device(s) to virtual machine. The device is managed by the VM only, 'typical' use case for Linux hosts: Windows guest VM with GPU access. Allows writing userspace drivers. GPU passthrough Debian setup Ensure: > egrep -q '^flags.*(svm|vmx)' /proc/cpuinfo && echo virtualization extensions available virtualization extensions available > aptitude install qemu-kvm ... Enable IOMMU (mapping of device address to main memory) : > cat /etc/default/grub.d/vfio.cfg GRUB_CMDLINE_LINUX_DEFAULT=\"quiet intel_iommu=on iommu=pt\" Find PCI port, using script that groups devices by their IOMMU groups #!/bin/bash shopt -s nullglob for g in /sys/kernel/iommu_groups/*; do echo \"IOMMU Group ${g##*/}:\" for d in $g/devices/*; do echo -e \"\\t$(lspci -nns ${d##*/})\" done; done; It may not be possible to 'push' single device to VM, normally whole IOMMU group is 'pushed'. This means that all devices belonging to such group won't be available on host. > cat /usr/share/initramfs-tools/modules.d/vfio vfio_pci ids=10df:1122,10de:1133,10de:1aaa,11de:bbbb vfio_iommu_type1 > update-initramfs -u -k all Disable any drivers that may take over device before VFIO (kernel) > cat /etc/modprobe.d/nvidia.conf softdep nouveau pre: vfio-pci softdep nvidia pre: vfio-pci softdep nvidia* pre: vfio-pci > cat /etc/modprobe.d/blacklist-nvidia-nouveau.conf blacklist nouveau options nouveau modeset=0 Reboot, and verify > lspci -v 01:00.0 VGA compatible controller: NVIDIA ... [GeForce ... Rev. A] (rev ...) (prog-if 00 [VGA controller]) ... Kernel driver in use: vfio-pci ... Create VM, attach GPU, configure evdev so that same mouse is shared between host and guest. References https://www.kernel.org/doc/Documentation/vfio.txt https://passthroughpo.st/gpu-debian/ https://passthroughpo.st/using-evdev-passthrough-seamless-vm-input/ https://mathiashueber.com/windows-virtual-machine-gpu-passthrough-ubuntu/ https://wiki.debian.org/VGAPassthrough","title":"VFIO"},{"location":"virt/VFIO/#virtual-function-io","text":"Set of technologies that allows to 'push' device(s) to virtual machine. The device is managed by the VM only, 'typical' use case for Linux hosts: Windows guest VM with GPU access. Allows writing userspace drivers.","title":"Virtual Function I/O"},{"location":"virt/VFIO/#gpu-passthrough-debian-setup","text":"Ensure: > egrep -q '^flags.*(svm|vmx)' /proc/cpuinfo && echo virtualization extensions available virtualization extensions available > aptitude install qemu-kvm ... Enable IOMMU (mapping of device address to main memory) : > cat /etc/default/grub.d/vfio.cfg GRUB_CMDLINE_LINUX_DEFAULT=\"quiet intel_iommu=on iommu=pt\" Find PCI port, using script that groups devices by their IOMMU groups #!/bin/bash shopt -s nullglob for g in /sys/kernel/iommu_groups/*; do echo \"IOMMU Group ${g##*/}:\" for d in $g/devices/*; do echo -e \"\\t$(lspci -nns ${d##*/})\" done; done; It may not be possible to 'push' single device to VM, normally whole IOMMU group is 'pushed'. This means that all devices belonging to such group won't be available on host. > cat /usr/share/initramfs-tools/modules.d/vfio vfio_pci ids=10df:1122,10de:1133,10de:1aaa,11de:bbbb vfio_iommu_type1 > update-initramfs -u -k all Disable any drivers that may take over device before VFIO (kernel) > cat /etc/modprobe.d/nvidia.conf softdep nouveau pre: vfio-pci softdep nvidia pre: vfio-pci softdep nvidia* pre: vfio-pci > cat /etc/modprobe.d/blacklist-nvidia-nouveau.conf blacklist nouveau options nouveau modeset=0 Reboot, and verify > lspci -v 01:00.0 VGA compatible controller: NVIDIA ... [GeForce ... Rev. A] (rev ...) (prog-if 00 [VGA controller]) ... Kernel driver in use: vfio-pci ... Create VM, attach GPU, configure evdev so that same mouse is shared between host and guest.","title":"GPU passthrough Debian setup"},{"location":"virt/VFIO/#references","text":"https://www.kernel.org/doc/Documentation/vfio.txt https://passthroughpo.st/gpu-debian/ https://passthroughpo.st/using-evdev-passthrough-seamless-vm-input/ https://mathiashueber.com/windows-virtual-machine-gpu-passthrough-ubuntu/ https://wiki.debian.org/VGAPassthrough","title":"References"},{"location":"virt/containerization/","text":"Containerization Often seen as na advancement from chroot technique. For given process and its children chroot creates the filesystem 'isolation' only. Containers provide more: process resources are isolated via kernel namespaces and resource usage is controlled via cgroups. Both the host and container share the same kernel . The \"container\" is not a in-kernel term. The closest definition: namespaces + cgroups = containers. Concepts Following linux kernel concepts are the building blocks of containers Namespaces Wrapper over global system resources. For all processes within the namespace makes the system resources appear like isolated (dedicated) instance. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. By default linux provides following namespaces namespace isolates Cgroup Cgroup root directory. Processes inside this namespace are only able to view paths relative to their namespace root IPC System V IPC, POSIX message queues Network Network devices, ports , etc. Mount Mount points PID Process IDs User User and group IDs UTS Hostname and NIS domain Each namespace is assigned unique inode number: > ls /proc/7320/ns -al dr-x--x--x 2 thedude thedude 0 May 20 20:19 . dr-xr-xr-x 9 thedude thedude 0 May 20 19:29 .. lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 cgroup -> cgroup:[4076531835] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 ipc -> ipc:[4026537839] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 mnt -> mnt:[4026537840] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 net -> net:[4026537969] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 pid -> pid:[4026537836] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 user -> user:[4026731837] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 uts -> uts:[4026571838] userspace tools lsns Control groups Usually referred as cgroups. Linux feature that organises processes into hierarchical groups whose usage of various types of resources can be limited and monitored Union file systems TODO Benefits faster than full VM (doesn't require running hypervisor) easier to provision (doesn't require full VM setup) cheaper better resource utilization easier to scale Solutions Docker LXC References Modern Linux Administration - Sam R. Alapati","title":"Containerization"},{"location":"virt/containerization/#containerization","text":"Often seen as na advancement from chroot technique. For given process and its children chroot creates the filesystem 'isolation' only. Containers provide more: process resources are isolated via kernel namespaces and resource usage is controlled via cgroups. Both the host and container share the same kernel . The \"container\" is not a in-kernel term. The closest definition: namespaces + cgroups = containers.","title":"Containerization"},{"location":"virt/containerization/#concepts","text":"Following linux kernel concepts are the building blocks of containers","title":"Concepts"},{"location":"virt/containerization/#namespaces","text":"Wrapper over global system resources. For all processes within the namespace makes the system resources appear like isolated (dedicated) instance. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. By default linux provides following namespaces namespace isolates Cgroup Cgroup root directory. Processes inside this namespace are only able to view paths relative to their namespace root IPC System V IPC, POSIX message queues Network Network devices, ports , etc. Mount Mount points PID Process IDs User User and group IDs UTS Hostname and NIS domain Each namespace is assigned unique inode number: > ls /proc/7320/ns -al dr-x--x--x 2 thedude thedude 0 May 20 20:19 . dr-xr-xr-x 9 thedude thedude 0 May 20 19:29 .. lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 cgroup -> cgroup:[4076531835] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 ipc -> ipc:[4026537839] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 mnt -> mnt:[4026537840] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 net -> net:[4026537969] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 pid -> pid:[4026537836] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 user -> user:[4026731837] lrwxrwxrwx 1 thedude thedude 0 May 20 20:19 uts -> uts:[4026571838]","title":"Namespaces"},{"location":"virt/containerization/#userspace-tools","text":"lsns","title":"userspace tools"},{"location":"virt/containerization/#control-groups","text":"Usually referred as cgroups. Linux feature that organises processes into hierarchical groups whose usage of various types of resources can be limited and monitored","title":"Control groups"},{"location":"virt/containerization/#union-file-systems","text":"TODO","title":"Union file systems"},{"location":"virt/containerization/#benefits","text":"faster than full VM (doesn't require running hypervisor) easier to provision (doesn't require full VM setup) cheaper better resource utilization easier to scale","title":"Benefits"},{"location":"virt/containerization/#solutions","text":"Docker LXC","title":"Solutions"},{"location":"virt/containerization/#references","text":"Modern Linux Administration - Sam R. Alapati","title":"References"},{"location":"virt/docker/","text":"Originally LXC-based, widespread. Created for shipping and running applications. Encourages running one process per container . Basics term meaning image Packaged application along with environment. Product of docker build command. Snapshot of a container. Analogous to the concept from Object Oriented Programming : class container Running instance of image. Analogous to the concept from Object Oriented Programming : class instance tag Extra information appended to image name. Helpful for enclosing image version information, if omitted: latest is assumed registry Repository that stores Docker Images, can be public or private Dockerfile Text file, contains commands that assemble image Each image is identified by ID, can contain different names and tags. Image name contains of slash delimited name components, with optionally repository name prefixed. If the name doesn't contain registry name, then the public docker.io is assumed. The process specified as ENTRYPOINT in Dockerfile becomes PID 1 process, it means that it will not handle SIGKILL signal. The container lifetime is equal to the encapsulating ( ENTRYPOINT s) process lifetime. The process dies, the container dies too. Usage Common commands: operation command Build image from Dockerfile . Invoke passing directory containing Dockerfile or pass the dockerfile with -f . The mandatory argument is called build context - all dockerfile instructions are relative to that path and all non-ignored files from that directory are send to Docker engine docker build -t my-tag . Read more Create and start container (simplest form) docker run --name <some_name> <image tag or name> [args] Stop container docker stop <container name or id> Remove container docker rm <container name or id> Remove image docker rmi <image name or id> Push image to registry Tag image with repository URL first. Mind that repo may support the v1 and/or v2 'naming' format. Briefly speaking: v1 format doesn't support multiple path segments in image name, thus image: rootName/subName is invalid. For v2 it is fine: docker tag <image> <repourl>/<name>:<tag> Login to desired repository, docker.io is the default docker login <repourl> docker push <repourl>/<name>:<tag> Show running containers docker ps Show container/image details docker inspect container_or_image_name Building images Dockerfile contains all of the needed instructions to build the image with given application. Each instruction corresponds to filesystem layer. Image is built within build context (the directory passed to docker build command - simply speaking). All COPY -kind instructions are relative to that build context . The bigger the build context (directory contents and size) the longer it takes to send them to Docker engine. Design goals when building images There is a lot of things to keep in mind when building images. Many of them will be containerized-application specific: different set of guidelies for JVM application, python application, etc. However there are some fundamental, containerized-application agnostic set of rules: image size The image is never downloaded once and there is a difference when downloading over 1GB image vs 100MB. Storage is also limited especially when images doesn't re-use the layers. Huge images impact the downtime of your application, the bigger the image, the longer it takes to download it, the longer the downtime. When the Kubernetes reschedules the POD to another Node which doesn't contain the image (or the image has changed), the time user waits for application is also longer Security the fewer the libraries in container the better, The main goal when building the image should be: - minimize the number of layers, the new layer is created only with: COPY , RUN and ADD commands. - use cache as much as possible. When the Dockerfile processed, each instruction is examined if its outcome is present in the cache already. The ADD and COPY will check if the underlying file has changed, the RUN commands will check only if the command string has not changed. Thus building the image with RUN apt-get update second time won't update the latest packages. - once the cache is invalidated (instruction that misses cache occurred), all subsequent instructions won't check the cache The verify layer re-use: docker system df -v one application in container management of the image that ships one application is easier: one log source, one application to monitor. Usage is easier too, as there is no need for supervisord -like tools that govern multiple applications inside one container. decoupled dependencies, easier to re-deploy process reaping When the (main) application spawns processes inside of container it's the application's responsibility to cleanup process after it completes. Usually it was done by PID 1 init process, but the docker lacks one (by default at least) Since container uses its own pid namespace, as long as the container lives, the host's init process cannot clean orphaned processes. Image types There are two types of images in docker terminology: - parent image - the image that is specified in FROM clause. Your image is based on parent image . Your image after build becomes parent image. - base image - the image that has no FROM or FROM scratch Most Dockerfile s use parent images in their FROM clause. Building base images In order to build base image: - use debootstrap for Debian-based distributions (tool that installs Debian-based distributions into given filesystem) - archive it and docker import image.tar name It is also possible to build base image from Dockerfile : FROM scratch ADD some_binary / CMD [\"/some_binary\"] Such image will be able to execute the specified binary only. Debugging Failed container can be pretty easily debugged 1. Grab exited container ID docker ps -a 2. Create image docker commit <container id> 3. Run image corresponding to failed container, overriding its potential ENTRYPOINT docker run -it --entrypoint /bin/bash <image id> Live container can be debugged as well: 1. Grab the container name or ID: docker ps 2. Attach to the container with: docker exec -it <container id> /bin/bash Networking Networking system in Docker is configurable. It is possible to set different networking driver for containers using: docker run --network=host ... bridge Default driver, creates the bridge that connects all containers withing Docker host. Docker host manages IP addresses and iptables rules governing inbound/outbound traffic. From Docker host it should be possible to reach any port on the running container (host has routing configured for containers subnet: ip r s ) However by default containers will be unreachable from outside (outside of Docker host). In order to tell Docker host, that it should add iptables rules allows access to containers the port publish option must be used: docker run... -p 8080:80 ... . The -p hostPort:containerPort maps (TCP by default) port 8080 on the host to the container port 80. host Uses the host networking interfaces. No network isolation. The ip a output from within the container will print same entries as ip a on the Docker host overlay Use when multiple containers need to communicate with each other and containers are scattered among different Docker hosts macvlan TODO none Disables networking custom TODO Volumes TODO Configuration dockerd uses following configuration files: /etc/docker/daemon.json /etc/default/docker (for Debian based OSes) Change storage-driver: > cat /etc/docker/daemon.json { \"storage-driver\": \"btrfs\" } For full list of supported drivers refer to storage drivers Change dockerd sockets: > cat /etc/docker/daemon.json { \"hosts\": [\"unix:///var/run/docker.sock\", \"tcp://0.0.0.0:2376\"] } It is possible that run init/upstart/service script specifies -H flag for startup command. This must be removed from script itself, otherwise dockerd will fail to start Underlying technology Several linux kernel features are used by Docker. Namespaces 5 Linux namespaces are used in Docker implementation pid net ipc mnt uts The cgoup namespace is not used. Control groups ( cgroups ) Allows limiting available resources (e.g. hardware resources) for containers. For instance allows to specify available amount of memory Union file systems UnionFS Container format The mix of aforementioned three (namespaces, cgroups and union fs) is called the container format. The current default: libcontainer . This abstractions will allow other formats like BSD Jalis to be used interchangeably References https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#label https://lwn.net/Articles/621006/ https://www.slideshare.net/kerneltlv/namespaces-and-cgroups-the-basis-of-linux-containers https://codefresh.io/docker-tutorial/java_docker_pipeline/ https://docs.docker.com/get-started/overview/ https://stackoverflow.com/questions/49162358/docker-init-zombies-why-does-it-matter","title":"Docker"},{"location":"virt/docker/#basics","text":"term meaning image Packaged application along with environment. Product of docker build command. Snapshot of a container. Analogous to the concept from Object Oriented Programming : class container Running instance of image. Analogous to the concept from Object Oriented Programming : class instance tag Extra information appended to image name. Helpful for enclosing image version information, if omitted: latest is assumed registry Repository that stores Docker Images, can be public or private Dockerfile Text file, contains commands that assemble image Each image is identified by ID, can contain different names and tags. Image name contains of slash delimited name components, with optionally repository name prefixed. If the name doesn't contain registry name, then the public docker.io is assumed. The process specified as ENTRYPOINT in Dockerfile becomes PID 1 process, it means that it will not handle SIGKILL signal. The container lifetime is equal to the encapsulating ( ENTRYPOINT s) process lifetime. The process dies, the container dies too.","title":"Basics"},{"location":"virt/docker/#usage","text":"Common commands: operation command Build image from Dockerfile . Invoke passing directory containing Dockerfile or pass the dockerfile with -f . The mandatory argument is called build context - all dockerfile instructions are relative to that path and all non-ignored files from that directory are send to Docker engine docker build -t my-tag . Read more Create and start container (simplest form) docker run --name <some_name> <image tag or name> [args] Stop container docker stop <container name or id> Remove container docker rm <container name or id> Remove image docker rmi <image name or id> Push image to registry Tag image with repository URL first. Mind that repo may support the v1 and/or v2 'naming' format. Briefly speaking: v1 format doesn't support multiple path segments in image name, thus image: rootName/subName is invalid. For v2 it is fine: docker tag <image> <repourl>/<name>:<tag> Login to desired repository, docker.io is the default docker login <repourl> docker push <repourl>/<name>:<tag> Show running containers docker ps Show container/image details docker inspect container_or_image_name","title":"Usage"},{"location":"virt/docker/#building-images","text":"Dockerfile contains all of the needed instructions to build the image with given application. Each instruction corresponds to filesystem layer. Image is built within build context (the directory passed to docker build command - simply speaking). All COPY -kind instructions are relative to that build context . The bigger the build context (directory contents and size) the longer it takes to send them to Docker engine.","title":"Building images"},{"location":"virt/docker/#design-goals-when-building-images","text":"There is a lot of things to keep in mind when building images. Many of them will be containerized-application specific: different set of guidelies for JVM application, python application, etc. However there are some fundamental, containerized-application agnostic set of rules:","title":"Design goals when building images"},{"location":"virt/docker/#image-size","text":"The image is never downloaded once and there is a difference when downloading over 1GB image vs 100MB. Storage is also limited especially when images doesn't re-use the layers. Huge images impact the downtime of your application, the bigger the image, the longer it takes to download it, the longer the downtime. When the Kubernetes reschedules the POD to another Node which doesn't contain the image (or the image has changed), the time user waits for application is also longer Security the fewer the libraries in container the better, The main goal when building the image should be: - minimize the number of layers, the new layer is created only with: COPY , RUN and ADD commands. - use cache as much as possible. When the Dockerfile processed, each instruction is examined if its outcome is present in the cache already. The ADD and COPY will check if the underlying file has changed, the RUN commands will check only if the command string has not changed. Thus building the image with RUN apt-get update second time won't update the latest packages. - once the cache is invalidated (instruction that misses cache occurred), all subsequent instructions won't check the cache The verify layer re-use: docker system df -v","title":"image size"},{"location":"virt/docker/#one-application-in-container","text":"management of the image that ships one application is easier: one log source, one application to monitor. Usage is easier too, as there is no need for supervisord -like tools that govern multiple applications inside one container. decoupled dependencies, easier to re-deploy","title":"one application in container"},{"location":"virt/docker/#process-reaping","text":"When the (main) application spawns processes inside of container it's the application's responsibility to cleanup process after it completes. Usually it was done by PID 1 init process, but the docker lacks one (by default at least) Since container uses its own pid namespace, as long as the container lives, the host's init process cannot clean orphaned processes.","title":"process reaping"},{"location":"virt/docker/#image-types","text":"There are two types of images in docker terminology: - parent image - the image that is specified in FROM clause. Your image is based on parent image . Your image after build becomes parent image. - base image - the image that has no FROM or FROM scratch Most Dockerfile s use parent images in their FROM clause.","title":"Image types"},{"location":"virt/docker/#building-base-images","text":"In order to build base image: - use debootstrap for Debian-based distributions (tool that installs Debian-based distributions into given filesystem) - archive it and docker import image.tar name It is also possible to build base image from Dockerfile : FROM scratch ADD some_binary / CMD [\"/some_binary\"] Such image will be able to execute the specified binary only.","title":"Building base images"},{"location":"virt/docker/#debugging","text":"Failed container can be pretty easily debugged 1. Grab exited container ID docker ps -a 2. Create image docker commit <container id> 3. Run image corresponding to failed container, overriding its potential ENTRYPOINT docker run -it --entrypoint /bin/bash <image id> Live container can be debugged as well: 1. Grab the container name or ID: docker ps 2. Attach to the container with: docker exec -it <container id> /bin/bash","title":"Debugging"},{"location":"virt/docker/#networking","text":"Networking system in Docker is configurable. It is possible to set different networking driver for containers using: docker run --network=host ...","title":"Networking"},{"location":"virt/docker/#bridge","text":"Default driver, creates the bridge that connects all containers withing Docker host. Docker host manages IP addresses and iptables rules governing inbound/outbound traffic. From Docker host it should be possible to reach any port on the running container (host has routing configured for containers subnet: ip r s ) However by default containers will be unreachable from outside (outside of Docker host). In order to tell Docker host, that it should add iptables rules allows access to containers the port publish option must be used: docker run... -p 8080:80 ... . The -p hostPort:containerPort maps (TCP by default) port 8080 on the host to the container port 80.","title":"bridge"},{"location":"virt/docker/#host","text":"Uses the host networking interfaces. No network isolation. The ip a output from within the container will print same entries as ip a on the Docker host","title":"host"},{"location":"virt/docker/#overlay","text":"Use when multiple containers need to communicate with each other and containers are scattered among different Docker hosts","title":"overlay"},{"location":"virt/docker/#macvlan","text":"TODO","title":"macvlan"},{"location":"virt/docker/#none","text":"Disables networking","title":"none"},{"location":"virt/docker/#custom","text":"TODO","title":"custom"},{"location":"virt/docker/#volumes","text":"TODO","title":"Volumes"},{"location":"virt/docker/#configuration","text":"dockerd uses following configuration files: /etc/docker/daemon.json /etc/default/docker (for Debian based OSes)","title":"Configuration"},{"location":"virt/docker/#change-storage-driver","text":"> cat /etc/docker/daemon.json { \"storage-driver\": \"btrfs\" } For full list of supported drivers refer to storage drivers","title":"Change storage-driver:"},{"location":"virt/docker/#change-dockerd-sockets","text":"> cat /etc/docker/daemon.json { \"hosts\": [\"unix:///var/run/docker.sock\", \"tcp://0.0.0.0:2376\"] } It is possible that run init/upstart/service script specifies -H flag for startup command. This must be removed from script itself, otherwise dockerd will fail to start","title":"Change dockerd sockets:"},{"location":"virt/docker/#underlying-technology","text":"Several linux kernel features are used by Docker.","title":"Underlying technology"},{"location":"virt/docker/#namespaces","text":"5 Linux namespaces are used in Docker implementation pid net ipc mnt uts The cgoup namespace is not used.","title":"Namespaces"},{"location":"virt/docker/#control-groups-cgroups","text":"Allows limiting available resources (e.g. hardware resources) for containers. For instance allows to specify available amount of memory","title":"Control groups (cgroups)"},{"location":"virt/docker/#union-file-systems","text":"UnionFS","title":"Union file systems"},{"location":"virt/docker/#container-format","text":"The mix of aforementioned three (namespaces, cgroups and union fs) is called the container format. The current default: libcontainer . This abstractions will allow other formats like BSD Jalis to be used interchangeably","title":"Container format"},{"location":"virt/docker/#references","text":"https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#label https://lwn.net/Articles/621006/ https://www.slideshare.net/kerneltlv/namespaces-and-cgroups-the-basis-of-linux-containers https://codefresh.io/docker-tutorial/java_docker_pipeline/ https://docs.docker.com/get-started/overview/ https://stackoverflow.com/questions/49162358/docker-init-zombies-why-does-it-matter","title":"References"},{"location":"virt/virtualbox/","text":"Usage GUI shortcuts In order to send ctrl+alt to guest vm use host key ( right ctrl by default). To send ctrl + alt + f1 use host key + f1","title":"VBox"},{"location":"virt/virtualbox/#usage","text":"","title":"Usage"},{"location":"virt/virtualbox/#gui-shortcuts","text":"In order to send ctrl+alt to guest vm use host key ( right ctrl by default). To send ctrl + alt + f1 use host key + f1","title":"GUI shortcuts"},{"location":"virt/vm/","text":"Virtualization Requires running VM Monitor (Hypervisor) on the host, bare-metal machine. Coordinates host machine resource sharing. Types of Hypervisors Type1 Type2 Benefits cheaper than bare-metal isolation (guest OS is fully isolated from host and other guest OSes) 'easy' migration HA Drawbacks performance overhead sometimes host performs overprovisioning, which can cause serious performance degradation Hypervisor is a single point of failure extra management","title":"Virtualization"},{"location":"virt/vm/#virtualization","text":"Requires running VM Monitor (Hypervisor) on the host, bare-metal machine. Coordinates host machine resource sharing.","title":"Virtualization"},{"location":"virt/vm/#types-of-hypervisors","text":"","title":"Types of Hypervisors"},{"location":"virt/vm/#type1","text":"","title":"Type1"},{"location":"virt/vm/#type2","text":"","title":"Type2"},{"location":"virt/vm/#benefits","text":"cheaper than bare-metal isolation (guest OS is fully isolated from host and other guest OSes) 'easy' migration HA","title":"Benefits"},{"location":"virt/vm/#drawbacks","text":"performance overhead sometimes host performs overprovisioning, which can cause serious performance degradation Hypervisor is a single point of failure extra management","title":"Drawbacks"}]}